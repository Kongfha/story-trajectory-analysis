# Pipeline Documentation

## Pipeline Stages
1. **Download and Clean (`00_download_and_clean.ipynb`)**
- Downloads Gutenberg plain-text books from configured URLs.
- Removes Gutenberg boilerplate.
- Saves cleaned raw text to `data/raw/{raw_filename}`.
- Writes metadata and catalog files.

2. **Chunk and Embed (`01_chunk_and_embed.ipynb`)**
- Splits each book into overlapping word windows.
- Encodes chunks with Sentence-Transformers.
- Saves chunk index + embeddings + run metadata under `data/processed/{processed_dir}/`.

3. **Transform and Cluster (`02_transform_and_cluster.ipynb`)**
- Fits PCA and saves low-dimensional trajectories.
- Persists global PCA fit artifacts (`outputs/pca/global_pca_fit.*`) for downstream reproducible analysis.
- Computes **Twist Signal** (`s_t`) and acceleration (`a_t`) for multiple `k` values.
- Detects top peaks and builds story-level features.
- Runs feature-based clustering and DTW-based similarity clustering.

4. **EDA and Visualization (`03_eda_and_visualization.ipynb`)**
- Loads outputs and metadata.
- Builds statistical + interactive views.
- Produces exploratory and deeper interpretation notes.
- Exports EDA figures/tables and insights.

5. **All-Novel Stacked Panels (`04_novel_stacked_twist_signal.ipynb`)**
- Loads per-book `signals_k{5,7,11}.npz` and `peaks_k{5,7,11}.json`.
- Generates one 3-row stacked figure per novel (`s_t` + `a_t` for each `k`).
- Exports grouped tables and a consolidated per-novel interpretation markdown.

6. **LLM Judge Overlay Prep (Prompt Package)**
- Builds long-context Gemini prompt payloads per novel from full raw text and chunk index lines.
- Produces strict JSON peak labels for overlay comparison (judge labels, not absolute ground truth).
- Validates output schema and spacing/range constraints before downstream plotting.

7. **LLM Judge Signal Analysis (`05_llm_judge_signal_analysis.ipynb`)**
- Focuses on `k=7` signal interpretation with smoothing + robust normalization + composite event score.
- Compares LLM peaks against existing pipeline peaks and processed event peaks.
- Exports analysis figures/tables and run-specific insight markdown under `outputs/llm_judge/analysis/`.

8. **PCA Component Insights (`06_pca_component_insights.ipynb`)**
- Loads persisted global PCA fit artifacts from `outputs/pca/`.
- Produces corpus-level PCA diagnostics and per-book PCA trajectory metrics.
- Links PCA dynamics to Twist Signal behavior for `k=5,7,11`.
- Runs permutation/bootstrapped robustness checks and exports integrity tables.
- Writes figures/tables/insight markdown under `outputs/pca_analysis/`.

## Twist Signal Definition
Given chunk embeddings `e_t` and context window size `k`:

- Context mean: `context_mean[t] = mean(e_{t-k}, ..., e_{t-1})` using available prefix when `t < k`.
- Twist Signal: `s_t = 1 - cosine(e_t, context_mean[t])`.
- Acceleration: `a_t = |s_t - s_{t-1}|` with `a_0 = 0`.

Note: this method was formerly referenced as “Option B”; project docs now use **Twist Signal**.

## Parameters and Defaults
Current defaults in pipeline notebooks:
- Chunking:
  - `window_words=300`
  - `stride_words=100`
- Embedding:
  - `batch_size=64`
  - current run model observed in artifact indexes: `sentence-transformers/all-mpnet-base-v2`
- Twist Signal:
  - `k_values=[5, 7, 11]`
  - primary reporting at `k=7`
- PCA:
  - dimensions `2` and `5`
- Peaks:
  - top peaks `top_K=3`
  - minimum separation `3` chunks
- Clustering:
  - default clusters `n_clusters=4`
- DTW:
  - signal resample length `L=200`

## Artifact Flow
File-level flow:
1. `data/raw/{raw_filename}.txt`
2. `data/processed/{processed_dir}/chunks.jsonl`
3. `data/processed/{processed_dir}/embeddings.npy`
4. `data/processed/{processed_dir}/index.json`
5. `data/processed/{processed_dir}/pca_d2.npy`, `pca_d5.npy`
6. `outputs/pca/global_pca_fit.npz`
7. `outputs/pca/global_pca_fit_meta.json`
8. `outputs/pca/global_pca_variance_summary.csv`
9. `data/processed/{processed_dir}/signals_k{K}.npz`
10. `data/processed/{processed_dir}/peaks_k{K}.json`
11. `outputs/features.csv`
12. `outputs/clusters_kmeans.csv`, `outputs/clusters_hier.csv`
13. `outputs/dtw_distance_k7.npy`
14. `outputs/eda/*` from EDA notebook
15. `outputs/eda/novel_stacks/figures/*.png`
16. `outputs/eda/novel_stacks/tables/*.csv`
17. `docs/NOVEL_STACKED_OUTPUT_INTERPRETATION.md`
18. `outputs/llm_judge/prompts/*.json` (generated by helper script)
19. `outputs/llm_judge/analysis/figures/*.png`
20. `outputs/llm_judge/analysis/tables/*.csv`
21. `outputs/llm_judge/analysis/insights_k7.md`
22. `outputs/pca_analysis/figures/*.png`
23. `outputs/pca_analysis/tables/*.csv`
24. `outputs/pca_analysis/insights.md`

Directory keying:
- `processed_dir` is the canonical per-book key and is based on abbreviated title slug.

## Reproducibility and Caching
- Seeds are fixed in notebooks (`SEED=42`) for sampling/clustering consistency.
- Embedding stage supports cache reuse if existing artifacts match expected parameters.
- PCA fit metadata is persisted to `outputs/pca/global_pca_fit_meta.json` for downstream reproducibility checks.
- Metadata file links each `book_id` to `raw_filename` and `processed_dir`.
- If legacy numeric processed folders exist, migration/fallback logic can still resolve them.

## Failure Modes and Recovery
Common issues and recovery steps:
- **Missing raw file**:
  - Re-run `00_download_and_clean.ipynb`.
  - Verify `raw_filename` and `raw_path` in `data/metadata.csv`.
- **Missing or stale embedding cache**:
  - Re-run `01_chunk_and_embed.ipynb`.
  - Set recompute flag if parameters changed.
- **Mismatch between metadata and processed folders**:
  - Confirm `processed_dir` exists under `data/processed/`.
- **DTW shape/symmetry problems**:
  - Re-run `02_transform_and_cluster.ipynb` and verify generated matrix diagnostics.
- **Missing global PCA fit artifacts (`outputs/pca/global_pca_fit.*`)**:
  - Re-run `02_transform_and_cluster.ipynb`.
  - Confirm `outputs/pca/global_pca_variance_summary.csv` exists before running `06_pca_component_insights.ipynb`.

## Extending the Pipeline
Recommended extension points:
- Add new signal variants while preserving `signals_k{K}.npz` compatibility.
- Add cluster methods and write separate output files instead of overwriting existing schemas.
- Add model-comparison runs by tagging outputs with model metadata.
- Add report-generation notebook/scripts that consume `outputs/` and `outputs/eda/`.
- Add LLM-judge overlays using `prompts/llm_judge/` and `tools/llm_judge/` contracts.

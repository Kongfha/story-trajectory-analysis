{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 Excitement Label Variant Analysis\n",
    "\n",
    "Compares three LLM excitement label variants and deep-dives on `indep_winsize_5`.\n",
    "\n",
    "Variants:\n",
    "- `base` -> `label.npy`\n",
    "- `winsize_5` -> `label_winsize_5.npy`\n",
    "- `indep_winsize_5` -> `label_indep_winsize_5.npy`\n",
    "\n",
    "This run uses `MA_WINDOW=5` for all moving-average presentation outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT=/Users/kongfha/Desktop/Time_Series_Mining/story-trajectory-analysis\n",
      "OUT_ROOT=/Users/kongfha/Desktop/Time_Series_Mining/story-trajectory-analysis/outputs/excitement_variant_analysis\n",
      "Variants: {'base': 'label.npy', 'winsize_5': 'label_winsize_5.npy', 'indep_winsize_5': 'label_indep_winsize_5.npy'}\n",
      "SEED=42, EPOCHS=200, BATCH_SIZE=4096, LR=0.01, WEIGHT_DECAY=0.0001, MA_WINDOW=5\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "PROJECT_ROOT = Path('.').resolve()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "METADATA_PATH = DATA_DIR / 'metadata.csv'\n",
    "SPLIT_PATH = PROJECT_ROOT / 'outputs' / 'excitement_linear' / 'tables' / 'split_manifest.csv'\n",
    "\n",
    "OUT_ROOT = PROJECT_ROOT / 'outputs' / 'excitement_variant_analysis'\n",
    "FIG_DIR = OUT_ROOT / 'figures'\n",
    "TABLE_DIR = OUT_ROOT / 'tables'\n",
    "MODEL_DIR = OUT_ROOT / 'model'\n",
    "for d in [OUT_ROOT, FIG_DIR, TABLE_DIR, MODEL_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "VARIANT_FILES = {\n",
    "    'base': 'label.npy',\n",
    "    'winsize_5': 'label_winsize_5.npy',\n",
    "    'indep_winsize_5': 'label_indep_winsize_5.npy',\n",
    "}\n",
    "VARIANTS = list(VARIANT_FILES.keys())\n",
    "\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 4096\n",
    "LR = 1e-2\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPS = 1e-8\n",
    "MA_WINDOW = 5\n",
    "\n",
    "print(f'PROJECT_ROOT={PROJECT_ROOT}')\n",
    "print(f'OUT_ROOT={OUT_ROOT}')\n",
    "print('Variants:', VARIANT_FILES)\n",
    "print(f'SEED={SEED}, EPOCHS={EPOCHS}, BATCH_SIZE={BATCH_SIZE}, LR={LR}, WEIGHT_DECAY={WEIGHT_DECAY}, MA_WINDOW={MA_WINDOW}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label_shape(y: np.ndarray, label_path: Path) -> np.ndarray:\n",
    "    y = np.asarray(y)\n",
    "    if y.ndim == 1:\n",
    "        out = y\n",
    "    elif y.ndim == 2 and 1 in y.shape:\n",
    "        out = y.reshape(-1)\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported label shape {y.shape} at {label_path}')\n",
    "    return out\n",
    "\n",
    "\n",
    "def moving_average_1d(x: np.ndarray, window: int) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float64).reshape(-1)\n",
    "    if window <= 1:\n",
    "        return x.copy()\n",
    "    if window % 2 == 0:\n",
    "        raise ValueError(f'MA window must be odd; got {window}')\n",
    "    return pd.Series(x).rolling(window=window, center=True, min_periods=1).mean().to_numpy()\n",
    "\n",
    "\n",
    "def metric_dict(y_true: np.ndarray, y_pred: np.ndarray) -> dict:\n",
    "    y_true = np.asarray(y_true, dtype=np.float64).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64).reshape(-1)\n",
    "    err = y_pred - y_true\n",
    "    mse = float(np.mean(err ** 2))\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    mae = float(np.mean(np.abs(err)))\n",
    "    denom = float(np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    r2 = float(1 - np.sum(err ** 2) / denom) if denom > 0 else np.nan\n",
    "    corr = float(np.corrcoef(y_true, y_pred)[0, 1]) if len(y_true) > 1 else np.nan\n",
    "    return {'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2, 'corr': corr}\n",
    "\n",
    "\n",
    "def iter_minibatches(n: int, batch_size: int, rng: np.random.Generator):\n",
    "    idx = rng.permutation(n)\n",
    "    for start in range(0, n, batch_size):\n",
    "        yield idx[start:start + batch_size]\n",
    "\n",
    "\n",
    "def winsize_5_constancy_violations(x: np.ndarray) -> tuple[int, int]:\n",
    "    x = np.asarray(x).reshape(-1)\n",
    "    violations = 0\n",
    "    total_blocks = 0\n",
    "    for s in range(0, len(x), 5):\n",
    "        blk = x[s:s+5]\n",
    "        if len(blk) > 1:\n",
    "            total_blocks += 1\n",
    "            if not np.all(blk == blk[0]):\n",
    "                violations += 1\n",
    "    return violations, total_blocks\n",
    "\n",
    "\n",
    "def required_output_files(test_ids: list[int], train_ids: list[int]) -> list[Path]:\n",
    "    files = [\n",
    "        TABLE_DIR / 'integrity_checks.csv',\n",
    "        TABLE_DIR / 'split_manifest_used.csv',\n",
    "        TABLE_DIR / 'label_distribution_by_variant.csv',\n",
    "        TABLE_DIR / 'variant_pairwise_agreement_global.csv',\n",
    "        TABLE_DIR / 'variant_pairwise_agreement_per_book.csv',\n",
    "        TABLE_DIR / 'model_global_metrics_by_variant.csv',\n",
    "        TABLE_DIR / 'model_per_novel_metrics_by_variant.csv',\n",
    "        TABLE_DIR / 'indep_winsize_5_support_stats.csv',\n",
    "        FIG_DIR / 'labels_grid_base.png',\n",
    "        FIG_DIR / 'labels_grid_winsize_5.png',\n",
    "        FIG_DIR / 'labels_grid_indep_winsize_5.png',\n",
    "        FIG_DIR / 'label_overlay_normpos_by_variant.png',\n",
    "        FIG_DIR / 'label_distribution_by_variant.png',\n",
    "        FIG_DIR / 'variant_pairwise_agreement_bar.png',\n",
    "        FIG_DIR / 'train_loss_curves_by_variant.png',\n",
    "        FIG_DIR / 'model_metric_comparison_by_variant.png',\n",
    "        FIG_DIR / 'indep_prediction_scatter_train_test.png',\n",
    "        FIG_DIR / 'indep_residual_hist_train_test.png',\n",
    "        FIG_DIR / 'indep_mae_raw_vs_moving_average.png',\n",
    "        MODEL_DIR / 'linear_weights_base.npz',\n",
    "        MODEL_DIR / 'linear_weights_winsize_5.npz',\n",
    "        MODEL_DIR / 'linear_weights_indep_winsize_5.npz',\n",
    "        OUT_ROOT / 'insights.md',\n",
    "    ]\n",
    "    files.extend([FIG_DIR / f'indep_novel_overlay_test_{bid}.png' for bid in test_ids])\n",
    "    files.extend([FIG_DIR / f'indep_novel_overlay_train_{bid}.png' for bid in train_ids])\n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded books: 20\n",
      "Embedding dim: 768\n",
      "Train/Test books: 16 4\n"
     ]
    }
   ],
   "source": [
    "# Load metadata + split + all required artifacts\n",
    "meta = pd.read_csv(METADATA_PATH)\n",
    "split_df = pd.read_csv(SPLIT_PATH)\n",
    "\n",
    "required_split_cols = {'book_id', 'title', 'processed_dir', 'T', 'split'}\n",
    "if not required_split_cols.issubset(split_df.columns):\n",
    "    raise ValueError(f'Split manifest missing columns: {required_split_cols - set(split_df.columns)}')\n",
    "\n",
    "split_df = split_df.copy()\n",
    "split_df['book_id'] = split_df['book_id'].astype(int)\n",
    "split_df['split'] = split_df['split'].astype(str)\n",
    "if not set(split_df['split']).issubset({'train', 'test'}):\n",
    "    raise ValueError('Split manifest has invalid split labels')\n",
    "\n",
    "train_ids = sorted(split_df.loc[split_df['split'] == 'train', 'book_id'].tolist())\n",
    "test_ids = sorted(split_df.loc[split_df['split'] == 'test', 'book_id'].tolist())\n",
    "\n",
    "integrity_rows = []\n",
    "integrity_rows.extend([\n",
    "    {'check': 'split_train_count', 'expected': 16, 'actual': len(train_ids), 'pass': len(train_ids) == 16},\n",
    "    {'check': 'split_test_count', 'expected': 4, 'actual': len(test_ids), 'pass': len(test_ids) == 4},\n",
    "    {'check': 'split_no_overlap', 'expected': True, 'actual': len(set(train_ids).intersection(set(test_ids))) == 0, 'pass': len(set(train_ids).intersection(set(test_ids))) == 0},\n",
    "    {'check': 'split_total_books', 'expected': 20, 'actual': len(set(train_ids + test_ids)), 'pass': len(set(train_ids + test_ids)) == 20},\n",
    "])\n",
    "\n",
    "split_df.sort_values(['split', 'book_id']).to_csv(TABLE_DIR / 'split_manifest_used.csv', index=False)\n",
    "\n",
    "payloads = []\n",
    "for r in meta.itertuples(index=False):\n",
    "    book_id = int(r.id)\n",
    "    processed_dir = str(r.processed_dir)\n",
    "    title = str(r.title)\n",
    "    pdir = PROCESSED_DIR / processed_dir\n",
    "\n",
    "    emb_path = pdir / 'embeddings.npy'\n",
    "    if not emb_path.exists():\n",
    "        raise FileNotFoundError(f'Missing embeddings: {emb_path}')\n",
    "    X = np.load(emb_path)\n",
    "    if X.ndim != 2:\n",
    "        raise ValueError(f'Expected 2D embeddings at {emb_path}, got {X.shape}')\n",
    "    T, D = X.shape\n",
    "\n",
    "    split_match = split_df[split_df['book_id'] == book_id]\n",
    "    if len(split_match) != 1:\n",
    "        raise ValueError(f'Expected exactly one split row for book_id={book_id}, got {len(split_match)}')\n",
    "    split_label = split_match.iloc[0]['split']\n",
    "\n",
    "    labels = {}\n",
    "    for variant, fname in VARIANT_FILES.items():\n",
    "        path = pdir / fname\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f'Missing label file for variant={variant}: {path}')\n",
    "\n",
    "        y_raw = np.load(path)\n",
    "        y = normalize_label_shape(y_raw, path).astype(np.float64)\n",
    "\n",
    "        in_range = bool(np.all((y >= 0) & (y <= 4)))\n",
    "        int_like = bool(np.allclose(y, np.round(y)))\n",
    "        align = int(len(y)) == int(T)\n",
    "\n",
    "        integrity_rows.append({\n",
    "            'check': f'book_{book_id}_{variant}_alignment',\n",
    "            'expected': 'len==T, integer-like labels in [0,4]',\n",
    "            'actual': f'len={len(y)}, T={T}, min={float(y.min()):.3f}, max={float(y.max()):.3f}, int_like={int_like}',\n",
    "            'pass': bool(align and in_range and int_like),\n",
    "        })\n",
    "\n",
    "        if not align:\n",
    "            raise ValueError(f'Length mismatch for {path}: len(y)={len(y)} vs T={T}')\n",
    "        if not in_range:\n",
    "            raise ValueError(f'Out-of-range labels for {path}')\n",
    "        if not int_like:\n",
    "            raise ValueError(f'Non-integer-like labels for {path}')\n",
    "\n",
    "        labels[variant] = y\n",
    "\n",
    "    payloads.append({\n",
    "        'book_id': book_id,\n",
    "        'title': title,\n",
    "        'processed_dir': processed_dir,\n",
    "        'split': split_label,\n",
    "        'T': int(T),\n",
    "        'D': int(D),\n",
    "        'X': X.astype(np.float64),\n",
    "        'labels': labels,\n",
    "    })\n",
    "\n",
    "payloads = sorted(payloads, key=lambda x: x['book_id'])\n",
    "if len(payloads) != 20:\n",
    "    raise ValueError(f'Expected 20 books in payloads, found {len(payloads)}')\n",
    "\n",
    "print('Loaded books:', len(payloads))\n",
    "print('Embedding dim:', payloads[0]['D'])\n",
    "print('Train/Test books:', len(train_ids), len(test_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved descriptive tables and figures.\n"
     ]
    }
   ],
   "source": [
    "# Cross-variant descriptive tables + figures\n",
    "\n",
    "# 1) Label distribution by variant (global + per-book)\n",
    "dist_rows = []\n",
    "for variant in VARIANTS:\n",
    "    all_y = np.concatenate([p['labels'][variant] for p in payloads]).astype(int)\n",
    "    vals, cnts = np.unique(all_y, return_counts=True)\n",
    "    total = int(len(all_y))\n",
    "    counts_map = {int(v): int(c) for v, c in zip(vals, cnts)}\n",
    "    for label in range(5):\n",
    "        c = counts_map.get(label, 0)\n",
    "        dist_rows.append({\n",
    "            'variant': variant,\n",
    "            'label': int(label),\n",
    "            'count': int(c),\n",
    "            'proportion': float(c / total if total > 0 else np.nan),\n",
    "            'scope': 'global',\n",
    "            'book_id': np.nan,\n",
    "            'processed_dir': np.nan,\n",
    "            'title': np.nan,\n",
    "        })\n",
    "\n",
    "for p in payloads:\n",
    "    for variant in VARIANTS:\n",
    "        y = p['labels'][variant].astype(int)\n",
    "        vals, cnts = np.unique(y, return_counts=True)\n",
    "        total = int(len(y))\n",
    "        counts_map = {int(v): int(c) for v, c in zip(vals, cnts)}\n",
    "        for label in range(5):\n",
    "            c = counts_map.get(label, 0)\n",
    "            dist_rows.append({\n",
    "                'variant': variant,\n",
    "                'label': int(label),\n",
    "                'count': int(c),\n",
    "                'proportion': float(c / total if total > 0 else np.nan),\n",
    "                'scope': 'book',\n",
    "                'book_id': int(p['book_id']),\n",
    "                'processed_dir': p['processed_dir'],\n",
    "                'title': p['title'],\n",
    "            })\n",
    "\n",
    "label_dist_df = pd.DataFrame(dist_rows)\n",
    "label_dist_df.to_csv(TABLE_DIR / 'label_distribution_by_variant.csv', index=False)\n",
    "\n",
    "# 2) Pairwise agreement (global + per-book)\n",
    "pair_global_rows = []\n",
    "pair_book_rows = []\n",
    "pairs = list(combinations(VARIANTS, 2))\n",
    "\n",
    "for va, vb in pairs:\n",
    "    a = np.concatenate([p['labels'][va] for p in payloads])\n",
    "    b = np.concatenate([p['labels'][vb] for p in payloads])\n",
    "    pair_global_rows.append({\n",
    "        'variant_a': va,\n",
    "        'variant_b': vb,\n",
    "        'mae': float(np.mean(np.abs(a - b))),\n",
    "        'exact_match': float(np.mean(a == b)),\n",
    "        'corr': float(np.corrcoef(a, b)[0, 1]),\n",
    "        'n_samples': int(len(a)),\n",
    "    })\n",
    "\n",
    "    for p in payloads:\n",
    "        ya = p['labels'][va]\n",
    "        yb = p['labels'][vb]\n",
    "        pair_book_rows.append({\n",
    "            'book_id': int(p['book_id']),\n",
    "            'processed_dir': p['processed_dir'],\n",
    "            'variant_a': va,\n",
    "            'variant_b': vb,\n",
    "            'mae': float(np.mean(np.abs(ya - yb))),\n",
    "            'exact_match': float(np.mean(ya == yb)),\n",
    "            'corr': float(np.corrcoef(ya, yb)[0, 1]) if len(ya) > 1 else np.nan,\n",
    "            'T': int(p['T']),\n",
    "        })\n",
    "\n",
    "pair_global_df = pd.DataFrame(pair_global_rows)\n",
    "pair_book_df = pd.DataFrame(pair_book_rows)\n",
    "pair_global_df.to_csv(TABLE_DIR / 'variant_pairwise_agreement_global.csv', index=False)\n",
    "pair_book_df.to_csv(TABLE_DIR / 'variant_pairwise_agreement_per_book.csv', index=False)\n",
    "\n",
    "# 3) winsize_5 constancy integrity\n",
    "viol_total, block_total = 0, 0\n",
    "for p in payloads:\n",
    "    v, b = winsize_5_constancy_violations(p['labels']['winsize_5'])\n",
    "    viol_total += v\n",
    "    block_total += b\n",
    "integrity_rows.append({\n",
    "    'check': 'winsize_5_block_constancy',\n",
    "    'expected': '0 violations',\n",
    "    'actual': f'{viol_total}/{block_total}',\n",
    "    'pass': viol_total == 0,\n",
    "})\n",
    "\n",
    "# Figures: 20-book grid per variant\n",
    "nrows, ncols = 5, 4\n",
    "for variant in VARIANTS:\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(22, 18), sharey=True)\n",
    "    axes = axes.flatten()\n",
    "    for ax, p in zip(axes, payloads):\n",
    "        y = p['labels'][variant]\n",
    "        t = np.arange(len(y))\n",
    "        ax.plot(t, y, linewidth=0.8, alpha=0.9)\n",
    "        ax.set_title(f\"{p['book_id']} | {p['title'][:30]}\", fontsize=9)\n",
    "        ax.set_ylim(-0.2, 4.2)\n",
    "        ax.grid(alpha=0.2)\n",
    "    for ax in axes[len(payloads):]:\n",
    "        ax.axis('off')\n",
    "    fig.suptitle(f'LLM Excitement Label per Chunk ({variant})', fontsize=16)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(FIG_DIR / f'labels_grid_{variant}.png', dpi=180, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "# Figure: normalized-position overlay by variant (3 panels)\n",
    "grid = np.linspace(0, 1, 201)\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(14, 14), sharex=True, sharey=True)\n",
    "for ax, variant in zip(axes, VARIANTS):\n",
    "    curves = []\n",
    "    for p in payloads:\n",
    "        y = p['labels'][variant]\n",
    "        x = np.linspace(0, 1, len(y))\n",
    "        ax.plot(x, y, alpha=0.08, linewidth=1.0)\n",
    "        curves.append(np.interp(grid, x, y))\n",
    "    mean_curve = np.mean(np.vstack(curves), axis=0)\n",
    "    ax.plot(grid, mean_curve, color='black', linewidth=2.2, label='mean profile')\n",
    "    ax.set_title(f'Overlay by normalized position: {variant}')\n",
    "    ax.set_ylabel('Excitement')\n",
    "    ax.grid(alpha=0.2)\n",
    "    ax.legend(loc='upper right')\n",
    "axes[-1].set_xlabel('Normalized position in novel')\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'label_overlay_normpos_by_variant.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Figure: global label distribution by variant\n",
    "global_dist = label_dist_df[label_dist_df['scope'] == 'global'].copy()\n",
    "pivot = global_dist.pivot_table(index='variant', columns='label', values='count', aggfunc='sum').fillna(0)\n",
    "pivot = pivot.reindex(index=VARIANTS, columns=[0,1,2,3,4])\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['#4c78a8', '#f58518', '#54a24b', '#e45756', '#72b7b2']\n",
    "bottom = np.zeros(len(pivot))\n",
    "for i, label in enumerate([0,1,2,3,4]):\n",
    "    vals = pivot[label].to_numpy()\n",
    "    ax.bar(np.arange(len(pivot)), vals, bottom=bottom, label=f'label {label}', color=colors[i], alpha=0.9)\n",
    "    bottom += vals\n",
    "ax.set_xticks(np.arange(len(pivot)))\n",
    "ax.set_xticklabels(pivot.index)\n",
    "ax.set_ylabel('Chunk count')\n",
    "ax.set_title('Global label distribution by variant')\n",
    "ax.legend(ncol=5, loc='upper center', bbox_to_anchor=(0.5, 1.15))\n",
    "ax.grid(axis='y', alpha=0.25)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'label_distribution_by_variant.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Figure: pairwise agreement bar chart\n",
    "pair_plot = pair_global_df.copy()\n",
    "pair_plot['pair'] = pair_plot['variant_a'] + ' vs ' + pair_plot['variant_b']\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 4.5))\n",
    "metrics = [('mae', 'MAE'), ('exact_match', 'Exact match'), ('corr', 'Correlation')]\n",
    "for ax, (col, title) in zip(axes, metrics):\n",
    "    ax.bar(pair_plot['pair'], pair_plot[col], color=['#4c78a8', '#f58518', '#54a24b'])\n",
    "    ax.set_title(title)\n",
    "    ax.tick_params(axis='x', rotation=20)\n",
    "    ax.grid(axis='y', alpha=0.2)\n",
    "fig.suptitle('Global pairwise agreement across label variants', y=1.02)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'variant_pairwise_agreement_bar.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "print('Saved descriptive tables and figures.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model tables and model weights.\n",
      "           variant  split  n_samples  n_novels       mse      rmse       mae  \\\n",
      "0             base   test       3706         4  1.424219  1.193406  1.040990   \n",
      "1             base  train      17631        16  1.447624  1.203172  1.063871   \n",
      "2  indep_winsize_5   test       3706         4  0.561263  0.749175  0.612690   \n",
      "3  indep_winsize_5  train      17631        16  0.528095  0.726702  0.592890   \n",
      "4        winsize_5   test       3706         4  0.979047  0.989468  0.768356   \n",
      "5        winsize_5  train      17631        16  0.855820  0.925105  0.724276   \n",
      "\n",
      "         r2      corr  ma_window    mae_ma  \n",
      "0  0.136429  0.379810          5  0.460144  \n",
      "1  0.173382  0.417272          5  0.438537  \n",
      "2  0.075496  0.306017          5  0.270080  \n",
      "3  0.193136  0.439557          5  0.244334  \n",
      "4  0.152135  0.411479          5  0.620573  \n",
      "5  0.282322  0.531895          5  0.569507  \n"
     ]
    }
   ],
   "source": [
    "# Train linear models for all variants\n",
    "\n",
    "model_histories = {}\n",
    "model_cache = {}\n",
    "global_rows = []\n",
    "per_book_rows = []\n",
    "indep_support_rows = []\n",
    "\n",
    "train_id_set = set(train_ids)\n",
    "\n",
    "for variant in VARIANTS:\n",
    "    # Build train/test arrays\n",
    "    X_train_list, y_train_list = [], []\n",
    "    X_test_list, y_test_list = [], []\n",
    "\n",
    "    for p in payloads:\n",
    "        y = p['labels'][variant].reshape(-1)\n",
    "        if p['split'] == 'train':\n",
    "            X_train_list.append(p['X'])\n",
    "            y_train_list.append(y)\n",
    "        else:\n",
    "            X_test_list.append(p['X'])\n",
    "            y_test_list.append(y)\n",
    "\n",
    "    X_train = np.vstack(X_train_list)\n",
    "    y_train = np.concatenate(y_train_list).reshape(-1, 1)\n",
    "    X_test = np.vstack(X_test_list)\n",
    "    y_test = np.concatenate(y_test_list).reshape(-1, 1)\n",
    "\n",
    "    x_mean = X_train.mean(axis=0, keepdims=True)\n",
    "    x_std = X_train.std(axis=0, keepdims=True)\n",
    "    x_std_safe = np.where(x_std < EPS, 1.0, x_std)\n",
    "\n",
    "    X_train_n = (X_train - x_mean) / x_std_safe\n",
    "    X_test_n = (X_test - x_mean) / x_std_safe\n",
    "\n",
    "    D = X_train_n.shape[1]\n",
    "    W = rng.normal(loc=0.0, scale=0.01, size=(D, 1))\n",
    "    b = np.zeros((1,), dtype=np.float64)\n",
    "\n",
    "    train_loss_hist, test_loss_hist = [], []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        for batch_idx in iter_minibatches(X_train_n.shape[0], BATCH_SIZE, rng):\n",
    "            Xb = X_train_n[batch_idx]\n",
    "            yb = y_train[batch_idx]\n",
    "\n",
    "            pred = Xb @ W + b\n",
    "            err = pred - yb\n",
    "\n",
    "            grad_W = (2.0 / len(Xb)) * (Xb.T @ err) + 2.0 * WEIGHT_DECAY * W\n",
    "            grad_b = (2.0 / len(Xb)) * np.sum(err, axis=0)\n",
    "\n",
    "            W -= LR * grad_W\n",
    "            b -= LR * grad_b\n",
    "\n",
    "        train_pred_epoch = (X_train_n @ W + b).reshape(-1)\n",
    "        test_pred_epoch = (X_test_n @ W + b).reshape(-1)\n",
    "        train_loss_hist.append(float(np.mean((train_pred_epoch - y_train.reshape(-1)) ** 2)))\n",
    "        test_loss_hist.append(float(np.mean((test_pred_epoch - y_test.reshape(-1)) ** 2)))\n",
    "\n",
    "    train_pred = (X_train_n @ W + b).reshape(-1)\n",
    "    test_pred = (X_test_n @ W + b).reshape(-1)\n",
    "\n",
    "    train_m = metric_dict(y_train.reshape(-1), train_pred)\n",
    "    test_m = metric_dict(y_test.reshape(-1), test_pred)\n",
    "\n",
    "    # MAE on MA(5) computed per-book to avoid boundary artifacts\n",
    "    abs_err_ma = {'train': [], 'test': []}\n",
    "\n",
    "    # per-book metrics + cache per-book predictions\n",
    "    per_book_pred = {}\n",
    "    for p in payloads:\n",
    "        Xp = (p['X'] - x_mean) / x_std_safe\n",
    "        y_true = p['labels'][variant].reshape(-1)\n",
    "        y_pred = (Xp @ W + b).reshape(-1)\n",
    "        per_book_pred[p['book_id']] = y_pred\n",
    "\n",
    "        y_true_ma = moving_average_1d(y_true, MA_WINDOW)\n",
    "        y_pred_ma = moving_average_1d(y_pred, MA_WINDOW)\n",
    "        mae_ma = float(np.mean(np.abs(y_pred_ma - y_true_ma)))\n",
    "\n",
    "        split = p['split']\n",
    "        abs_err_ma[split].append(np.abs(y_pred_ma - y_true_ma))\n",
    "\n",
    "        m = metric_dict(y_true, y_pred)\n",
    "        per_book_rows.append({\n",
    "            'variant': variant,\n",
    "            'book_id': int(p['book_id']),\n",
    "            'title': p['title'],\n",
    "            'processed_dir': p['processed_dir'],\n",
    "            'split': split,\n",
    "            'T': int(p['T']),\n",
    "            'mse': m['mse'],\n",
    "            'rmse': m['rmse'],\n",
    "            'mae': m['mae'],\n",
    "            'r2': m['r2'],\n",
    "            'corr': m['corr'],\n",
    "            'mae_ma': mae_ma,\n",
    "        })\n",
    "\n",
    "        if variant == 'indep_winsize_5':\n",
    "            res = y_pred - y_true\n",
    "            indep_support_rows.append({\n",
    "                'book_id': int(p['book_id']),\n",
    "                'split': split,\n",
    "                'T': int(p['T']),\n",
    "                'y_true_mean': float(np.mean(y_true)),\n",
    "                'y_true_std': float(np.std(y_true)),\n",
    "                'y_pred_mean': float(np.mean(y_pred)),\n",
    "                'y_pred_std': float(np.std(y_pred)),\n",
    "                'pred_min': float(np.min(y_pred)),\n",
    "                'pred_max': float(np.max(y_pred)),\n",
    "                'res_mean': float(np.mean(res)),\n",
    "                'res_std': float(np.std(res)),\n",
    "                'res_p05': float(np.quantile(res, 0.05)),\n",
    "                'res_p95': float(np.quantile(res, 0.95)),\n",
    "                'corr_true_pred': float(np.corrcoef(y_true, y_pred)[0, 1]) if len(y_true) > 1 else np.nan,\n",
    "                'title': p['title'],\n",
    "            })\n",
    "\n",
    "    train_abs_ma = np.concatenate(abs_err_ma['train']) if len(abs_err_ma['train']) else np.array([np.nan])\n",
    "    test_abs_ma = np.concatenate(abs_err_ma['test']) if len(abs_err_ma['test']) else np.array([np.nan])\n",
    "\n",
    "    global_rows.extend([\n",
    "        {\n",
    "            'variant': variant,\n",
    "            'split': 'train',\n",
    "            'n_samples': int(len(y_train)),\n",
    "            'n_novels': int(len(train_ids)),\n",
    "            'mse': train_m['mse'],\n",
    "            'rmse': train_m['rmse'],\n",
    "            'mae': train_m['mae'],\n",
    "            'r2': train_m['r2'],\n",
    "            'corr': train_m['corr'],\n",
    "            'ma_window': MA_WINDOW,\n",
    "            'mae_ma': float(np.mean(train_abs_ma)),\n",
    "        },\n",
    "        {\n",
    "            'variant': variant,\n",
    "            'split': 'test',\n",
    "            'n_samples': int(len(y_test)),\n",
    "            'n_novels': int(len(test_ids)),\n",
    "            'mse': test_m['mse'],\n",
    "            'rmse': test_m['rmse'],\n",
    "            'mae': test_m['mae'],\n",
    "            'r2': test_m['r2'],\n",
    "            'corr': test_m['corr'],\n",
    "            'ma_window': MA_WINDOW,\n",
    "            'mae_ma': float(np.mean(test_abs_ma)),\n",
    "        },\n",
    "    ])\n",
    "\n",
    "    np.savez(\n",
    "        MODEL_DIR / f'linear_weights_{variant}.npz',\n",
    "        W=W.astype(np.float32),\n",
    "        b=b.astype(np.float32),\n",
    "        x_mean=x_mean.reshape(-1).astype(np.float32),\n",
    "        x_std=x_std_safe.reshape(-1).astype(np.float32),\n",
    "        seed=np.array([SEED], dtype=np.int32),\n",
    "        lr=np.array([LR], dtype=np.float32),\n",
    "        epochs=np.array([EPOCHS], dtype=np.int32),\n",
    "        batch_size=np.array([BATCH_SIZE], dtype=np.int32),\n",
    "        weight_decay=np.array([WEIGHT_DECAY], dtype=np.float32),\n",
    "        variant=np.array([variant]),\n",
    "    )\n",
    "\n",
    "    model_histories[variant] = {\n",
    "        'train_loss': train_loss_hist,\n",
    "        'test_loss': test_loss_hist,\n",
    "    }\n",
    "    model_cache[variant] = {\n",
    "        'W': W,\n",
    "        'b': b,\n",
    "        'x_mean': x_mean,\n",
    "        'x_std_safe': x_std_safe,\n",
    "        'per_book_pred': per_book_pred,\n",
    "        'y_train_true': y_train.reshape(-1),\n",
    "        'y_test_true': y_test.reshape(-1),\n",
    "        'y_train_pred': train_pred,\n",
    "        'y_test_pred': test_pred,\n",
    "    }\n",
    "\n",
    "model_global_df = pd.DataFrame(global_rows).sort_values(['variant', 'split']).reset_index(drop=True)\n",
    "model_per_book_df = pd.DataFrame(per_book_rows).sort_values(['variant', 'split', 'book_id']).reset_index(drop=True)\n",
    "indep_support_df = pd.DataFrame(indep_support_rows).sort_values(['split', 'book_id']).reset_index(drop=True)\n",
    "\n",
    "model_global_df.to_csv(TABLE_DIR / 'model_global_metrics_by_variant.csv', index=False)\n",
    "model_per_book_df.to_csv(TABLE_DIR / 'model_per_novel_metrics_by_variant.csv', index=False)\n",
    "indep_support_df.to_csv(TABLE_DIR / 'indep_winsize_5_support_stats.csv', index=False)\n",
    "\n",
    "print('Saved model tables and model weights.')\n",
    "print(model_global_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved modeling figures.\n",
      "Selected train indep overlays: [43, 1257]\n"
     ]
    }
   ],
   "source": [
    "# Figures from modeling + indep deep dive\n",
    "\n",
    "# train loss curves by variant\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(11, 12), sharex=True)\n",
    "for ax, variant in zip(axes, VARIANTS):\n",
    "    tr = model_histories[variant]['train_loss']\n",
    "    te = model_histories[variant]['test_loss']\n",
    "    ax.plot(np.arange(1, EPOCHS + 1), tr, label='train_mse', linewidth=1.6)\n",
    "    ax.plot(np.arange(1, EPOCHS + 1), te, label='test_mse', linewidth=1.4)\n",
    "    ax.set_title(f'Loss curves: {variant}')\n",
    "    ax.set_ylabel('MSE')\n",
    "    ax.grid(alpha=0.25)\n",
    "    ax.legend()\n",
    "axes[-1].set_xlabel('Epoch')\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'train_loss_curves_by_variant.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# test metric comparison by variant\n",
    "test_metrics = model_global_df[model_global_df['split'] == 'test'].copy().set_index('variant').loc[VARIANTS].reset_index()\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n",
    "metric_specs = [('rmse', 'Test RMSE'), ('mae', 'Test MAE'), ('r2', 'Test R2'), ('corr', 'Test Corr')]\n",
    "for ax, (col, title) in zip(axes.flatten(), metric_specs):\n",
    "    vals = test_metrics[col].to_numpy()\n",
    "    ax.bar(np.arange(len(VARIANTS)), vals, color=['#4c78a8', '#f58518', '#54a24b'])\n",
    "    ax.set_xticks(np.arange(len(VARIANTS)))\n",
    "    ax.set_xticklabels(VARIANTS, rotation=20)\n",
    "    ax.set_title(title)\n",
    "    ax.grid(axis='y', alpha=0.25)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'model_metric_comparison_by_variant.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# indep variant diagnostics\n",
    "indep = model_cache['indep_winsize_5']\n",
    "y_train_true = indep['y_train_true']\n",
    "y_train_pred = indep['y_train_pred']\n",
    "y_test_true = indep['y_test_true']\n",
    "y_test_pred = indep['y_test_pred']\n",
    "\n",
    "# scatter\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(y_train_true, y_train_pred, s=8, alpha=0.15, label='train')\n",
    "ax.scatter(y_test_true, y_test_pred, s=12, alpha=0.30, label='test')\n",
    "lo = min(float(y_train_true.min()), float(y_test_true.min()), float(y_train_pred.min()), float(y_test_pred.min()))\n",
    "hi = max(float(y_train_true.max()), float(y_test_true.max()), float(y_train_pred.max()), float(y_test_pred.max()))\n",
    "ax.plot([lo, hi], [lo, hi], linestyle='--', color='black', linewidth=1.2)\n",
    "ax.set_xlabel('True excitement')\n",
    "ax.set_ylabel('Predicted excitement')\n",
    "ax.set_title('indep_winsize_5: Prediction scatter')\n",
    "ax.grid(alpha=0.2)\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'indep_prediction_scatter_train_test.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# residual hist\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(y_train_pred - y_train_true, bins=60, alpha=0.45, label='train')\n",
    "ax.hist(y_test_pred - y_test_true, bins=60, alpha=0.55, label='test')\n",
    "ax.set_xlabel('Residual (pred - true)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('indep_winsize_5: Residual distribution')\n",
    "ax.grid(alpha=0.2)\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'indep_residual_hist_train_test.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# raw vs MA(5) MAE bars for indep\n",
    "indep_global = model_global_df[model_global_df['variant'] == 'indep_winsize_5'].copy().set_index('split').loc[['train', 'test']].reset_index()\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "x = np.arange(len(indep_global))\n",
    "bw = 0.34\n",
    "ax.bar(x - bw / 2, indep_global['mae'], width=bw, label='Raw MAE', alpha=0.9)\n",
    "ax.bar(x + bw / 2, indep_global['mae_ma'], width=bw, label=f'MA({MA_WINDOW}) MAE', alpha=0.9)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(indep_global['split'])\n",
    "ax.set_ylabel('MAE')\n",
    "ax.set_title('indep_winsize_5: Raw vs MA(5) MAE')\n",
    "ax.grid(axis='y', alpha=0.25)\n",
    "ax.legend()\n",
    "for i, row in indep_global.iterrows():\n",
    "    ax.text(i - bw / 2, row['mae'] + 0.01, f\"{row['mae']:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "    ax.text(i + bw / 2, row['mae_ma'] + 0.01, f\"{row['mae_ma']:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'indep_mae_raw_vs_moving_average.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# overlays for indep: 4 test + 2 train (best/worst train RMSE)\n",
    "by_id = {p['book_id']: p for p in payloads}\n",
    "indep_per = model_per_book_df[model_per_book_df['variant'] == 'indep_winsize_5'].copy()\n",
    "\n",
    "train_rmse = indep_per[indep_per['split'] == 'train'][['book_id', 'rmse']].sort_values('rmse')\n",
    "train_best = int(train_rmse.iloc[0]['book_id'])\n",
    "train_worst = int(train_rmse.iloc[-1]['book_id'])\n",
    "selected_train_overlay_ids = sorted(list({train_best, train_worst}))\n",
    "if len(selected_train_overlay_ids) == 1:\n",
    "    selected_train_overlay_ids.append(int(train_rmse.iloc[1]['book_id']))\n",
    "    selected_train_overlay_ids = sorted(selected_train_overlay_ids)\n",
    "\n",
    "for bid in test_ids:\n",
    "    p = by_id[bid]\n",
    "    y_true = p['labels']['indep_winsize_5'].reshape(-1)\n",
    "    y_pred = model_cache['indep_winsize_5']['per_book_pred'][bid].reshape(-1)\n",
    "    y_true_ma = moving_average_1d(y_true, MA_WINDOW)\n",
    "    y_pred_ma = moving_average_1d(y_pred, MA_WINDOW)\n",
    "    raw_mae = float(np.mean(np.abs(y_pred - y_true)))\n",
    "    ma_mae = float(np.mean(np.abs(y_pred_ma - y_true_ma)))\n",
    "    t = np.arange(len(y_true))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 4.5))\n",
    "    ax.plot(t, y_true, label='llm_label_raw', linewidth=1.0, alpha=0.30, color='tab:blue')\n",
    "    ax.plot(t, y_pred, label='linear_pred_raw', linewidth=1.0, alpha=0.30, color='tab:orange')\n",
    "    ax.plot(t, y_true_ma, label=f'llm_label_MA{MA_WINDOW}', linewidth=2.0, color='tab:blue')\n",
    "    ax.plot(t, y_pred_ma, label=f'linear_pred_MA{MA_WINDOW}', linewidth=2.0, color='tab:orange')\n",
    "    ax.set_title(f\"indep test {bid} | {p['title']} | raw MAE={raw_mae:.3f}, MA({MA_WINDOW}) MAE={ma_mae:.3f}\")\n",
    "    ax.set_xlabel('Chunk index')\n",
    "    ax.set_ylabel('Excitement')\n",
    "    ax.set_ylim(-0.5, 4.5)\n",
    "    ax.grid(alpha=0.25)\n",
    "    ax.legend(ncol=2)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(FIG_DIR / f'indep_novel_overlay_test_{bid}.png', dpi=180, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "for bid in selected_train_overlay_ids:\n",
    "    p = by_id[bid]\n",
    "    y_true = p['labels']['indep_winsize_5'].reshape(-1)\n",
    "    y_pred = model_cache['indep_winsize_5']['per_book_pred'][bid].reshape(-1)\n",
    "    y_true_ma = moving_average_1d(y_true, MA_WINDOW)\n",
    "    y_pred_ma = moving_average_1d(y_pred, MA_WINDOW)\n",
    "    raw_mae = float(np.mean(np.abs(y_pred - y_true)))\n",
    "    ma_mae = float(np.mean(np.abs(y_pred_ma - y_true_ma)))\n",
    "    t = np.arange(len(y_true))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 4.5))\n",
    "    ax.plot(t, y_true, label='llm_label_raw', linewidth=1.0, alpha=0.30, color='tab:blue')\n",
    "    ax.plot(t, y_pred, label='linear_pred_raw', linewidth=1.0, alpha=0.30, color='tab:orange')\n",
    "    ax.plot(t, y_true_ma, label=f'llm_label_MA{MA_WINDOW}', linewidth=2.0, color='tab:blue')\n",
    "    ax.plot(t, y_pred_ma, label=f'linear_pred_MA{MA_WINDOW}', linewidth=2.0, color='tab:orange')\n",
    "    ax.set_title(f\"indep train {bid} | {p['title']} | raw MAE={raw_mae:.3f}, MA({MA_WINDOW}) MAE={ma_mae:.3f}\")\n",
    "    ax.set_xlabel('Chunk index')\n",
    "    ax.set_ylabel('Excitement')\n",
    "    ax.set_ylim(-0.5, 4.5)\n",
    "    ax.grid(alpha=0.25)\n",
    "    ax.legend(ncol=2)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(FIG_DIR / f'indep_novel_overlay_train_{bid}.png', dpi=180, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "print('Saved modeling figures.')\n",
    "print('Selected train indep overlays:', selected_train_overlay_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved insights markdown: /Users/kongfha/Desktop/Time_Series_Mining/story-trajectory-analysis/outputs/excitement_variant_analysis/insights.md\n",
      "Saved integrity checks: /Users/kongfha/Desktop/Time_Series_Mining/story-trajectory-analysis/outputs/excitement_variant_analysis/tables/integrity_checks.csv\n",
      "Integrity all-pass: True\n"
     ]
    }
   ],
   "source": [
    "# Insights markdown + finalize integrity checks\n",
    "\n",
    "variant_test = model_global_df[model_global_df['split'] == 'test'].copy().sort_values('rmse')\n",
    "indep_test = model_global_df[(model_global_df['variant'] == 'indep_winsize_5') & (model_global_df['split'] == 'test')].iloc[0]\n",
    "indep_train = model_global_df[(model_global_df['variant'] == 'indep_winsize_5') & (model_global_df['split'] == 'train')].iloc[0]\n",
    "\n",
    "pair_global = pd.read_csv(TABLE_DIR / 'variant_pairwise_agreement_global.csv')\n",
    "label_dist = pd.read_csv(TABLE_DIR / 'label_distribution_by_variant.csv')\n",
    "indep_dist = label_dist[(label_dist['scope'] == 'global') & (label_dist['variant'] == 'indep_winsize_5')].sort_values('label')\n",
    "\n",
    "indep_per_test = model_per_book_df[(model_per_book_df['variant'] == 'indep_winsize_5') & (model_per_book_df['split'] == 'test')].sort_values('rmse')\n",
    "\n",
    "insights_lines = [\n",
    "    '# Excitement Variant Analysis Insights (MA(W), current run W=5)',\n",
    "    '',\n",
    "    '## 1) Dataset and Integrity Summary',\n",
    "    f'- Books analyzed: **{len(payloads)}** (reused split: {len(train_ids)} train / {len(test_ids)} test novels).',\n",
    "    '- All three label variants passed shape, range, and alignment checks (`len(label)==T`, labels in `[0,4]`, integer-like).',\n",
    "    '- `winsize_5` block-constancy check passed (no within-block variation for 5-chunk blocks).',\n",
    "    '',\n",
    "    '## 2) Variant Comparison Findings',\n",
    "    '- Test split global metrics by variant (lower RMSE/MAE better; higher R2/corr better):',\n",
    "]\n",
    "for _, row in variant_test.iterrows():\n",
    "    insights_lines.append(\n",
    "        f\"  - `{row['variant']}`: RMSE={row['rmse']:.3f}, MAE={row['mae']:.3f}, R2={row['r2']:.3f}, corr={row['corr']:.3f}, MA(5) MAE={row['mae_ma']:.3f}\"\n",
    "    )\n",
    "insights_lines += [\n",
    "    '',\n",
    "    '- Pairwise label agreement indicates the variants are materially different sources, not trivial rewrites.',\n",
    "    '- See: `tables/variant_pairwise_agreement_global.csv` and `tables/variant_pairwise_agreement_per_book.csv`.',\n",
    "    '',\n",
    "    '## 3) indep_winsize_5 Verdict (Primary Focus)',\n",
    "    '- Chunk-level reliability: **limited/moderate**. Raw pointwise error remains substantial.',\n",
    "    '- Trend-level utility (MA(5)): **useful as a coarse proxy** for trajectory/pacing interpretation.',\n",
    "    f\"- indep global train: RMSE={indep_train['rmse']:.3f}, MAE={indep_train['mae']:.3f}, R2={indep_train['r2']:.3f}, corr={indep_train['corr']:.3f}, MA(5) MAE={indep_train['mae_ma']:.3f}.\",\n",
    "    f\"- indep global test: RMSE={indep_test['rmse']:.3f}, MAE={indep_test['mae']:.3f}, R2={indep_test['r2']:.3f}, corr={indep_test['corr']:.3f}, MA(5) MAE={indep_test['mae_ma']:.3f}.\",\n",
    "    '',\n",
    "    '## 4) Book-Level Highlights (indep_winsize_5, test novels)',\n",
    "]\n",
    "for _, row in indep_per_test.iterrows():\n",
    "    insights_lines.append(\n",
    "        f\"- {int(row['book_id'])} | {row['title']}: RMSE={row['rmse']:.3f}, MAE={row['mae']:.3f}, R2={row['r2']:.3f}, corr={row['corr']:.3f}, MA(5) MAE={row['mae_ma']:.3f}.\"\n",
    "    )\n",
    "insights_lines += [\n",
    "    '',\n",
    "    '## 5) Use Now vs Avoid Now (indep_winsize_5)',\n",
    "    '- Use now: relative trend profiling, chapter-level pacing summaries, cross-book smoothed trajectory comparison.',\n",
    "    '- Avoid now: chunk-level absolute scoring, spike-triggered threshold decisions, fine-grained event detection from raw predictions.',\n",
    "    '',\n",
    "    '## 6) Next Experiments + Acceptance Criteria',\n",
    "    '1. Compare Ridge/ElasticNet against current linear GD head on the same split.',\n",
    "    '2. Add temporal features (`x_t - x_{t-1}`, short rolling context) while keeping linear head.',\n",
    "    '3. Try imbalance-aware objectives for rare labels.',\n",
    "    '4. Accept a new model only if both improve: test RMSE and worst-case test-novel RMSE.',\n",
    "    '',\n",
    "    '## Provenance',\n",
    "    '- Figures: `outputs/excitement_variant_analysis/figures/*.png`',\n",
    "    '- Tables: `outputs/excitement_variant_analysis/tables/*.csv`',\n",
    "    '- Models: `outputs/excitement_variant_analysis/model/*.npz`',\n",
    "]\n",
    "\n",
    "insights_text = '\\n'.join(insights_lines) + '\\n'\n",
    "(OUT_ROOT / 'insights.md').write_text(insights_text, encoding='utf-8')\n",
    "\n",
    "# Write current integrity rows first so file-existence completeness can include integrity file itself\n",
    "pd.DataFrame(integrity_rows).to_csv(TABLE_DIR / 'integrity_checks.csv', index=False)\n",
    "\n",
    "# Final integrity checks: output completeness\n",
    "expected_files = required_output_files(test_ids=test_ids, train_ids=selected_train_overlay_ids)\n",
    "missing = [str(p) for p in expected_files if not p.exists()]\n",
    "\n",
    "integrity_rows.extend([\n",
    "    {'check': 'output_files_complete', 'expected': len(expected_files), 'actual': len(expected_files) - len(missing), 'pass': len(missing) == 0},\n",
    "    {'check': 'indep_overlay_test_count', 'expected': 4, 'actual': len(list(FIG_DIR.glob('indep_novel_overlay_test_*.png'))), 'pass': len(list(FIG_DIR.glob('indep_novel_overlay_test_*.png'))) == 4},\n",
    "    {'check': 'indep_overlay_train_count', 'expected': 2, 'actual': len(list(FIG_DIR.glob('indep_novel_overlay_train_*.png'))), 'pass': len(list(FIG_DIR.glob('indep_novel_overlay_train_*.png'))) == 2},\n",
    "    {'check': 'ma_window_is_5', 'expected': 5, 'actual': int(model_global_df['ma_window'].iloc[0]), 'pass': bool((model_global_df['ma_window'] == 5).all())},\n",
    "])\n",
    "\n",
    "integrity_df = pd.DataFrame(integrity_rows)\n",
    "integrity_df.to_csv(TABLE_DIR / 'integrity_checks.csv', index=False)\n",
    "\n",
    "print('Saved insights markdown:', OUT_ROOT / 'insights.md')\n",
    "print('Saved integrity checks:', TABLE_DIR / 'integrity_checks.csv')\n",
    "print('Integrity all-pass:', bool(integrity_df['pass'].all()))\n",
    "if missing:\n",
    "    print('Missing files:')\n",
    "    for m in missing:\n",
    "        print('-', m)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

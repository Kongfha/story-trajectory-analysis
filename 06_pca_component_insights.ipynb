{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - PCA Component Insights (Global + Book)\n",
    "\n",
    "This notebook builds reproducible PCA component interpretation at corpus and book level, then links PCA dynamics to Twist Signal behavior.\n",
    "\n",
    "## Required upstream run\n",
    "Run `02_transform_and_cluster.ipynb` first to generate:\n",
    "- `outputs/pca/global_pca_fit.npz`\n",
    "- `outputs/pca/global_pca_fit_meta.json`\n",
    "- `outputs/pca/global_pca_variance_summary.csv`\n",
    "\n",
    "## Outputs\n",
    "- `outputs/pca_analysis/tables/book_component_stats.csv`\n",
    "- `outputs/pca_analysis/tables/book_component_signal_assoc.csv`\n",
    "- `outputs/pca_analysis/tables/component_exemplar_chunks.csv`\n",
    "- `outputs/pca_analysis/tables/component_genre_association.csv`\n",
    "- `outputs/pca_analysis/tables/temporal_trend_stats.csv`\n",
    "- `outputs/pca_analysis/tables/corpus_assoc_bootstrap.csv`\n",
    "- `outputs/pca_analysis/tables/projection_consistency_checks.csv`\n",
    "- `outputs/pca_analysis/tables/pca_integrity_checks.csv`\n",
    "- `outputs/pca_analysis/figures/*.png`\n",
    "- `outputs/pca_analysis/insights.md`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "PROJECT_ROOT = Path('.').resolve()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "OUTPUTS_DIR = PROJECT_ROOT / 'outputs'\n",
    "PCA_DIR = OUTPUTS_DIR / 'pca'\n",
    "ANALYSIS_DIR = OUTPUTS_DIR / 'pca_analysis'\n",
    "TABLE_DIR = ANALYSIS_DIR / 'tables'\n",
    "FIG_DIR = ANALYSIS_DIR / 'figures'\n",
    "\n",
    "for d in [ANALYSIS_DIR, TABLE_DIR, FIG_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "METADATA_PATH = DATA_DIR / 'metadata.csv'\n",
    "GLOBAL_PCA_PATH = PCA_DIR / 'global_pca_fit.npz'\n",
    "GLOBAL_PCA_META_PATH = PCA_DIR / 'global_pca_fit_meta.json'\n",
    "GLOBAL_PCA_VAR_PATH = PCA_DIR / 'global_pca_variance_summary.csv'\n",
    "\n",
    "k_values = [5, 7, 11]\n",
    "primary_k = 7\n",
    "min_exemplar_distance = 5\n",
    "max_exemplars_per_book = 3\n",
    "top_exemplars_per_direction = 15\n",
    "n_perm = 1000\n",
    "n_boot = 2000\n",
    "\n",
    "print(f'Project root: {PROJECT_ROOT}')\n",
    "print(f'Processed dir: {PROCESSED_DIR}')\n",
    "print(f'PCA artifact dir: {PCA_DIR}')\n",
    "print(f'Analysis output dir: {ANALYSIS_DIR}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_safe(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    if len(x) != len(y) or len(x) < 3:\n",
    "        return np.nan\n",
    "    sx = np.std(x)\n",
    "    sy = np.std(y)\n",
    "    if sx == 0 or sy == 0:\n",
    "        return np.nan\n",
    "    return float(np.corrcoef(x, y)[0, 1])\n",
    "\n",
    "\n",
    "def bh_adjust(pvals: np.ndarray) -> np.ndarray:\n",
    "    pvals = np.asarray(pvals, dtype=float)\n",
    "    out = np.full_like(pvals, np.nan)\n",
    "    mask = np.isfinite(pvals)\n",
    "    if not np.any(mask):\n",
    "        return out\n",
    "\n",
    "    p = pvals[mask]\n",
    "    m = len(p)\n",
    "    order = np.argsort(p)\n",
    "    ranked = p[order]\n",
    "    q = ranked * m / (np.arange(1, m + 1))\n",
    "    q = np.minimum.accumulate(q[::-1])[::-1]\n",
    "    q = np.clip(q, 0.0, 1.0)\n",
    "\n",
    "    restored = np.empty_like(q)\n",
    "    restored[order] = q\n",
    "    out[mask] = restored\n",
    "    return out\n",
    "\n",
    "\n",
    "def permutation_corr_pvalue(x: np.ndarray, y: np.ndarray, n_perm: int, rng: np.random.Generator) -> tuple[float, float]:\n",
    "    obs = corr_safe(x, y)\n",
    "    if not np.isfinite(obs):\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    null = np.empty(n_perm, dtype=float)\n",
    "    for i in range(n_perm):\n",
    "        null[i] = corr_safe(x, rng.permutation(y))\n",
    "\n",
    "    null = null[np.isfinite(null)]\n",
    "    if len(null) == 0:\n",
    "        return obs, np.nan\n",
    "\n",
    "    p = (np.sum(np.abs(null) >= abs(obs)) + 1) / (len(null) + 1)\n",
    "    return obs, float(p)\n",
    "\n",
    "\n",
    "def bootstrap_median_ci(values: np.ndarray, n_boot: int, rng: np.random.Generator, alpha: float = 0.05) -> tuple[float, float, float]:\n",
    "    values = np.asarray(values, dtype=float)\n",
    "    values = values[np.isfinite(values)]\n",
    "    if len(values) == 0:\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "    obs = float(np.median(values))\n",
    "    boot = np.empty(n_boot, dtype=float)\n",
    "    n = len(values)\n",
    "    for i in range(n_boot):\n",
    "        sample = rng.choice(values, size=n, replace=True)\n",
    "        boot[i] = np.median(sample)\n",
    "\n",
    "    lo = float(np.quantile(boot, alpha / 2))\n",
    "    hi = float(np.quantile(boot, 1 - alpha / 2))\n",
    "    return obs, lo, hi\n",
    "\n",
    "\n",
    "def sign_change_rate(x: np.ndarray) -> float:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if len(x) < 2:\n",
    "        return np.nan\n",
    "    s = np.sign(x)\n",
    "    valid = (s[:-1] != 0) & (s[1:] != 0)\n",
    "    if not np.any(valid):\n",
    "        return 0.0\n",
    "    changes = (s[:-1][valid] != s[1:][valid]).sum()\n",
    "    return float(changes / valid.sum())\n",
    "\n",
    "\n",
    "def ensure_required_artifacts() -> None:\n",
    "    required = [GLOBAL_PCA_PATH, GLOBAL_PCA_META_PATH, GLOBAL_PCA_VAR_PATH, METADATA_PATH]\n",
    "    missing = [p for p in required if not p.exists()]\n",
    "    if missing:\n",
    "        missing_txt = \"\\n\".join(f\"- {m}\" for m in missing)\n",
    "        raise FileNotFoundError(\n",
    "            \"Missing required upstream PCA artifacts. Run 02_transform_and_cluster.ipynb first.\\n\" + missing_txt\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_required_artifacts()\n",
    "\n",
    "meta = pd.read_csv(METADATA_PATH)\n",
    "meta['id'] = meta['id'].astype(int)\n",
    "meta = meta.sort_values('id').reset_index(drop=True)\n",
    "\n",
    "with np.load(GLOBAL_PCA_PATH) as pca_npz:\n",
    "    components = pca_npz['components']\n",
    "    explained_variance = pca_npz['explained_variance']\n",
    "    explained_variance_ratio = pca_npz['explained_variance_ratio']\n",
    "    singular_values = pca_npz['singular_values']\n",
    "    mean_vector = pca_npz['mean']\n",
    "\n",
    "pca_meta = json.loads(GLOBAL_PCA_META_PATH.read_text(encoding='utf-8'))\n",
    "variance_df = pd.read_csv(GLOBAL_PCA_VAR_PATH)\n",
    "\n",
    "print('Loaded global PCA artifacts:')\n",
    "print(f'- components: {components.shape}')\n",
    "print(f'- explained_variance_ratio: {explained_variance_ratio.shape}')\n",
    "print(f'- mean vector: {mean_vector.shape}')\n",
    "print(f'- metadata keys: {sorted(pca_meta.keys())}')\n",
    "\n",
    "integrity_rows = []\n",
    "integrity_rows.append({'check': 'components_rows_eq_5', 'passed': bool(components.shape[0] == 5), 'detail': str(components.shape)})\n",
    "integrity_rows.append({'check': 'mean_matches_embedding_dim', 'passed': bool(mean_vector.shape[0] == components.shape[1]), 'detail': f\"mean={mean_vector.shape[0]}, emb_dim={components.shape[1]}\"})\n",
    "integrity_rows.append({'check': 'evr_in_0_1', 'passed': bool(np.all((explained_variance_ratio >= 0) & (explained_variance_ratio <= 1))), 'detail': f\"min={float(np.min(explained_variance_ratio)):.6f}, max={float(np.max(explained_variance_ratio)):.6f}\"})\n",
    "integrity_rows.append({'check': 'evr_sum_positive', 'passed': bool(float(np.sum(explained_variance_ratio)) > 0), 'detail': f\"sum={float(np.sum(explained_variance_ratio)):.6f}\"})\n",
    "integrity_rows.append({'check': 'evr_cumulative_monotonic', 'passed': bool(np.all(np.diff(variance_df['cumulative_explained_variance_ratio'].to_numpy()) >= -1e-12)), 'detail': 'checked global_pca_variance_summary.csv'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_records = []\n",
    "artifact_issue_rows = []\n",
    "\n",
    "for _, mrow in meta.iterrows():\n",
    "    book_id = int(mrow['id'])\n",
    "    processed_dir = str(mrow['processed_dir'])\n",
    "    bdir = PROCESSED_DIR / processed_dir\n",
    "\n",
    "    record = {\n",
    "        'book_id': book_id,\n",
    "        'processed_dir': processed_dir,\n",
    "        'title': str(mrow.get('title', '')),\n",
    "        'genre_primary': str(mrow.get('genre_primary', '')),\n",
    "        'book_dir': bdir,\n",
    "    }\n",
    "\n",
    "    pca_path = bdir / 'pca_d5.npy'\n",
    "    chunks_path = bdir / 'chunks.jsonl'\n",
    "\n",
    "    if not pca_path.exists():\n",
    "        artifact_issue_rows.append({'book_id': book_id, 'processed_dir': processed_dir, 'issue': 'missing_pca_d5', 'severity': 'error'})\n",
    "        continue\n",
    "    if not chunks_path.exists():\n",
    "        artifact_issue_rows.append({'book_id': book_id, 'processed_dir': processed_dir, 'issue': 'missing_chunks_jsonl', 'severity': 'error'})\n",
    "        continue\n",
    "\n",
    "    z5 = np.load(pca_path)\n",
    "    if z5.ndim != 2 or z5.shape[1] != 5:\n",
    "        artifact_issue_rows.append({'book_id': book_id, 'processed_dir': processed_dir, 'issue': f'invalid_pca_shape_{z5.shape}', 'severity': 'error'})\n",
    "        continue\n",
    "\n",
    "    chunk_previews = []\n",
    "    with open(chunks_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            chunk_previews.append(str(obj.get('text_preview', '')))\n",
    "\n",
    "    if len(chunk_previews) != len(z5):\n",
    "        artifact_issue_rows.append({'book_id': book_id, 'processed_dir': processed_dir, 'issue': f'chunks_vs_pca_length_mismatch_{len(chunk_previews)}_{len(z5)}', 'severity': 'error'})\n",
    "        continue\n",
    "\n",
    "    signals = {}\n",
    "    for k in k_values:\n",
    "        sp = bdir / f'signals_k{k}.npz'\n",
    "        if not sp.exists():\n",
    "            artifact_issue_rows.append({'book_id': book_id, 'processed_dir': processed_dir, 'issue': f'missing_signals_k{k}', 'severity': 'warning'})\n",
    "            signals[k] = None\n",
    "            continue\n",
    "\n",
    "        dat = np.load(sp)\n",
    "        s = dat['s']\n",
    "        a = dat['a']\n",
    "        if len(s) != len(z5) or len(a) != len(z5):\n",
    "            artifact_issue_rows.append({'book_id': book_id, 'processed_dir': processed_dir, 'issue': f'signal_length_mismatch_k{k}_T{len(z5)}_s{len(s)}_a{len(a)}', 'severity': 'warning'})\n",
    "            signals[k] = None\n",
    "            continue\n",
    "\n",
    "        signals[k] = {'s': s.astype(np.float64), 'a': a.astype(np.float64)}\n",
    "\n",
    "    record['z5'] = z5.astype(np.float64)\n",
    "    record['chunk_previews'] = chunk_previews\n",
    "    record['signals'] = signals\n",
    "    book_records.append(record)\n",
    "\n",
    "artifact_issues_df = pd.DataFrame(artifact_issue_rows)\n",
    "if artifact_issues_df.empty:\n",
    "    artifact_issues_df = pd.DataFrame(columns=['book_id', 'processed_dir', 'issue', 'severity'])\n",
    "artifact_issues_path = TABLE_DIR / 'book_artifact_integrity.csv'\n",
    "artifact_issues_df.to_csv(artifact_issues_path, index=False)\n",
    "\n",
    "print(f'Books loaded for PCA analysis: {len(book_records)}')\n",
    "print(f'Artifact issues saved: {artifact_issues_path}')\n",
    "display(artifact_issues_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus-level PCA diagnostics\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "variance_plot_df = pd.DataFrame({\n",
    "    'pc': [f'PC{i}' for i in range(1, len(explained_variance_ratio) + 1)],\n",
    "    'explained_variance_ratio': explained_variance_ratio,\n",
    "    'cumulative_explained_variance_ratio': np.cumsum(explained_variance_ratio),\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax[0].bar(variance_plot_df['pc'], variance_plot_df['explained_variance_ratio'], color='#2a9d8f')\n",
    "ax[0].set_title('PCA Explained Variance Ratio (PC1-PC5)')\n",
    "ax[0].set_ylabel('Explained Variance Ratio')\n",
    "\n",
    "ax[1].plot(variance_plot_df['pc'], variance_plot_df['cumulative_explained_variance_ratio'], marker='o', color='#e76f51')\n",
    "ax[1].set_title('Cumulative Explained Variance (PC1-PC5)')\n",
    "ax[1].set_ylabel('Cumulative EVR')\n",
    "ax[1].set_ylim(0, 1)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'pca_variance_diagnostics.png', dpi=160, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "score_rows = []\n",
    "book_mean_rows = []\n",
    "for rec in book_records:\n",
    "    z = rec['z5']\n",
    "    pos = np.linspace(0.0, 1.0, len(z))\n",
    "    for i in range(len(z)):\n",
    "        for pc in range(5):\n",
    "            score_rows.append({\n",
    "                'book_id': rec['book_id'],\n",
    "                'processed_dir': rec['processed_dir'],\n",
    "                'title': rec['title'],\n",
    "                'genre_primary': rec['genre_primary'],\n",
    "                'chunk_index': i,\n",
    "                'position_norm': float(pos[i]),\n",
    "                'pc': f'PC{pc+1}',\n",
    "                'score': float(z[i, pc]),\n",
    "            })\n",
    "\n",
    "    book_mean_rows.append({\n",
    "        'book_id': rec['book_id'],\n",
    "        'processed_dir': rec['processed_dir'],\n",
    "        'title': rec['title'],\n",
    "        'genre_primary': rec['genre_primary'],\n",
    "        **{f'mean_pc{pc+1}': float(np.mean(z[:, pc])) for pc in range(5)}\n",
    "    })\n",
    "\n",
    "score_long_df = pd.DataFrame(score_rows)\n",
    "book_mean_df = pd.DataFrame(book_mean_rows)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.boxplot(data=score_long_df, x='pc', y='score', color='#8ecae6', fliersize=1)\n",
    "plt.title('Chunk-Level PCA Score Distribution by Component')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'component_score_distributions.png', dpi=160, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "pair_df = book_mean_df[['genre_primary', 'mean_pc1', 'mean_pc2', 'mean_pc3']].copy()\n",
    "pair_df = pair_df.rename(columns={'mean_pc1': 'PC1', 'mean_pc2': 'PC2', 'mean_pc3': 'PC3'})\n",
    "g = sns.pairplot(pair_df, vars=['PC1', 'PC2', 'PC3'], hue='genre_primary', diag_kind='hist', plot_kws={'alpha': 0.7, 's': 40})\n",
    "g.fig.suptitle('Book-Level Mean PCA Scores by Genre (PC1-PC3)', y=1.02)\n",
    "g.savefig(FIG_DIR / 'component_pairwise_by_genre.png', dpi=160, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal trend significance with permutation tests + BH correction by component family\n",
    "trend_rows = []\n",
    "for rec in book_records:\n",
    "    z = rec['z5']\n",
    "    pos = np.linspace(0.0, 1.0, len(z))\n",
    "    for pc in range(5):\n",
    "        corr_obs, p_perm = permutation_corr_pvalue(z[:, pc], pos, n_perm=n_perm, rng=rng)\n",
    "        trend_rows.append({\n",
    "            'book_id': rec['book_id'],\n",
    "            'processed_dir': rec['processed_dir'],\n",
    "            'title': rec['title'],\n",
    "            'genre_primary': rec['genre_primary'],\n",
    "            'pc': f'PC{pc+1}',\n",
    "            'corr_pc_position': corr_obs,\n",
    "            'perm_pvalue': p_perm,\n",
    "        })\n",
    "\n",
    "trend_df = pd.DataFrame(trend_rows)\n",
    "trend_df['perm_qvalue'] = np.nan\n",
    "for pc in sorted(trend_df['pc'].unique()):\n",
    "    mask = trend_df['pc'] == pc\n",
    "    trend_df.loc[mask, 'perm_qvalue'] = bh_adjust(trend_df.loc[mask, 'perm_pvalue'].to_numpy())\n",
    "\n",
    "trend_df.to_csv(TABLE_DIR / 'temporal_trend_stats.csv', index=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(data=trend_df, x='pc', y='corr_pc_position', color='#f4a261')\n",
    "plt.axhline(0.0, color='black', linewidth=1)\n",
    "plt.title('Per-Book Temporal Trend Correlations by PCA Component')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'temporal_trend_summary.png', dpi=160, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "display(trend_df.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component semantics via constrained exemplar selection\n",
    "candidate_rows = []\n",
    "for rec in book_records:\n",
    "    z = rec['z5']\n",
    "    for i, preview in enumerate(rec['chunk_previews']):\n",
    "        for pc in range(5):\n",
    "            candidate_rows.append({\n",
    "                'book_id': rec['book_id'],\n",
    "                'processed_dir': rec['processed_dir'],\n",
    "                'title': rec['title'],\n",
    "                'genre_primary': rec['genre_primary'],\n",
    "                'chunk_index': i,\n",
    "                'pc': f'PC{pc+1}',\n",
    "                'score': float(z[i, pc]),\n",
    "                'text_preview': preview,\n",
    "            })\n",
    "\n",
    "cand_df = pd.DataFrame(candidate_rows)\n",
    "\n",
    "\n",
    "def select_exemplars(df: pd.DataFrame, direction: str, top_n: int, min_dist: int, max_per_book: int) -> pd.DataFrame:\n",
    "    if direction == 'positive':\n",
    "        ordered = df.sort_values('score', ascending=False)\n",
    "    else:\n",
    "        ordered = df.sort_values('score', ascending=True)\n",
    "\n",
    "    selected = []\n",
    "    per_book_count = {}\n",
    "    per_book_indices = {}\n",
    "\n",
    "    for _, row in ordered.iterrows():\n",
    "        bid = int(row['book_id'])\n",
    "        idx = int(row['chunk_index'])\n",
    "\n",
    "        if per_book_count.get(bid, 0) >= max_per_book:\n",
    "            continue\n",
    "\n",
    "        prev_idx = per_book_indices.get(bid, [])\n",
    "        if any(abs(idx - pidx) < min_dist for pidx in prev_idx):\n",
    "            continue\n",
    "\n",
    "        selected.append(row.to_dict())\n",
    "        per_book_count[bid] = per_book_count.get(bid, 0) + 1\n",
    "        per_book_indices.setdefault(bid, []).append(idx)\n",
    "\n",
    "        if len(selected) >= top_n:\n",
    "            break\n",
    "\n",
    "    out = pd.DataFrame(selected)\n",
    "    if not out.empty:\n",
    "        out['direction'] = direction\n",
    "    return out\n",
    "\n",
    "\n",
    "exemplar_frames = []\n",
    "for pc in [f'PC{i}' for i in range(1, 6)]:\n",
    "    pc_df = cand_df[cand_df['pc'] == pc]\n",
    "    pos = select_exemplars(pc_df, 'positive', top_exemplars_per_direction, min_exemplar_distance, max_exemplars_per_book)\n",
    "    neg = select_exemplars(pc_df, 'negative', top_exemplars_per_direction, min_exemplar_distance, max_exemplars_per_book)\n",
    "    exemplar_frames.extend([pos, neg])\n",
    "\n",
    "exemplar_df = pd.concat([f for f in exemplar_frames if not f.empty], ignore_index=True)\n",
    "exemplar_df = exemplar_df[['book_id', 'processed_dir', 'title', 'genre_primary', 'chunk_index', 'pc', 'direction', 'score', 'text_preview']]\n",
    "exemplar_df.to_csv(TABLE_DIR / 'component_exemplar_chunks.csv', index=False)\n",
    "\n",
    "print(f'Saved: {TABLE_DIR / \"component_exemplar_chunks.csv\"}')\n",
    "display(exemplar_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Book-level PCA metrics and signal associations (k=5,7,11)\n",
    "book_stats_rows = []\n",
    "assoc_rows = []\n",
    "assoc_issue_rows = []\n",
    "\n",
    "for rec in book_records:\n",
    "    z = rec['z5']\n",
    "    dz = np.diff(z, axis=0)\n",
    "    speed = np.linalg.norm(dz, axis=1)\n",
    "    pos = np.linspace(0.0, 1.0, len(z))\n",
    "\n",
    "    row = {\n",
    "        'book_id': rec['book_id'],\n",
    "        'processed_dir': rec['processed_dir'],\n",
    "        'title': rec['title'],\n",
    "        'genre_primary': rec['genre_primary'],\n",
    "        'T': len(z),\n",
    "        'mean_speed': float(np.mean(speed)) if len(speed) else np.nan,\n",
    "        'p95_speed': float(np.quantile(speed, 0.95)) if len(speed) else np.nan,\n",
    "        'speed_std': float(np.std(speed)) if len(speed) else np.nan,\n",
    "    }\n",
    "\n",
    "    for pc in range(5):\n",
    "        x = z[:, pc]\n",
    "        row[f'mean_pc{pc+1}'] = float(np.mean(x))\n",
    "        row[f'std_pc{pc+1}'] = float(np.std(x))\n",
    "        row[f'corr_pc{pc+1}_position'] = corr_safe(x, pos)\n",
    "        row[f'sign_change_rate_pc{pc+1}'] = sign_change_rate(x)\n",
    "\n",
    "    row['sign_change_rate_mean'] = float(np.nanmean([row[f'sign_change_rate_pc{i}'] for i in range(1, 6)]))\n",
    "    book_stats_rows.append(row)\n",
    "\n",
    "    for k in k_values:\n",
    "        sig = rec['signals'].get(k)\n",
    "        if sig is None:\n",
    "            assoc_issue_rows.append({'book_id': rec['book_id'], 'processed_dir': rec['processed_dir'], 'k': k, 'issue': 'missing_or_invalid_signal', 'severity': 'warning'})\n",
    "            continue\n",
    "\n",
    "        s = sig['s']\n",
    "        a = sig['a']\n",
    "        if len(speed) != len(s) - 1 or len(speed) != len(a) - 1:\n",
    "            assoc_issue_rows.append({'book_id': rec['book_id'], 'processed_dir': rec['processed_dir'], 'k': k, 'issue': 'length_mismatch_speed_signal', 'severity': 'warning'})\n",
    "            continue\n",
    "\n",
    "        assoc_rows.append({\n",
    "            'book_id': rec['book_id'],\n",
    "            'processed_dir': rec['processed_dir'],\n",
    "            'title': rec['title'],\n",
    "            'genre_primary': rec['genre_primary'],\n",
    "            'k': k,\n",
    "            'T': len(z),\n",
    "            'corr_speed_s': corr_safe(speed, s[1:]),\n",
    "            'corr_speed_a': corr_safe(speed, a[1:]),\n",
    "            'mean_speed': float(np.mean(speed)),\n",
    "            'p95_speed': float(np.quantile(speed, 0.95)),\n",
    "        })\n",
    "\n",
    "book_stats_df = pd.DataFrame(book_stats_rows)\n",
    "book_stats_df.to_csv(TABLE_DIR / 'book_component_stats.csv', index=False)\n",
    "\n",
    "assoc_df = pd.DataFrame(assoc_rows)\n",
    "if assoc_df.empty:\n",
    "    assoc_df = pd.DataFrame(columns=['book_id', 'processed_dir', 'title', 'genre_primary', 'k', 'T', 'corr_speed_s', 'corr_speed_a', 'mean_speed', 'p95_speed'])\n",
    "assoc_df.to_csv(TABLE_DIR / 'book_component_signal_assoc.csv', index=False)\n",
    "\n",
    "assoc_issues_df = pd.DataFrame(assoc_issue_rows)\n",
    "if assoc_issues_df.empty:\n",
    "    assoc_issues_df = pd.DataFrame(columns=['book_id', 'processed_dir', 'k', 'issue', 'severity'])\n",
    "assoc_issues_df.to_csv(TABLE_DIR / 'book_signal_assoc_issues.csv', index=False)\n",
    "\n",
    "# Genre-level component association with effect-size style normalization\n",
    "long_rows = []\n",
    "for _, r in book_stats_df.iterrows():\n",
    "    for pc in range(1, 6):\n",
    "        long_rows.append({\n",
    "            'book_id': int(r['book_id']),\n",
    "            'processed_dir': r['processed_dir'],\n",
    "            'genre_primary': r['genre_primary'],\n",
    "            'pc': f'PC{pc}',\n",
    "            'mean_score': float(r[f'mean_pc{pc}']),\n",
    "        })\n",
    "\n",
    "long_component_df = pd.DataFrame(long_rows)\n",
    "corpus_stats = long_component_df.groupby('pc')['mean_score'].agg(['mean', 'std']).reset_index().rename(columns={'mean': 'corpus_mean', 'std': 'corpus_std'})\n",
    "genre_stats = long_component_df.groupby(['genre_primary', 'pc'])['mean_score'].agg(['mean', 'count']).reset_index().rename(columns={'mean': 'genre_mean', 'count': 'genre_book_count'})\n",
    "component_genre_assoc = genre_stats.merge(corpus_stats, on='pc', how='left')\n",
    "component_genre_assoc['delta_vs_corpus_mean'] = component_genre_assoc['genre_mean'] - component_genre_assoc['corpus_mean']\n",
    "component_genre_assoc['effect_size_vs_corpus'] = component_genre_assoc['delta_vs_corpus_mean'] / component_genre_assoc['corpus_std'].replace(0, np.nan)\n",
    "component_genre_assoc.to_csv(TABLE_DIR / 'component_genre_association.csv', index=False)\n",
    "\n",
    "print(f'Saved: {TABLE_DIR / \"book_component_stats.csv\"}')\n",
    "print(f'Saved: {TABLE_DIR / \"book_component_signal_assoc.csv\"}')\n",
    "print(f'Saved: {TABLE_DIR / \"component_genre_association.csv\"}')\n",
    "display(book_stats_df.head(10))\n",
    "display(assoc_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moderate rigor checks: bootstrap CIs for corpus-level median signal associations\n",
    "boot_rows = []\n",
    "for k in sorted(assoc_df['k'].unique()):\n",
    "    sub = assoc_df[assoc_df['k'] == k]\n",
    "\n",
    "    med_s, lo_s, hi_s = bootstrap_median_ci(sub['corr_speed_s'].to_numpy(), n_boot=n_boot, rng=rng)\n",
    "    med_a, lo_a, hi_a = bootstrap_median_ci(sub['corr_speed_a'].to_numpy(), n_boot=n_boot, rng=rng)\n",
    "\n",
    "    boot_rows.append({\n",
    "        'k': int(k),\n",
    "        'metric': 'corr_speed_s',\n",
    "        'median': med_s,\n",
    "        'ci_lower': lo_s,\n",
    "        'ci_upper': hi_s,\n",
    "        'n_books': int(sub['book_id'].nunique()),\n",
    "    })\n",
    "    boot_rows.append({\n",
    "        'k': int(k),\n",
    "        'metric': 'corr_speed_a',\n",
    "        'median': med_a,\n",
    "        'ci_lower': lo_a,\n",
    "        'ci_upper': hi_a,\n",
    "        'n_books': int(sub['book_id'].nunique()),\n",
    "    })\n",
    "\n",
    "boot_df = pd.DataFrame(boot_rows)\n",
    "boot_df.to_csv(TABLE_DIR / 'corpus_assoc_bootstrap.csv', index=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plot_df = boot_df.copy()\n",
    "plot_df['label'] = plot_df['metric'] + '_k' + plot_df['k'].astype(str)\n",
    "plot_df = plot_df.sort_values(['metric', 'k'])\n",
    "plt.errorbar(\n",
    "    x=np.arange(len(plot_df)),\n",
    "    y=plot_df['median'],\n",
    "    yerr=[plot_df['median'] - plot_df['ci_lower'], plot_df['ci_upper'] - plot_df['median']],\n",
    "    fmt='o',\n",
    "    capsize=4,\n",
    ")\n",
    "plt.xticks(np.arange(len(plot_df)), plot_df['label'], rotation=45, ha='right')\n",
    "plt.title('Bootstrap 95% CI: Corpus Median PCA-Speed / Signal Associations')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'bootstrap_assoc_summary.png', dpi=160, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Book deep-dive figure (primary k=7)\n",
    "k7 = assoc_df[assoc_df['k'] == primary_k].copy()\n",
    "if not k7.empty:\n",
    "    top_books = k7.sort_values('corr_speed_a', ascending=False).head(4)['book_id'].tolist()\n",
    "    fig, axes = plt.subplots(len(top_books), 1, figsize=(12, 3.5 * len(top_books)), sharex=False)\n",
    "    if len(top_books) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    lookup = {r['book_id']: r for r in book_records}\n",
    "    for ax, bid in zip(axes, top_books):\n",
    "        rec = lookup[int(bid)]\n",
    "        z = rec['z5']\n",
    "        speed = np.linalg.norm(np.diff(z, axis=0), axis=1)\n",
    "        sig = rec['signals'][primary_k]\n",
    "\n",
    "        ax.plot(np.arange(1, len(speed) + 1), speed, label='pca_speed', linewidth=1.2)\n",
    "        ax.plot(sig['s'], label='s_t', linewidth=1.0, alpha=0.8)\n",
    "        ax.plot(sig['a'], label='a_t', linewidth=1.0, alpha=0.8)\n",
    "        ax.set_title(f\"Book {rec['book_id']} | {rec['title']} (k={primary_k})\")\n",
    "        ax.set_xlabel('Chunk index')\n",
    "        ax.legend(loc='upper right', ncol=3)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(FIG_DIR / 'book_deep_dive_speed_signal_k7.png', dpi=160, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection consistency checks against stored pca_d5.npy\n",
    "sample_books = [r['book_id'] for r in sorted(book_records, key=lambda x: x['book_id'])[:5]]\n",
    "proj_rows = []\n",
    "\n",
    "for bid in sample_books:\n",
    "    rec = next(r for r in book_records if r['book_id'] == bid)\n",
    "    emb_path = rec['book_dir'] / 'embeddings.npy'\n",
    "    if not emb_path.exists():\n",
    "        proj_rows.append({'book_id': bid, 'processed_dir': rec['processed_dir'], 'checked': False, 'max_abs_diff': np.nan, 'mean_abs_diff': np.nan, 'detail': 'missing_embeddings.npy'})\n",
    "        continue\n",
    "\n",
    "    emb = np.load(emb_path).astype(np.float64)\n",
    "    z_recomputed = (emb - mean_vector.astype(np.float64)) @ components.astype(np.float64).T\n",
    "    z_saved = rec['z5']\n",
    "\n",
    "    if z_recomputed.shape != z_saved.shape:\n",
    "        proj_rows.append({'book_id': bid, 'processed_dir': rec['processed_dir'], 'checked': False, 'max_abs_diff': np.nan, 'mean_abs_diff': np.nan, 'detail': f'shape_mismatch_recomputed_{z_recomputed.shape}_saved_{z_saved.shape}'})\n",
    "        continue\n",
    "\n",
    "    diff = np.abs(z_recomputed - z_saved)\n",
    "    proj_rows.append({\n",
    "        'book_id': bid,\n",
    "        'processed_dir': rec['processed_dir'],\n",
    "        'checked': True,\n",
    "        'max_abs_diff': float(np.max(diff)),\n",
    "        'mean_abs_diff': float(np.mean(diff)),\n",
    "        'detail': 'ok',\n",
    "    })\n",
    "\n",
    "projection_df = pd.DataFrame(proj_rows)\n",
    "projection_df.to_csv(TABLE_DIR / 'projection_consistency_checks.csv', index=False)\n",
    "\n",
    "# Integrity checks required by plan\n",
    "required_assoc_rows = len(book_records) * len(k_values)\n",
    "assoc_rows_actual = len(assoc_df)\n",
    "assoc_no_nan = bool(assoc_df[['corr_speed_s', 'corr_speed_a', 'mean_speed', 'p95_speed']].isna().sum().sum() == 0) if not assoc_df.empty else False\n",
    "\n",
    "pvals_ok = bool(((trend_df['perm_pvalue'].dropna() >= 0) & (trend_df['perm_pvalue'].dropna() <= 1)).all())\n",
    "qvals_ok = bool(((trend_df['perm_qvalue'].dropna() >= 0) & (trend_df['perm_qvalue'].dropna() <= 1)).all())\n",
    "ci_order_ok = bool((boot_df['ci_lower'] <= boot_df['ci_upper']).all()) if not boot_df.empty else False\n",
    "\n",
    "ex_counts = exemplar_df.groupby(['pc', 'direction']).size().reset_index(name='n')\n",
    "expected_pairs = {(f'PC{i}', d) for i in range(1, 6) for d in ['positive', 'negative']}\n",
    "actual_pairs = set(zip(ex_counts['pc'], ex_counts['direction']))\n",
    "all_pairs_present = expected_pairs.issubset(actual_pairs)\n",
    "\n",
    "# De-dup enforcement checks\n",
    "rule_violations = []\n",
    "for (pc, direction, book_id), grp in exemplar_df.groupby(['pc', 'direction', 'book_id']):\n",
    "    if len(grp) > max_exemplars_per_book:\n",
    "        rule_violations.append(f'{pc}-{direction}-book{book_id}:too_many')\n",
    "    idx_sorted = sorted(grp['chunk_index'].astype(int).tolist())\n",
    "    for i in range(1, len(idx_sorted)):\n",
    "        if abs(idx_sorted[i] - idx_sorted[i - 1]) < min_exemplar_distance:\n",
    "            rule_violations.append(f'{pc}-{direction}-book{book_id}:distance_violation')\n",
    "\n",
    "end_to_end_files = [\n",
    "    TABLE_DIR / 'book_component_stats.csv',\n",
    "    TABLE_DIR / 'book_component_signal_assoc.csv',\n",
    "    TABLE_DIR / 'component_exemplar_chunks.csv',\n",
    "    TABLE_DIR / 'component_genre_association.csv',\n",
    "    TABLE_DIR / 'temporal_trend_stats.csv',\n",
    "    TABLE_DIR / 'corpus_assoc_bootstrap.csv',\n",
    "    TABLE_DIR / 'projection_consistency_checks.csv',\n",
    "]\n",
    "\n",
    "integrity_rows.extend([\n",
    "    {'check': 'projection_consistency_checked_books', 'passed': bool(projection_df['checked'].all()), 'detail': f\"rows={len(projection_df)}\"},\n",
    "    {'check': 'projection_consistency_max_abs_diff_lt_1e-4', 'passed': bool((projection_df['max_abs_diff'].dropna() < 1e-4).all()), 'detail': f\"max={projection_df['max_abs_diff'].dropna().max() if not projection_df['max_abs_diff'].dropna().empty else np.nan}\"},\n",
    "    {'check': 'association_row_count_full_coverage', 'passed': bool(assoc_rows_actual == required_assoc_rows), 'detail': f\"actual={assoc_rows_actual}, expected={required_assoc_rows}\"},\n",
    "    {'check': 'association_no_nan_core_numeric', 'passed': assoc_no_nan, 'detail': 'corr_speed_s/corr_speed_a/mean_speed/p95_speed'},\n",
    "    {'check': 'perm_pvalues_in_0_1', 'passed': pvals_ok, 'detail': 'trend_df perm_pvalue bounds'},\n",
    "    {'check': 'perm_qvalues_in_0_1', 'passed': qvals_ok, 'detail': 'trend_df perm_qvalue bounds'},\n",
    "    {'check': 'bootstrap_ci_ordered', 'passed': ci_order_ok, 'detail': 'ci_lower <= ci_upper'},\n",
    "    {'check': 'exemplar_pairs_present_pc1to5_posneg', 'passed': all_pairs_present, 'detail': f\"pairs={len(actual_pairs)}\"},\n",
    "    {'check': 'exemplar_dedup_rules_enforced', 'passed': bool(len(rule_violations) == 0), 'detail': ';'.join(rule_violations[:5]) if rule_violations else 'ok'},\n",
    "    {'check': 'end_to_end_required_files_exist', 'passed': bool(all(p.exists() for p in end_to_end_files)), 'detail': 'verified table outputs'},\n",
    "])\n",
    "\n",
    "integrity_df = pd.DataFrame(integrity_rows)\n",
    "integrity_df.to_csv(TABLE_DIR / 'pca_integrity_checks.csv', index=False)\n",
    "\n",
    "display(projection_df)\n",
    "display(integrity_df)\n",
    "print(f'Saved: {TABLE_DIR / \"pca_integrity_checks.csv\"}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build narrative insights markdown\n",
    "k7 = assoc_df[assoc_df['k'] == primary_k].copy()\n",
    "\n",
    "if k7.empty:\n",
    "    raise RuntimeError('No k=7 association rows available; cannot build insights narrative.')\n",
    "\n",
    "high_vol = book_stats_df.sort_values('mean_speed', ascending=False).head(3)\n",
    "high_coupling = k7.sort_values('corr_speed_a', ascending=False).head(3)\n",
    "\n",
    "trend_abs = trend_df.copy()\n",
    "trend_abs['abs_corr'] = trend_abs['corr_pc_position'].abs()\n",
    "atypical = trend_abs.sort_values('abs_corr', ascending=False).head(3)\n",
    "\n",
    "sens = assoc_df.groupby('k')[['corr_speed_s', 'corr_speed_a']].median().reset_index()\n",
    "\n",
    "lines = []\n",
    "lines.append('# PCA Component Insights (Global + Book)')\n",
    "lines.append('')\n",
    "lines.append('## Corpus-Level Component Diagnostics')\n",
    "for i, evr in enumerate(explained_variance_ratio, start=1):\n",
    "    lines.append(f'- PC{i} explained_variance_ratio={evr:.4f} | cumulative={np.sum(explained_variance_ratio[:i]):.4f}')\n",
    "lines.append('')\n",
    "\n",
    "lines.append('## Component Semantics (Exemplar-Based)')\n",
    "for pc in [f'PC{i}' for i in range(1, 6)]:\n",
    "    pos = exemplar_df[(exemplar_df['pc'] == pc) & (exemplar_df['direction'] == 'positive')].head(2)\n",
    "    neg = exemplar_df[(exemplar_df['pc'] == pc) & (exemplar_df['direction'] == 'negative')].head(2)\n",
    "    lines.append(f'- {pc}:')\n",
    "    for _, r in pos.iterrows():\n",
    "        preview = ' '.join(str(r['text_preview']).split())[:140]\n",
    "        lines.append(f\"  + positive exemplar: {r['book_id']} | {r['title']} | score={r['score']:.3f} | {preview}\")\n",
    "    for _, r in neg.iterrows():\n",
    "        preview = ' '.join(str(r['text_preview']).split())[:140]\n",
    "        lines.append(f\"  - negative exemplar: {r['book_id']} | {r['title']} | score={r['score']:.3f} | {preview}\")\n",
    "lines.append('')\n",
    "\n",
    "lines.append('## Book-Level Highlights')\n",
    "lines.append('Highest PCA trajectory volatility (mean_speed):')\n",
    "for _, r in high_vol.iterrows():\n",
    "    lines.append(f\"- {int(r['book_id'])} | {r['title']} | mean_speed={r['mean_speed']:.4f} | p95_speed={r['p95_speed']:.4f}\")\n",
    "lines.append('')\n",
    "\n",
    "lines.append(f'Strongest PCA-speed / acceleration coupling (k={primary_k}):')\n",
    "for _, r in high_coupling.iterrows():\n",
    "    lines.append(f\"- {int(r['book_id'])} | {r['title']} | corr_speed_a={r['corr_speed_a']:.4f} | corr_speed_s={r['corr_speed_s']:.4f}\")\n",
    "lines.append('')\n",
    "\n",
    "lines.append('Most atypical temporal component trends (|corr(PC, position)|):')\n",
    "for _, r in atypical.iterrows():\n",
    "    lines.append(f\"- {int(r['book_id'])} | {r['title']} | {r['pc']} corr={r['corr_pc_position']:.4f} | q={r['perm_qvalue']:.4f}\")\n",
    "lines.append('')\n",
    "\n",
    "lines.append('## Sensitivity Across k = 5, 7, 11')\n",
    "for _, r in sens.iterrows():\n",
    "    lines.append(f\"- k={int(r['k'])}: median corr_speed_s={r['corr_speed_s']:.4f}, median corr_speed_a={r['corr_speed_a']:.4f}\")\n",
    "lines.append('')\n",
    "\n",
    "lines.append('## Caveats')\n",
    "lines.append('- PCA components are derived from embedding geometry and require semantic triangulation with text exemplars.')\n",
    "lines.append('- Association metrics are correlational and do not establish causal narrative mechanisms.')\n",
    "lines.append('- Missing/invalid signal artifacts are skipped and logged in integrity tables.')\n",
    "lines.append('')\n",
    "\n",
    "insights_path = ANALYSIS_DIR / 'insights.md'\n",
    "insights_path.write_text(\"\\n\".join(lines) + \"\\n\", encoding='utf-8')\n",
    "\n",
    "print(f'Saved: {insights_path}')\n",
    "print(\"\\n\".join(lines[:40]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
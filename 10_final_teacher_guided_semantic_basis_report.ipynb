{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Final Teacher-Guided Semantic Basis Projection Report\n",
    "\n",
    "This notebook builds the final report suite from existing project artifacts.\n",
    "Outputs:\n",
    "- `outputs/final_report/tables/*.csv`\n",
    "- `outputs/final_report/figures/*.png`\n",
    "- `docs/FINAL_REPORT.md`\n",
    "- `docs/OTHER_EXPERIMENTS.md`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import textwrap\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "SEED = 42\n",
    "MA_WINDOW = 5\n",
    "EPS = 1e-8\n",
    "\n",
    "PROJECT_ROOT = Path('.').resolve()\n",
    "OUTPUTS_ROOT = PROJECT_ROOT / 'outputs'\n",
    "DOCS_ROOT = PROJECT_ROOT / 'docs'\n",
    "\n",
    "FINAL_OUT = OUTPUTS_ROOT / 'final_report'\n",
    "FINAL_FIG = FINAL_OUT / 'figures'\n",
    "FINAL_TAB = FINAL_OUT / 'tables'\n",
    "for d in [FINAL_OUT, FINAL_FIG, FINAL_TAB, DOCS_ROOT]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SRC_VARIANT_TAB = OUTPUTS_ROOT / 'excitement_variant_analysis' / 'tables'\n",
    "SRC_VARIANT_FIG = OUTPUTS_ROOT / 'excitement_variant_analysis' / 'figures'\n",
    "SRC_CLUSTER_TAB = OUTPUTS_ROOT / 'excitement_indep_clustering' / 'tables'\n",
    "SRC_CLUSTER_FIG = OUTPUTS_ROOT / 'excitement_indep_clustering' / 'figures'\n",
    "SRC_LINEAR_TAB = OUTPUTS_ROOT / 'excitement_linear' / 'tables'\n",
    "\n",
    "print('PROJECT_ROOT:', PROJECT_ROOT)\n",
    "print('FINAL_OUT:', FINAL_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load source tables and build summary tables for report claims\n",
    "required_sources = {\n",
    "    'variant_global_metrics': SRC_VARIANT_TAB / 'model_global_metrics_by_variant.csv',\n",
    "    'variant_per_novel_metrics': SRC_VARIANT_TAB / 'model_per_novel_metrics_by_variant.csv',\n",
    "    'variant_pairwise': SRC_VARIANT_TAB / 'variant_pairwise_agreement_global.csv',\n",
    "    'cluster_quality': SRC_CLUSTER_TAB / 'cluster_quality_by_method.csv',\n",
    "    'cluster_profile': SRC_CLUSTER_TAB / 'cluster_profile_summary.csv',\n",
    "    'cluster_representatives': SRC_CLUSTER_TAB / 'cluster_representatives.csv',\n",
    "    'cluster_genre_prop': SRC_CLUSTER_TAB / 'genre_by_feature_cluster_proportions.csv',\n",
    "    'cluster_assign_feature': SRC_CLUSTER_TAB / 'cluster_assignments_feature.csv',\n",
    "    'cluster_agreement': SRC_CLUSTER_TAB / 'cluster_method_agreement.csv',\n",
    "    'split_manifest': SRC_LINEAR_TAB / 'split_manifest.csv',\n",
    "    'metadata': PROJECT_ROOT / 'data' / 'metadata.csv',\n",
    "    'ciw5_model_npz': OUTPUTS_ROOT / 'excitement_variant_analysis' / 'model' / 'linear_weights_indep_winsize_5.npz',\n",
    "}\n",
    "\n",
    "source_checks = []\n",
    "for name, path in required_sources.items():\n",
    "    exists = bool(path.exists())\n",
    "    source_checks.append({\n",
    "        'check': f'source_exists::{name}',\n",
    "        'expected': True,\n",
    "        'actual': exists,\n",
    "        'pass': exists,\n",
    "    })\n",
    "    if not exists:\n",
    "        raise FileNotFoundError(f'Missing required source: {path}')\n",
    "\n",
    "variant_df = pd.read_csv(required_sources['variant_global_metrics'])\n",
    "variant_per_book_df = pd.read_csv(required_sources['variant_per_novel_metrics'])\n",
    "pairwise_df = pd.read_csv(required_sources['variant_pairwise'])\n",
    "cluster_quality_df = pd.read_csv(required_sources['cluster_quality'])\n",
    "cluster_profile_df = pd.read_csv(required_sources['cluster_profile'])\n",
    "cluster_rep_df = pd.read_csv(required_sources['cluster_representatives'])\n",
    "cluster_genre_prop_df = pd.read_csv(required_sources['cluster_genre_prop'])\n",
    "cluster_assign_df = pd.read_csv(required_sources['cluster_assign_feature'])\n",
    "cluster_agree_df = pd.read_csv(required_sources['cluster_agreement'])\n",
    "split_manifest_df = pd.read_csv(required_sources['split_manifest'])\n",
    "metadata_df = pd.read_csv(required_sources['metadata'])\n",
    "ciw5_model_npz = np.load(required_sources['ciw5_model_npz'], allow_pickle=True)\n",
    "\n",
    "variant_name_map = {\n",
    "    'base': ('NC-1', 'No-Context Chunk Teacher Labels'),\n",
    "    'winsize_5': ('SW-5', 'Shared-Window Labels'),\n",
    "    'indep_winsize_5': ('CIW-5', 'Context-Window Independent Labels'),\n",
    "}\n",
    "\n",
    "# Dataset profile table for report reproducibility\n",
    "meta_cols = ['id', 'genre_primary']\n",
    "dataset_profile_df = (\n",
    "    split_manifest_df\n",
    "    .merge(metadata_df[meta_cols], left_on='book_id', right_on='id', how='left')\n",
    "    .drop(columns=['id'])\n",
    "    .rename(columns={'genre_primary': 'genre_primary'})\n",
    ")\n",
    "dataset_profile_df['genre_primary'] = dataset_profile_df['genre_primary'].fillna('Unknown')\n",
    "dataset_profile_df = dataset_profile_df[['book_id', 'title', 'processed_dir', 'genre_primary', 'T', 'split']].sort_values(\n",
    "    ['split', 'book_id']\n",
    ").reset_index(drop=True)\n",
    "dataset_profile_df.to_csv(FINAL_TAB / 'dataset_profile_for_report.csv', index=False)\n",
    "\n",
    "# Variant ranking diagnostics (trend-first is the locked selector)\n",
    "variant_summary = variant_df.copy()\n",
    "variant_summary['variant_code'] = variant_summary['variant'].map(lambda v: variant_name_map[v][0])\n",
    "variant_summary['variant_name'] = variant_summary['variant'].map(lambda v: variant_name_map[v][1])\n",
    "\n",
    "\n",
    "def assign_rank(df: pd.DataFrame, split_col: str, sort_cols: list[str], ascending: list[bool]) -> pd.Series:\n",
    "    out = pd.Series(index=df.index, dtype='int64')\n",
    "    for split_name, idx in df.groupby(split_col).groups.items():\n",
    "        ordered = df.loc[idx].sort_values(sort_cols, ascending=ascending, kind='mergesort')\n",
    "        out.loc[ordered.index] = np.arange(1, len(ordered) + 1)\n",
    "    return out.astype(int)\n",
    "\n",
    "\n",
    "variant_summary['rank_trend_primary'] = assign_rank(\n",
    "    variant_summary,\n",
    "    'split',\n",
    "    ['mae_ma', 'mae', 'rmse', 'variant_code'],\n",
    "    [True, True, True, True],\n",
    ")\n",
    "variant_summary['rank_raw_error'] = assign_rank(\n",
    "    variant_summary,\n",
    "    'split',\n",
    "    ['mae', 'rmse', 'mae_ma', 'variant_code'],\n",
    "    [True, True, True, True],\n",
    ")\n",
    "variant_summary['rank_corr'] = assign_rank(\n",
    "    variant_summary,\n",
    "    'split',\n",
    "    ['corr', 'r2', 'mae_ma', 'variant_code'],\n",
    "    [False, False, True, True],\n",
    ")\n",
    "\n",
    "variant_selection_summary = variant_summary[\n",
    "    ['variant_code', 'variant_name', 'split', 'rmse', 'mae', 'mae_ma', 'r2', 'corr', 'rank_trend_primary']\n",
    "].rename(columns={'mae_ma': 'mae_ma5'})\n",
    "variant_selection_summary.to_csv(FINAL_TAB / 'variant_selection_summary.csv', index=False)\n",
    "\n",
    "variant_selection_diagnostics = variant_summary[\n",
    "    ['variant_code', 'split', 'rmse', 'mae', 'mae_ma', 'r2', 'corr', 'rank_trend_primary', 'rank_raw_error', 'rank_corr']\n",
    "].rename(columns={'mae_ma': 'mae_ma5'}).sort_values(['split', 'rank_trend_primary', 'variant_code']).reset_index(drop=True)\n",
    "variant_selection_diagnostics.to_csv(FINAL_TAB / 'variant_selection_diagnostics.csv', index=False)\n",
    "\n",
    "# Selected variant by locked criterion on test split\n",
    "variant_test_rank = variant_selection_diagnostics[variant_selection_diagnostics['split'] == 'test'].sort_values(\n",
    "    ['rank_trend_primary', 'mae_ma5', 'mae', 'rmse', 'variant_code'],\n",
    "    ascending=[True, True, True, True, True],\n",
    "    kind='mergesort',\n",
    ")\n",
    "selected_variant_code = str(variant_test_rank.iloc[0]['variant_code'])\n",
    "\n",
    "# Per-book CIW-5 deep-dive table\n",
    "ciw5_deepdive_df = variant_per_book_df[variant_per_book_df['variant'] == 'indep_winsize_5'].copy()\n",
    "ciw5_deepdive_df['error_gap_raw_vs_ma'] = ciw5_deepdive_df['mae'] - ciw5_deepdive_df['mae_ma']\n",
    "ciw5_deepdive_df = ciw5_deepdive_df[\n",
    "    ['book_id', 'title', 'split', 'T', 'mse', 'rmse', 'mae', 'mae_ma', 'corr', 'r2', 'error_gap_raw_vs_ma']\n",
    "].sort_values(['split', 'book_id']).reset_index(drop=True)\n",
    "ciw5_deepdive_df.to_csv(FINAL_TAB / 'ciw5_per_book_deepdive.csv', index=False)\n",
    "\n",
    "# Determine selected clustering settings from quality tables (data-driven)\n",
    "feature_quality = cluster_quality_df[cluster_quality_df['branch'] == 'feature'].copy()\n",
    "feature_quality['_stability'] = feature_quality['kmeans_stability_ari'].fillna(-np.inf)\n",
    "feature_quality['_db'] = feature_quality['davies_bouldin'].fillna(np.inf)\n",
    "selected_feature_quality = feature_quality.sort_values(\n",
    "    ['silhouette', '_stability', '_db', 'k', 'method'],\n",
    "    ascending=[False, False, True, True, True],\n",
    "    kind='mergesort',\n",
    ").iloc[0]\n",
    "selected_feature_method = str(selected_feature_quality['method'])\n",
    "selected_feature_k = int(selected_feature_quality['k'])\n",
    "\n",
    "selected_assign_df = cluster_assign_df.copy()\n",
    "if {'method', 'k'}.issubset(selected_assign_df.columns):\n",
    "    filt = selected_assign_df[\n",
    "        (selected_assign_df['method'].astype(str) == selected_feature_method)\n",
    "        & (selected_assign_df['k'].astype(int) == selected_feature_k)\n",
    "    ]\n",
    "    if not filt.empty:\n",
    "        selected_assign_df = filt.copy()\n",
    "\n",
    "selected_dtw_quality = cluster_quality_df[cluster_quality_df['branch'] == 'dtw'].sort_values(\n",
    "    ['silhouette', 'k'], ascending=[False, True], kind='mergesort'\n",
    ").iloc[0]\n",
    "selected_dtw_k = int(selected_dtw_quality['k'])\n",
    "\n",
    "# Cluster summary table for report\n",
    "feature_profile = cluster_profile_df[cluster_profile_df['branch'] == 'feature'].copy()\n",
    "feature_top3 = feature_profile[feature_profile['rank_abs_delta'] <= 3].sort_values(['cluster', 'rank_abs_delta'])\n",
    "\n",
    "rep_centroid = cluster_rep_df[\n",
    "    (cluster_rep_df['branch'] == 'feature') & (cluster_rep_df['role'] == 'centroid_medoid')\n",
    "].copy()\n",
    "\n",
    "cluster_sizes = selected_assign_df['cluster'].value_counts().sort_index()\n",
    "cluster_rows = []\n",
    "for cluster_id in sorted(cluster_sizes.index.tolist()):\n",
    "    sub = feature_top3[feature_top3['cluster'] == cluster_id].sort_values('rank_abs_delta')\n",
    "    feats = sub['feature'].tolist()[:3]\n",
    "    feats = feats + [''] * (3 - len(feats))\n",
    "\n",
    "    rep_sub = rep_centroid[rep_centroid['cluster'] == cluster_id]\n",
    "    if rep_sub.empty:\n",
    "        rep_book = 'N/A'\n",
    "    else:\n",
    "        rep_row = rep_sub.iloc[0]\n",
    "        rep_book = f\"{int(rep_row['book_id'])} | {rep_row['title']}\"\n",
    "\n",
    "    genre_row = cluster_genre_prop_df[cluster_genre_prop_df['cluster'] == cluster_id]\n",
    "    if genre_row.empty:\n",
    "        dominant_genre = 'Unknown'\n",
    "        dominant_prop = float('nan')\n",
    "    else:\n",
    "        genre_row = genre_row.iloc[0]\n",
    "        genre_cols = [c for c in cluster_genre_prop_df.columns if c != 'cluster']\n",
    "        dominant_genre = max(genre_cols, key=lambda c: float(genre_row[c]))\n",
    "        dominant_prop = float(genre_row[dominant_genre])\n",
    "\n",
    "    cluster_rows.append({\n",
    "        'cluster': int(cluster_id),\n",
    "        'n_books': int(cluster_sizes.loc[cluster_id]),\n",
    "        'top_feature_1': feats[0],\n",
    "        'top_feature_2': feats[1],\n",
    "        'top_feature_3': feats[2],\n",
    "        'representative_book': rep_book,\n",
    "        'dominant_genre': dominant_genre,\n",
    "        'dominant_genre_prop': dominant_prop,\n",
    "    })\n",
    "\n",
    "cluster_summary_df = pd.DataFrame(cluster_rows).sort_values('cluster').reset_index(drop=True)\n",
    "cluster_summary_df.to_csv(FINAL_TAB / 'cluster_summary_for_report.csv', index=False)\n",
    "\n",
    "# Key results registry\n",
    "key_rows = []\n",
    "\n",
    "\n",
    "def add_metric(metric_key: str, value, source_file: str, source_row_filter: str, notes: str = ''):\n",
    "    key_rows.append({\n",
    "        'metric_key': metric_key,\n",
    "        'value': value,\n",
    "        'source_file': source_file,\n",
    "        'source_row_filter': source_row_filter,\n",
    "        'notes': notes,\n",
    "    })\n",
    "\n",
    "\n",
    "# Dataset and split metrics\n",
    "add_metric('corpus::n_books', int(dataset_profile_df['book_id'].nunique()), str(FINAL_TAB / 'dataset_profile_for_report.csv'), 'unique(book_id)')\n",
    "add_metric('corpus::n_chunks_total', int(dataset_profile_df['T'].sum()), str(FINAL_TAB / 'dataset_profile_for_report.csv'), 'sum(T)')\n",
    "add_metric('corpus::train_novels', int((dataset_profile_df['split'] == 'train').sum()), str(FINAL_TAB / 'dataset_profile_for_report.csv'), 'split=train,count_rows')\n",
    "add_metric('corpus::test_novels', int((dataset_profile_df['split'] == 'test').sum()), str(FINAL_TAB / 'dataset_profile_for_report.csv'), 'split=test,count_rows')\n",
    "add_metric('corpus::min_T', int(dataset_profile_df['T'].min()), str(FINAL_TAB / 'dataset_profile_for_report.csv'), 'min(T)')\n",
    "add_metric('corpus::max_T', int(dataset_profile_df['T'].max()), str(FINAL_TAB / 'dataset_profile_for_report.csv'), 'max(T)')\n",
    "\n",
    "# Optimization settings (CIW-5 student)\n",
    "add_metric('ciw5::train::seed', int(np.array(ciw5_model_npz['seed']).reshape(-1)[0]), str(required_sources['ciw5_model_npz']), 'seed[0]')\n",
    "add_metric('ciw5::train::lr', float(np.array(ciw5_model_npz['lr']).reshape(-1)[0]), str(required_sources['ciw5_model_npz']), 'lr[0]')\n",
    "add_metric('ciw5::train::epochs', int(np.array(ciw5_model_npz['epochs']).reshape(-1)[0]), str(required_sources['ciw5_model_npz']), 'epochs[0]')\n",
    "add_metric('ciw5::train::batch_size', int(np.array(ciw5_model_npz['batch_size']).reshape(-1)[0]), str(required_sources['ciw5_model_npz']), 'batch_size[0]')\n",
    "add_metric('ciw5::train::weight_decay', float(np.array(ciw5_model_npz['weight_decay']).reshape(-1)[0]), str(required_sources['ciw5_model_npz']), 'weight_decay[0]')\n",
    "\n",
    "# Variant metrics and ranking diagnostics\n",
    "for _, row in variant_selection_diagnostics.iterrows():\n",
    "    key = str(row['variant_code'])\n",
    "    split = str(row['split'])\n",
    "    add_metric(f'variant::{key}::{split}::rmse', float(row['rmse']), str(required_sources['variant_global_metrics']), f'variant_code={key},split={split}')\n",
    "    add_metric(f'variant::{key}::{split}::mae', float(row['mae']), str(required_sources['variant_global_metrics']), f'variant_code={key},split={split}')\n",
    "    add_metric(f'variant::{key}::{split}::mae_ma5', float(row['mae_ma5']), str(required_sources['variant_global_metrics']), f'variant_code={key},split={split}')\n",
    "    add_metric(f'variant::{key}::{split}::r2', float(row['r2']), str(required_sources['variant_global_metrics']), f'variant_code={key},split={split}')\n",
    "    add_metric(f'variant::{key}::{split}::corr', float(row['corr']), str(required_sources['variant_global_metrics']), f'variant_code={key},split={split}')\n",
    "    add_metric(f'variant::{key}::{split}::rank_trend_primary', int(row['rank_trend_primary']), str(FINAL_TAB / 'variant_selection_diagnostics.csv'), f'variant_code={key},split={split}')\n",
    "    add_metric(f'variant::{key}::{split}::rank_raw_error', int(row['rank_raw_error']), str(FINAL_TAB / 'variant_selection_diagnostics.csv'), f'variant_code={key},split={split}')\n",
    "    add_metric(f'variant::{key}::{split}::rank_corr', int(row['rank_corr']), str(FINAL_TAB / 'variant_selection_diagnostics.csv'), f'variant_code={key},split={split}')\n",
    "\n",
    "add_metric(\n",
    "    'variant::CIW-5::test::mae_drop_raw_to_ma5',\n",
    "    float(variant_selection_diagnostics[(variant_selection_diagnostics['variant_code'] == 'CIW-5') & (variant_selection_diagnostics['split'] == 'test')]['mae'].iloc[0]\n",
    "          - variant_selection_diagnostics[(variant_selection_diagnostics['variant_code'] == 'CIW-5') & (variant_selection_diagnostics['split'] == 'test')]['mae_ma5'].iloc[0]),\n",
    "    str(FINAL_TAB / 'variant_selection_diagnostics.csv'),\n",
    "    'variant_code=CIW-5,split=test,mae-mae_ma5',\n",
    ")\n",
    "\n",
    "add_metric('selected_variant_code', selected_variant_code, str(FINAL_TAB / 'variant_selection_diagnostics.csv'), 'split=test,rank_trend_primary=1', 'Trend-fidelity-first criterion')\n",
    "\n",
    "# Pairwise label agreement (teacher side diagnostics)\n",
    "for _, row in pairwise_df.iterrows():\n",
    "    a = str(row['variant_a'])\n",
    "    b = str(row['variant_b'])\n",
    "    tag = f'{a}__vs__{b}'\n",
    "    add_metric(f'teacher_pairwise::{tag}::mae', float(row['mae']), str(required_sources['variant_pairwise']), f'variant_a={a},variant_b={b}')\n",
    "    add_metric(f'teacher_pairwise::{tag}::exact_match', float(row['exact_match']), str(required_sources['variant_pairwise']), f'variant_a={a},variant_b={b}')\n",
    "    add_metric(f'teacher_pairwise::{tag}::corr', float(row['corr']), str(required_sources['variant_pairwise']), f'variant_a={a},variant_b={b}')\n",
    "\n",
    "# Clustering metrics\n",
    "add_metric('cluster::feature::selected_method', selected_feature_method, str(required_sources['cluster_quality']), f'branch=feature,method={selected_feature_method},k={selected_feature_k}')\n",
    "add_metric('cluster::feature::selected_k', selected_feature_k, str(required_sources['cluster_quality']), f'branch=feature,method={selected_feature_method},k={selected_feature_k}')\n",
    "add_metric('cluster::feature::silhouette', float(selected_feature_quality['silhouette']), str(required_sources['cluster_quality']), f'branch=feature,method={selected_feature_method},k={selected_feature_k}')\n",
    "add_metric('cluster::feature::davies_bouldin', float(selected_feature_quality['davies_bouldin']), str(required_sources['cluster_quality']), f'branch=feature,method={selected_feature_method},k={selected_feature_k}')\n",
    "add_metric('cluster::feature::calinski_harabasz', float(selected_feature_quality['calinski_harabasz']), str(required_sources['cluster_quality']), f'branch=feature,method={selected_feature_method},k={selected_feature_k}')\n",
    "add_metric('cluster::feature::kmeans_stability_ari', float(selected_feature_quality['kmeans_stability_ari']) if not pd.isna(selected_feature_quality['kmeans_stability_ari']) else np.nan, str(required_sources['cluster_quality']), f'branch=feature,method={selected_feature_method},k={selected_feature_k}')\n",
    "\n",
    "add_metric('cluster::dtw::selected_k', selected_dtw_k, str(required_sources['cluster_quality']), f\"branch=dtw,method={selected_dtw_quality['method']},k={selected_dtw_k}\")\n",
    "add_metric('cluster::dtw::silhouette', float(selected_dtw_quality['silhouette']), str(required_sources['cluster_quality']), f\"branch=dtw,method={selected_dtw_quality['method']},k={selected_dtw_k}\")\n",
    "\n",
    "ari = float(cluster_agree_df[(cluster_agree_df['row_type'] == 'metric') & (cluster_agree_df['metric'] == 'ari')]['value'].iloc[0])\n",
    "nmi = float(cluster_agree_df[(cluster_agree_df['row_type'] == 'metric') & (cluster_agree_df['metric'] == 'nmi')]['value'].iloc[0])\n",
    "add_metric('cluster::agreement::ari', ari, str(required_sources['cluster_agreement']), 'row_type=metric,metric=ari')\n",
    "add_metric('cluster::agreement::nmi', nmi, str(required_sources['cluster_agreement']), 'row_type=metric,metric=nmi')\n",
    "\n",
    "# CIW-5 per-book metrics in registry for traceability\n",
    "for _, row in ciw5_deepdive_df.iterrows():\n",
    "    bid = int(row['book_id'])\n",
    "    split = str(row['split'])\n",
    "    add_metric(f'ciw5::book::{bid}::{split}::mae', float(row['mae']), str(FINAL_TAB / 'ciw5_per_book_deepdive.csv'), f'book_id={bid},split={split}')\n",
    "    add_metric(f'ciw5::book::{bid}::{split}::mae_ma5', float(row['mae_ma']), str(FINAL_TAB / 'ciw5_per_book_deepdive.csv'), f'book_id={bid},split={split}')\n",
    "    add_metric(f'ciw5::book::{bid}::{split}::corr', float(row['corr']), str(FINAL_TAB / 'ciw5_per_book_deepdive.csv'), f'book_id={bid},split={split}')\n",
    "    add_metric(f'ciw5::book::{bid}::{split}::error_gap_raw_vs_ma', float(row['error_gap_raw_vs_ma']), str(FINAL_TAB / 'ciw5_per_book_deepdive.csv'), f'book_id={bid},split={split}')\n",
    "\n",
    "key_registry_df = pd.DataFrame(key_rows)\n",
    "key_registry_df.to_csv(FINAL_TAB / 'key_results_registry.csv', index=False)\n",
    "\n",
    "# Method claims checklist mapped to evidence\n",
    "claim_rows = [\n",
    "    {\n",
    "        'claim_id': 'CLM01',\n",
    "        'claim_text': 'The corpus contains 20 books with a deterministic novel-level split.',\n",
    "        'metric_key_or_source': 'corpus::n_books;corpus::train_novels;corpus::test_novels',\n",
    "    },\n",
    "    {\n",
    "        'claim_id': 'CLM02',\n",
    "        'claim_text': 'The student model is trained with fixed optimization settings and L2 regularization.',\n",
    "        'metric_key_or_source': 'ciw5::train::seed;ciw5::train::lr;ciw5::train::epochs;ciw5::train::batch_size;ciw5::train::weight_decay',\n",
    "    },\n",
    "    {\n",
    "        'claim_id': 'CLM03',\n",
    "        'claim_text': 'CIW-5 is selected by trend-fidelity-first criterion on the test split.',\n",
    "        'metric_key_or_source': 'selected_variant_code;variant::CIW-5::test::rank_trend_primary',\n",
    "    },\n",
    "    {\n",
    "        'claim_id': 'CLM04',\n",
    "        'claim_text': 'CIW-5 has the strongest MA(5) trend-level error profile on test data.',\n",
    "        'metric_key_or_source': 'variant::CIW-5::test::mae_ma5;variant::NC-1::test::mae_ma5;variant::SW-5::test::mae_ma5',\n",
    "    },\n",
    "    {\n",
    "        'claim_id': 'CLM05',\n",
    "        'claim_text': 'CIW-5 reduces error when evaluated on smoothed trajectories.',\n",
    "        'metric_key_or_source': 'variant::CIW-5::test::mae_drop_raw_to_ma5',\n",
    "    },\n",
    "    {\n",
    "        'claim_id': 'CLM06',\n",
    "        'claim_text': 'Feature clustering uses a data-driven selection and DTW is retained as validation.',\n",
    "        'metric_key_or_source': 'cluster::feature::selected_method;cluster::feature::selected_k;cluster::dtw::selected_k',\n",
    "    },\n",
    "    {\n",
    "        'claim_id': 'CLM07',\n",
    "        'claim_text': 'Feature and DTW clustering agreement is limited and should be interpreted cautiously.',\n",
    "        'metric_key_or_source': 'cluster::agreement::ari;cluster::agreement::nmi',\n",
    "    },\n",
    "    {\n",
    "        'claim_id': 'CLM08',\n",
    "        'claim_text': 'Teacher-side variant protocols exhibit measurable disagreement.',\n",
    "        'metric_key_or_source': 'path:' + str(required_sources['variant_pairwise']),\n",
    "    },\n",
    "    {\n",
    "        'claim_id': 'CLM09',\n",
    "        'claim_text': 'Per-book CIW-5 behavior is explicitly documented for all novels.',\n",
    "        'metric_key_or_source': 'path:' + str(FINAL_TAB / 'ciw5_per_book_deepdive.csv'),\n",
    "    },\n",
    "    {\n",
    "        'claim_id': 'CLM10',\n",
    "        'claim_text': 'Final report figures and tables are reproducibly generated by stage-10 notebook.',\n",
    "        'metric_key_or_source': 'path:' + str(FINAL_TAB / 'variant_selection_diagnostics.csv') + ';path:' + str(FINAL_TAB / 'key_results_registry.csv'),\n",
    "    },\n",
    "]\n",
    "\n",
    "registry_keys = set(key_registry_df['metric_key'].astype(str).tolist())\n",
    "\n",
    "\n",
    "def claim_status(refs: str) -> str:\n",
    "    parts = [p.strip() for p in str(refs).split(';') if p.strip()]\n",
    "    ok = True\n",
    "    for p in parts:\n",
    "        if p.startswith('path:'):\n",
    "            ok = ok and Path(p.replace('path:', '', 1)).exists()\n",
    "        else:\n",
    "            ok = ok and (p in registry_keys)\n",
    "    return 'mapped' if ok else 'missing'\n",
    "\n",
    "\n",
    "method_claims_df = pd.DataFrame(claim_rows)\n",
    "method_claims_df['status'] = method_claims_df['metric_key_or_source'].map(claim_status)\n",
    "method_claims_df.to_csv(FINAL_TAB / 'method_claims_checklist.csv', index=False)\n",
    "\n",
    "# Early checks used again in final integrity table\n",
    "source_checks.append({\n",
    "    'check': 'selected_variant_is_ciw5',\n",
    "    'expected': 'CIW-5',\n",
    "    'actual': selected_variant_code,\n",
    "    'pass': selected_variant_code == 'CIW-5',\n",
    "})\n",
    "source_checks.append({\n",
    "    'check': 'split_is_16_train_4_test',\n",
    "    'expected': 'train=16,test=4',\n",
    "    'actual': f\"train={(dataset_profile_df['split'] == 'train').sum()},test={(dataset_profile_df['split'] == 'test').sum()}\",\n",
    "    'pass': ((dataset_profile_df['split'] == 'train').sum() == 16) and ((dataset_profile_df['split'] == 'test').sum() == 4),\n",
    "})\n",
    "\n",
    "print('Saved:', FINAL_TAB / 'dataset_profile_for_report.csv')\n",
    "print('Saved:', FINAL_TAB / 'variant_selection_summary.csv')\n",
    "print('Saved:', FINAL_TAB / 'variant_selection_diagnostics.csv')\n",
    "print('Saved:', FINAL_TAB / 'ciw5_per_book_deepdive.csv')\n",
    "print('Saved:', FINAL_TAB / 'cluster_summary_for_report.csv')\n",
    "print('Saved:', FINAL_TAB / 'key_results_registry.csv')\n",
    "print('Saved:', FINAL_TAB / 'method_claims_checklist.csv')\n",
    "print('Selected variant:', selected_variant_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build curated figures for final report\n",
    "variant_order = ['NC-1', 'SW-5', 'CIW-5']\n",
    "variant_colors = {'NC-1': '#4c78a8', 'SW-5': '#f58518', 'CIW-5': '#54a24b'}\n",
    "\n",
    "# Load generated support tables\n",
    "variant_selection_summary = pd.read_csv(FINAL_TAB / 'variant_selection_summary.csv')\n",
    "variant_selection_diagnostics = pd.read_csv(FINAL_TAB / 'variant_selection_diagnostics.csv')\n",
    "ciw5_deepdive_df = pd.read_csv(FINAL_TAB / 'ciw5_per_book_deepdive.csv')\n",
    "\n",
    "# Figure 01: pipeline overview diagram\n",
    "fig, ax = plt.subplots(figsize=(16, 4.8))\n",
    "ax.axis('off')\n",
    "\n",
    "steps = [\n",
    "    ('Get Data', 'Gutenberg corpus\\n20 novels'),\n",
    "    ('Sliding-Window Embeddings', 'Chunking and\\nsentence-transformer vectors'),\n",
    "    ('Teacher Labels', 'NC-1, SW-5, CIW-5\\npseudo-ground truth'),\n",
    "    ('Linear Semantic Basis', '1-layer perceptron\\ny_hat = x^T w + b'),\n",
    "    ('Variant Selection', 'Trend-fidelity first\\nCIW-5 selected'),\n",
    "    ('Clustering', 'Feature-primary\\ntrajectory archetypes'),\n",
    "    ('Applications', 'Interpretable semantic\\ntime-series analytics'),\n",
    "]\n",
    "\n",
    "x_positions = np.linspace(0.05, 0.95, len(steps))\n",
    "y = 0.5\n",
    "w = 0.12\n",
    "h = 0.32\n",
    "for i, (title, desc) in enumerate(steps):\n",
    "    x = x_positions[i] - w / 2\n",
    "    box = FancyBboxPatch((x, y - h / 2), w, h, boxstyle='round,pad=0.02', ec='black', fc='#f8f9fb', lw=1.2)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x_positions[i], y + 0.06, title, ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    ax.text(x_positions[i], y - 0.07, desc, ha='center', va='center', fontsize=9)\n",
    "    if i < len(steps) - 1:\n",
    "        ax.annotate('', xy=(x_positions[i + 1] - w / 2 + 0.01, y), xytext=(x_positions[i] + w / 2 - 0.01, y), arrowprops=dict(arrowstyle='->', lw=1.2))\n",
    "\n",
    "ax.set_title('Teacher-Guided Semantic Basis Projection Workflow', fontsize=14, pad=14)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FINAL_FIG / 'fig01_pipeline_overview.png', dpi=220, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Figure 02: variant comparison metrics on test split\n",
    "test_metrics = variant_selection_summary[variant_selection_summary['split'] == 'test'].copy()\n",
    "test_metrics = test_metrics.set_index('variant_code').loc[variant_order].reset_index()\n",
    "metric_cols = ['rmse', 'mae', 'mae_ma5', 'r2', 'corr']\n",
    "metric_titles = ['RMSE', 'MAE', f'MAE MA({MA_WINDOW})', 'R2', 'Correlation']\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(22, 4.2))\n",
    "for ax, metric, title in zip(axes, metric_cols, metric_titles):\n",
    "    vals = test_metrics[metric].to_numpy(dtype=float)\n",
    "    bars = ax.bar(np.arange(len(variant_order)), vals, color=[variant_colors[v] for v in variant_order], alpha=0.9)\n",
    "\n",
    "    best_idx = int(np.argmin(vals)) if metric in ['rmse', 'mae', 'mae_ma5'] else int(np.argmax(vals))\n",
    "    bars[best_idx].set_edgecolor('black')\n",
    "    bars[best_idx].set_linewidth(2)\n",
    "\n",
    "    for i, v in enumerate(vals):\n",
    "        ax.text(i, v, f'{v:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    ax.set_xticks(np.arange(len(variant_order)))\n",
    "    ax.set_xticklabels(variant_order)\n",
    "    ax.set_title(title)\n",
    "    ax.grid(axis='y', alpha=0.2)\n",
    "\n",
    "fig.suptitle('Test Split Variant Comparison (trend-fidelity-first ranking)', fontsize=13)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FINAL_FIG / 'fig02_variant_comparison_test_metrics.png', dpi=220, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Figure 03: CIW-5 model behavior composite\n",
    "img_paths_03 = [\n",
    "    SRC_VARIANT_FIG / 'indep_prediction_scatter_train_test.png',\n",
    "    SRC_VARIANT_FIG / 'indep_residual_hist_train_test.png',\n",
    "    SRC_VARIANT_FIG / 'train_loss_curves_by_variant.png',\n",
    "]\n",
    "img_titles_03 = [\n",
    "    'True vs Predicted (CIW-5)',\n",
    "    'Residual Distribution (CIW-5)',\n",
    "    'Loss Curves (all variants, CIW-5 included)',\n",
    "]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5.2))\n",
    "for ax, p, t in zip(axes, img_paths_03, img_titles_03):\n",
    "    ax.imshow(mpimg.imread(p))\n",
    "    ax.axis('off')\n",
    "    ax.set_title(t, fontsize=10)\n",
    "fig.suptitle('CIW-5 Linear Projection Behavior', fontsize=13)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FINAL_FIG / 'fig03_ciw5_model_behavior.png', dpi=220, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Figure 04: CIW-5 test overlays montage\n",
    "overlay_paths = sorted(SRC_VARIANT_FIG.glob('indep_novel_overlay_test_*.png'), key=lambda p: int(p.stem.split('_')[-1]))\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 9))\n",
    "for ax, p in zip(axes.flatten(), overlay_paths):\n",
    "    ax.imshow(mpimg.imread(p))\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Test novel overlay: {p.stem.split(\"_\")[-1]}', fontsize=10)\n",
    "for ax in axes.flatten()[len(overlay_paths):]:\n",
    "    ax.axis('off')\n",
    "fig.suptitle('CIW-5 Test Novel Overlays', fontsize=13)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FINAL_FIG / 'fig04_ciw5_test_overlays_reference.png', dpi=220, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Figure 05: feature cluster map\n",
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "ax.imshow(mpimg.imread(SRC_CLUSTER_FIG / 'feature_pca_scatter_feature_clusters.png'))\n",
    "ax.axis('off')\n",
    "ax.set_title('Feature Cluster Map (CIW-5 derived features)', fontsize=13)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FINAL_FIG / 'fig05_feature_cluster_map.png', dpi=220, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Figure 06: cluster genre composition composite\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5.5))\n",
    "axes[0].imshow(mpimg.imread(SRC_CLUSTER_FIG / 'genre_by_feature_cluster_counts.png'))\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Genre counts by cluster', fontsize=11)\n",
    "axes[1].imshow(mpimg.imread(SRC_CLUSTER_FIG / 'genre_by_feature_cluster_proportions.png'))\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Genre proportions by cluster', fontsize=11)\n",
    "fig.suptitle('Genre Composition of Feature Clusters', fontsize=13)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FINAL_FIG / 'fig06_cluster_genre_composition.png', dpi=220, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Figure 07: feature-cluster signatures and member trajectories composite\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5.5))\n",
    "axes[0].imshow(mpimg.imread(SRC_CLUSTER_FIG / 'cluster_feature_signature_heatmap_top12.png'))\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Cluster feature signatures', fontsize=11)\n",
    "axes[1].imshow(mpimg.imread(SRC_CLUSTER_FIG / 'feature_cluster_member_trajectories_ma5.png'))\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Feature-cluster member trajectories (MA5)', fontsize=11)\n",
    "fig.suptitle('Feature-Cluster Signatures and MA5 Trajectory Archetypes', fontsize=13)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FINAL_FIG / 'fig07_cluster_signatures_and_agreement.png', dpi=220, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Figure 08: variant rank sensitivity across criteria\n",
    "diag_test = variant_selection_diagnostics[variant_selection_diagnostics['split'] == 'test'].copy()\n",
    "diag_test = diag_test.set_index('variant_code').loc[variant_order].reset_index()\n",
    "rank_cols = ['rank_trend_primary', 'rank_raw_error', 'rank_corr']\n",
    "rank_titles = ['Trend-first rank', 'Raw-error rank', 'Correlation-first rank']\n",
    "rank_mat = diag_test[rank_cols].to_numpy(dtype=float)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8.5, 4.8))\n",
    "im = ax.imshow(rank_mat, cmap='YlGn_r', vmin=1, vmax=len(variant_order), aspect='auto')\n",
    "for i in range(rank_mat.shape[0]):\n",
    "    for j in range(rank_mat.shape[1]):\n",
    "        ax.text(j, i, f'{int(rank_mat[i, j])}', ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "ax.set_xticks(np.arange(len(rank_cols)))\n",
    "ax.set_xticklabels(rank_titles)\n",
    "ax.set_yticks(np.arange(len(variant_order)))\n",
    "ax.set_yticklabels(variant_order)\n",
    "ax.set_title('Variant Rank Sensitivity on Test Split')\n",
    "cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "cbar.set_label('Rank (1 = best)', rotation=90)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FINAL_FIG / 'fig08_variant_rank_sensitivity.png', dpi=220, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Figure 09: CIW-5 per-book test breakdown\n",
    "ciw5_test = ciw5_deepdive_df[ciw5_deepdive_df['split'] == 'test'].copy().sort_values('mae_ma')\n",
    "\n",
    "def short_title(t: str, n: int = 18) -> str:\n",
    "    return t if len(t) <= n else t[: n - 3] + '...'\n",
    "\n",
    "x = np.arange(len(ciw5_test))\n",
    "labels = [f\"{int(r.book_id)}\\n{short_title(str(r.title), 20)}\" for r in ciw5_test.itertuples()]\n",
    "width = 0.35\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 5.2))\n",
    "ax1.bar(x - width / 2, ciw5_test['mae'].to_numpy(float), width=width, color='#4c78a8', label='Raw MAE')\n",
    "ax1.bar(x + width / 2, ciw5_test['mae_ma'].to_numpy(float), width=width, color='#54a24b', label=f'MA({MA_WINDOW}) MAE')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(labels)\n",
    "ax1.set_ylabel('MAE')\n",
    "ax1.set_title('CIW-5 Test-Novel Error Breakdown (Raw vs MA(5))')\n",
    "ax1.grid(axis='y', alpha=0.2)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(x, ciw5_test['corr'].to_numpy(float), color='#f58518', marker='o', lw=2, label='Raw corr')\n",
    "ax2.set_ylabel('Correlation')\n",
    "\n",
    "h1, l1 = ax1.get_legend_handles_labels()\n",
    "h2, l2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(h1 + h2, l1 + l2, loc='upper right')\n",
    "fig.tight_layout()\n",
    "fig.savefig(FINAL_FIG / 'fig09_ciw5_per_book_test_breakdown.png', dpi=220, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Figure 10: contribution and use-cases map\n",
    "fig, ax = plt.subplots(figsize=(14, 7.5))\n",
    "ax.axis('off')\n",
    "\n",
    "left_x, mid_x, right_x = 0.07, 0.39, 0.71\n",
    "box_w, box_h = 0.22, 0.18\n",
    "rows_y = [0.78, 0.52, 0.26]\n",
    "\n",
    "left_text = [\n",
    "    ('Representation', 'Sliding-window embeddings\\nfrom narrative chunks'),\n",
    "    ('Teacher signals', 'LLM pseudo-ground truth\\nunder three protocols'),\n",
    "    ('Student extraction', 'Linear semantic basis\\nprojection'),\n",
    "]\n",
    "mid_text = [\n",
    "    ('Core contribution', 'Semantic-to-time-series\\nprojection framework'),\n",
    "    ('Validation logic', 'Trend-first variant selection\\nplus cluster structure'),\n",
    "    ('Interpretability', 'Explicit basis vector,\\nbook-level archetypes'),\n",
    "]\n",
    "right_text = [\n",
    "    ('Applied analytics', 'Pacing diagnostics\\nand cross-book comparison'),\n",
    "    ('Workflow reuse', 'New abstract semantics\\nwith the same pipeline'),\n",
    "    ('Research extension', 'Teacher-student signal\\ndistillation studies'),\n",
    "]\n",
    "\n",
    "for y, (a1, a2), (b1, b2), (c1, c2) in zip(rows_y, left_text, mid_text, right_text):\n",
    "    for x0, t1, t2, fc in [\n",
    "        (left_x, a1, a2, '#edf2fb'),\n",
    "        (mid_x, b1, b2, '#e9f7ef'),\n",
    "        (right_x, c1, c2, '#fff4e6'),\n",
    "    ]:\n",
    "        rect = FancyBboxPatch((x0, y), box_w, box_h, boxstyle='round,pad=0.015', ec='black', fc=fc, lw=1.1)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x0 + box_w / 2, y + 0.115, t1, ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "        ax.text(x0 + box_w / 2, y + 0.055, t2, ha='center', va='center', fontsize=9)\n",
    "\n",
    "    ax.annotate('', xy=(mid_x - 0.01, y + box_h / 2), xytext=(left_x + box_w + 0.01, y + box_h / 2), arrowprops=dict(arrowstyle='->', lw=1.2))\n",
    "    ax.annotate('', xy=(right_x - 0.01, y + box_h / 2), xytext=(mid_x + box_w + 0.01, y + box_h / 2), arrowprops=dict(arrowstyle='->', lw=1.2))\n",
    "\n",
    "ax.set_title('Method Contribution and Use-Case Map', fontsize=14, pad=14)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FINAL_FIG / 'fig10_contribution_and_use_cases_map.png', dpi=220, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Figure 11: dedicated feature-cluster member trajectories (MA5)\n",
    "fig, ax = plt.subplots(figsize=(14, 9))\n",
    "ax.imshow(mpimg.imread(SRC_CLUSTER_FIG / 'feature_cluster_member_trajectories_ma5.png'))\n",
    "ax.axis('off')\n",
    "ax.set_title('Feature Cluster Member Trajectories (MA5)', fontsize=13)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FINAL_FIG / 'fig11_feature_cluster_member_trajectories_ma5.png', dpi=220, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "print('Generated curated figures in', FINAL_FIG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render FINAL_REPORT.md and OTHER_EXPERIMENTS.md\n",
    "import re\n",
    "\n",
    "# Utility to fetch registry values\n",
    "reg = pd.read_csv(FINAL_TAB / 'key_results_registry.csv')\n",
    "\n",
    "\n",
    "def metric(key: str, digits: int = 3):\n",
    "    v = reg.loc[reg['metric_key'] == key, 'value'].iloc[0]\n",
    "    try:\n",
    "        vf = float(v)\n",
    "        return f\"{vf:.{digits}f}\"\n",
    "    except Exception:\n",
    "        return str(v)\n",
    "\n",
    "\n",
    "variant_table = pd.read_csv(FINAL_TAB / 'variant_selection_summary.csv')\n",
    "variant_diag = pd.read_csv(FINAL_TAB / 'variant_selection_diagnostics.csv')\n",
    "variant_test = variant_diag[variant_diag['split'] == 'test'].sort_values('rank_trend_primary')\n",
    "cluster_summary = pd.read_csv(FINAL_TAB / 'cluster_summary_for_report.csv')\n",
    "dataset_profile = pd.read_csv(FINAL_TAB / 'dataset_profile_for_report.csv')\n",
    "ciw5_deepdive = pd.read_csv(FINAL_TAB / 'ciw5_per_book_deepdive.csv')\n",
    "claims_df = pd.read_csv(FINAL_TAB / 'method_claims_checklist.csv')\n",
    "\n",
    "\n",
    "def df_to_md(df: pd.DataFrame, cols: list[str], float_digits: int = 3) -> str:\n",
    "    d = df[cols].copy()\n",
    "    out = []\n",
    "    out.append('| ' + ' | '.join(cols) + ' |')\n",
    "    out.append('|' + '|'.join(['---'] * len(cols)) + '|')\n",
    "    for _, row in d.iterrows():\n",
    "        vals = []\n",
    "        for c in cols:\n",
    "            v = row[c]\n",
    "            if isinstance(v, float):\n",
    "                vals.append(f'{v:.{float_digits}f}')\n",
    "            else:\n",
    "                vals.append(str(v).replace('|', '\\|'))\n",
    "        out.append('| ' + ' | '.join(vals) + ' |')\n",
    "    return chr(10).join(out)\n",
    "\n",
    "\n",
    "split_profile = (\n",
    "    dataset_profile\n",
    "    .groupby('split', as_index=False)\n",
    "    .agg(n_books=('book_id', 'nunique'), n_chunks=('T', 'sum'), median_T=('T', 'median'), min_T=('T', 'min'), max_T=('T', 'max'))\n",
    ")\n",
    "\n",
    "variant_md_table = df_to_md(\n",
    "    variant_test,\n",
    "    ['variant_code', 'rmse', 'mae', 'mae_ma5', 'r2', 'corr', 'rank_trend_primary', 'rank_raw_error', 'rank_corr'],\n",
    ")\n",
    "\n",
    "cluster_md_table = df_to_md(\n",
    "    cluster_summary,\n",
    "    ['cluster', 'n_books', 'top_feature_1', 'top_feature_2', 'top_feature_3', 'representative_book', 'dominant_genre', 'dominant_genre_prop'],\n",
    ")\n",
    "\n",
    "split_md_table = df_to_md(split_profile, ['split', 'n_books', 'n_chunks', 'median_T', 'min_T', 'max_T'])\n",
    "\n",
    "ciw5_test_md = df_to_md(\n",
    "    ciw5_deepdive[ciw5_deepdive['split'] == 'test'].sort_values('mae_ma'),\n",
    "    ['book_id', 'title', 'T', 'mae', 'mae_ma', 'corr', 'error_gap_raw_vs_ma'],\n",
    ")\n",
    "\n",
    "final_report_lines = []\n",
    "final_report_lines.append('# Teacher-Guided Semantic Basis Projection: A General Semantic-to-Time-Series Framework (Excitement Case Study)')\n",
    "final_report_lines.append('')\n",
    "final_report_lines.append('## Executive Abstract')\n",
    "final_report_lines.append('This report presents a full project narrative for a method that maps abstract semantic constructs into interpretable time series. The concrete case study is narrative excitement, but the method is intentionally formulated as a general framework. The pipeline starts from long-form text, constructs sliding-window embedding trajectories, produces LLM teacher labels under three protocols, learns a linear semantic basis projection with a one-layer perceptron, selects the best teacher protocol using trend-fidelity criteria, and then analyzes structure through unsupervised clustering.')\n",
    "final_report_lines.append('')\n",
    "final_report_lines.append('The central hypothesis is that sentence-embedding geometry contains latent semantic directions for abstract concepts such as excitement. Instead of maximizing variance as PCA does, this work optimizes semantic alignment to teacher labels. Under the locked selection rule, CIW-5 is selected as the primary teacher protocol. The results support a practical workflow for teacher-guided, interpretable semantic signal extraction from embeddings.')\n",
    "final_report_lines.append('')\n",
    "\n",
    "final_report_lines.append('## 1. Project Objective and Contribution Statement')\n",
    "final_report_lines.append('The project objective is to establish and evaluate a reproducible method for converting abstract semantics into chunk-level time-series signals that are interpretable, measurable, and suitable for downstream analysis. This differs from conventional document-level sentiment analysis because the focus is trajectory behavior across narrative progression.')\n",
    "final_report_lines.append('')\n",
    "final_report_lines.append('Main contribution of this stage:')\n",
    "final_report_lines.append('1. A teacher-guided semantic basis projection formulation that is simple and explicit.')\n",
    "final_report_lines.append('2. A variant-comparison protocol for teacher labels that prioritizes trend fidelity.')\n",
    "final_report_lines.append('3. A clustering analysis layer over the selected signal to derive pacing archetypes and genre-linked structure.')\n",
    "final_report_lines.append('4. A packaging workflow where every claim is mapped to reproducible artifacts.')\n",
    "final_report_lines.append('')\n",
    "\n",
    "final_report_lines.append('## 2. Problem Formulation and Hypothesis')\n",
    "final_report_lines.append('Let `x_t ∈ R^D` be the sentence embedding of chunk `t` for a book. The student model predicts semantic intensity with a single linear map:')\n",
    "final_report_lines.append('')\n",
    "final_report_lines.append('`ŷ_t = x_t^T w + b`')\n",
    "final_report_lines.append('')\n",
    "final_report_lines.append('where `w ∈ R^D` is the semantic basis vector and `b` is a scalar bias. Training minimizes mean squared error with L2 regularization:')\n",
    "final_report_lines.append('')\n",
    "final_report_lines.append('`L = (1/N) Σ_t (ŷ_t - y_t)^2 + λ ||w||_2^2`')\n",
    "final_report_lines.append('')\n",
    "final_report_lines.append('Hypothesis: if excitement is encoded in embedding geometry, a supervised linear axis should recover trend-aligned signals against teacher labels. This objective is distinct from PCA, which optimizes variance explanation without semantic supervision.')\n",
    "final_report_lines.append('')\n",
    "\n",
    "final_report_lines.append('## 3. Data and Representation Pipeline')\n",
    "final_report_lines.append('The corpus contains `{}` novels and `{}` total chunks across all books. Chunk counts range from `{}` to `{}` per book. Novel-level split is deterministic and leakage-safe, with `{}` train novels and `{}` test novels.'.format(\n",
    "    metric('corpus::n_books', 0),\n",
    "    metric('corpus::n_chunks_total', 0),\n",
    "    metric('corpus::min_T', 0),\n",
    "    metric('corpus::max_T', 0),\n",
    "    metric('corpus::train_novels', 0),\n",
    "    metric('corpus::test_novels', 0),\n",
    "))\n",
    "final_report_lines.append('')\n",
    "final_report_lines.append('Split profile:')\n",
    "final_report_lines.append('')\n",
    "final_report_lines.append(split_md_table)\n",
    "final_report_lines.append('')\n",
    "final_report_lines.append('Representation route used in this report:')\n",
    "final_report_lines.append('`Data -> Sliding-window embeddings -> Teacher pseudo-ground truth variants -> Linear semantic basis projection -> Variant selection -> Clustering -> Utility analysis`')\n",
    "final_report_lines.append('')\n",
    "\n",
    "final_report_lines.append('## 4. Teacher Protocols and Pseudo-Ground-Truth Design')\n",
    "final_report_lines.append('Teacher labels are generated by LLM judging on a 0-4 excitement scale. Labels are treated as pseudo-ground truth because they are model-derived supervision, not direct human annotation. Three variants are used:')\n",
    "final_report_lines.append('1. `NC-1`: No-Context Chunk Teacher Labels (`label.npy`).')\n",
    "final_report_lines.append('2. `SW-5`: Shared-Window Labels (`label_winsize_5.npy`).')\n",
    "final_report_lines.append('3. `CIW-5`: Context-Window Independent Labels (`label_indep_winsize_5.npy`).')\n",
    "final_report_lines.append('')\n",
    "final_report_lines.append('The variant study is essential because teacher protocol changes alter supervision smoothness, local consistency, and calibration behavior.')\n",
    "final_report_lines.append('')\n",
    "\n",
    "final_report_lines.append('## 5. Student Model: Semantic Basis Extraction')\n",
    "final_report_lines.append('The student is a one-layer perceptron over standardized embeddings. The learned vector `w` is interpreted as a semantic basis direction in embedding space rather than a black-box latent representation. This gives a clear mapping between representation and predicted signal while preserving computational simplicity.')\n",
    "final_report_lines.append('')\n",
    "final_report_lines.append('Training configuration for CIW-5 model (from saved model artifact):')\n",
    "final_report_lines.append('1. `seed = {}`'.format(metric('ciw5::train::seed', 0)))\n",
    "final_report_lines.append('2. `lr = {}`'.format(metric('ciw5::train::lr', 5)))\n",
    "final_report_lines.append('3. `epochs = {}`'.format(metric('ciw5::train::epochs', 0)))\n",
    "final_report_lines.append('4. `batch_size = {}`'.format(metric('ciw5::train::batch_size', 0)))\n",
    "final_report_lines.append('5. `weight_decay = {}`'.format(metric('ciw5::train::weight_decay', 6)))\n",
    "final_report_lines.append('')\n",
    "\n",
    "final_report_lines.append('## 6. Evaluation Protocol and Trend-Fidelity Criterion')\n",
    "final_report_lines.append('Primary selection policy is trend-fidelity-first on test data with MA(5) smoothing. For each variant, ranking is determined by:')\n",
    "final_report_lines.append('1. Lowest `MAE_MA5`')\n",
    "final_report_lines.append('2. Then lowest raw `MAE`')\n",
    "final_report_lines.append('3. Then lowest `RMSE`')\n",
    "final_report_lines.append('')\n",
    "final_report_lines.append('Secondary diagnostics (`R2`, correlation) are reported for context but do not override the primary criterion.')\n",
    "final_report_lines.append('')\n",
    "\n",
    "final_report_lines.append('## 7. Variant Study Results and Selection Rationale')\n",
    "final_report_lines.append('Test-split variant diagnostics:')\n",
    "final_report_lines.append('')\n",
    "final_report_lines.append(variant_md_table)\n",
    "final_report_lines.append('')\n",
    "final_report_lines.append('Interpretation:')\n",
    "final_report_lines.append('1. `CIW-5` ranks first by the locked trend-fidelity rule.')\n",
    "final_report_lines.append('2. `CIW-5` test raw metrics are `RMSE={}`, `MAE={}`.'.format(metric('variant::CIW-5::test::rmse'), metric('variant::CIW-5::test::mae')))\n",
    "final_report_lines.append('3. `CIW-5` trend metric is `MAE_MA5={}`, with raw-to-smoothed drop `{}`.'.format(metric('variant::CIW-5::test::mae_ma5'), metric('variant::CIW-5::test::mae_drop_raw_to_ma5')))\n",
    "final_report_lines.append('4. `selected_variant_code = {}` based on deterministic ranking.'.format(metric('selected_variant_code', 0)))\n",
    "final_report_lines.append('')\n",
    "\n",
    "final_report_lines.append('## 8. Selected Variant (CIW-5) Deep Behavior Analysis')\n",
    "final_report_lines.append('Per-test-book CIW-5 diagnostics are summarized below. `error_gap_raw_vs_ma` indicates the improvement from raw MAE to MA(5) MAE.')\n",
    "final_report_lines.append('')\n",
    "final_report_lines.append(ciw5_test_md)\n",
    "final_report_lines.append('')\n",
    "final_report_lines.append('This table supports two conclusions:')\n",
    "final_report_lines.append('1. Trend-level agreement is consistently better than chunk-level agreement.')\n",
    "final_report_lines.append('2. Book-level heterogeneity remains substantial, so deployment should prioritize comparative trend profiling over absolute chunk score decisions.')\n",
    "final_report_lines.append('')\n",
    "\n",
    "final_report_lines.append('## 9. Clustering on Selected Signal and Archetype Interpretation')\n",
    "final_report_lines.append('Clustering in the final presentation is applied to CIW-5 derived trajectory features using the feature branch.')\n",
    "final_report_lines.append('')\n",
    "final_report_lines.append('Clustering input definition in this project:')\n",
    "final_report_lines.append('1. Feature-branch clustering is performed on a per-book feature vector extracted from the **raw CIW-5 trajectory**.')\n",
    "final_report_lines.append('2. This feature vector also includes three MA(5)-derived summary features (`mean_ma5`, `std_ma5`, `p95_ma5`).')\n",
    "final_report_lines.append('3. Therefore, clustering is **not** performed on CIW-5 MA(5)-only sequence values. It is performed on a mixed descriptor set dominated by raw CIW-5 statistics plus MA(5) summaries.')\n",
    "final_report_lines.append('4. MA(5) trajectories are used for additional visualization and archetype interpretation panels.')\n",
    "final_report_lines.append('')\n",
    "final_report_lines.append('Extracted per-book features from CIW-5 time series:')\n",
    "final_report_lines.append('1. Length and level/distribution: `T`, `mean_y`, `std_y`, `median_y`, `iqr_y`, `min_y`, `max_y`, `p10_y`, `p90_y`, `range_y`.')\n",
    "final_report_lines.append('2. Label composition: `prop_label_0`, `prop_label_1`, `prop_label_2`, `prop_label_3`, `prop_label_4`, `entropy_labels`.')\n",
    "final_report_lines.append('3. Local dynamics: `mean_abs_diff`, `std_diff`, `p95_abs_diff`, `jump_ge_2_rate`, `up_rate`, `down_rate`, `flat_rate`, `lag1_autocorr`, `sign_change_rate`.')\n",
    "final_report_lines.append('4. Position/trend structure: `corr_with_position`, `slope_position`, `mean_early`, `mean_mid`, `mean_late`.')\n",
    "final_report_lines.append('5. Smoothed summaries (MA5): `mean_ma5`, `std_ma5`, `p95_ma5`.')\n",
    "final_report_lines.append('')\n",
    "final_report_lines.append('Selected feature configuration: method `{}`, `k={}`, silhouette `{}`, stability ARI `{}`.'.format(\n",
    "    metric('cluster::feature::selected_method', 0),\n",
    "    metric('cluster::feature::selected_k', 0),\n",
    "    metric('cluster::feature::silhouette'),\n",
    "    metric('cluster::feature::kmeans_stability_ari'),\n",
    "))\n",
    "final_report_lines.append('')\n",
    "final_report_lines.append('Cluster summary (feature-primary):')\n",
    "final_report_lines.append('')\n",
    "final_report_lines.append(cluster_md_table)\n",
    "final_report_lines.append('')\n",
    "final_report_lines.append('Reading guidance: feature clusters are interpreted through engineered trajectory descriptors and MA(5) member trajectories. This keeps the final presentation focused on interpretable archetypes from CIW-5 features.')\n",
    "final_report_lines.append('')\n",
    "\n",
    "final_report_lines.append('## 10. Figure Explanations and Evidence')\n",
    "fig_sections = [\n",
    "    ('Figure 1. Pipeline Overview', 'fig01_pipeline_overview.png',\n",
    "     'The full workflow from data ingestion to semantic time-series applications.',\n",
    "     'Read left to right. Each box is a stage and arrows indicate dependency flow.',\n",
    "     'The report contribution is centered on the supervised semantic-axis extraction stage, not on unsupervised variance decomposition.'),\n",
    "    ('Figure 2. Variant Comparison on Test Split', 'fig02_variant_comparison_test_metrics.png',\n",
    "     'Comparative test metrics for NC-1, SW-5, and CIW-5 across raw and smoothed errors plus correlation diagnostics.',\n",
    "     'For RMSE/MAE/MAE_MA5 lower is better. For R2/correlation higher is better. Selection still follows trend-first ranking.',\n",
    "     'CIW-5 is selected because it is best on the primary trend metric while remaining competitive on raw metrics.'),\n",
    "    ('Figure 3. CIW-5 Model Behavior', 'fig03_ciw5_model_behavior.png',\n",
    "     'Scatter, residual, and training diagnostics associated with the selected CIW-5 student model.',\n",
    "     'Use scatter for calibration spread, residual histogram for bias shape, and loss curves for optimization stability.',\n",
    "     'The model is stable and interpretable, but chunk-level residual spread confirms that trend-level interpretation is the safer use mode.'),\n",
    "    ('Figure 4. CIW-5 Test Overlays', 'fig04_ciw5_test_overlays_reference.png',\n",
    "     'Overlay of teacher and student trajectories for all held-out test novels.',\n",
    "     'Track directional movement and pacing regions instead of exact pointwise matching.',\n",
    "     'The selected model preserves broad narrative dynamics on unseen novels, which justifies trend-level utility claims.'),\n",
    "    ('Figure 5. Feature Cluster Map', 'fig05_feature_cluster_map.png',\n",
    "     'Feature-space map of books with selected feature-cluster assignments.',\n",
    "     'Each point is one book and color indicates cluster identity.',\n",
    "     'The map provides the geometric context for archetype interpretation in the cluster summary table.'),\n",
    "    ('Figure 6. Genre Composition by Cluster', 'fig06_cluster_genre_composition.png',\n",
    "     'Cluster composition shown in both counts and row-normalized proportions by `genre_primary`.',\n",
    "     'Left panel shows absolute counts. Right panel shows within-cluster composition.',\n",
    "     'Genre concentration varies across clusters, supporting the claim that the extracted signal captures narratively meaningful structure.'),\n",
    "    ('Figure 7. Feature Cluster Signatures and Member Trajectories', 'fig07_cluster_signatures_and_agreement.png',\n",
    "     'Top feature signatures by cluster together with MA(5) member-trajectory archetypes.',\n",
    "     'Use the signature panel to read which features distinguish each cluster, then inspect MA(5) trajectory panels for pacing shape patterns.',\n",
    "     'Together, these views connect feature-level semantics to observable cluster trajectory behavior in CIW-5.'),\n",
    "    ('Figure 8. Variant Rank Sensitivity', 'fig08_variant_rank_sensitivity.png',\n",
    "     'Rank matrix of variants under trend-first, raw-error-first, and correlation-first criteria.',\n",
    "     'Lower rank numbers are better. Compare rows (variants) across columns (criteria).',\n",
    "     'This figure makes selection logic transparent and shows how conclusions shift under alternate objectives.'),\n",
    "    ('Figure 9. CIW-5 Per-Book Test Breakdown', 'fig09_ciw5_per_book_test_breakdown.png',\n",
    "     'Book-level raw MAE and MA(5) MAE bars with correlation line for the test set.',\n",
    "     'Compare paired bars within each book to inspect smoothing gains and use line markers for correlation context.',\n",
    "     'The figure quantifies where trend-level gains are strongest and where residual uncertainty remains.'),\n",
    "    ('Figure 10. Contribution and Use-Cases Map', 'fig10_contribution_and_use_cases_map.png',\n",
    "     'Conceptual mapping from method components to research contributions and practical uses.',\n",
    "     'Read each row as input component -> methodological contribution -> usage pathway.',\n",
    "     'The project is positioned as a reusable semantic-to-time-series framework, with excitement as the demonstrated task.'),\n",
    "    ('Figure 11. Feature Cluster Member Trajectories (MA5)', 'fig11_feature_cluster_member_trajectories_ma5.png',\n",
    "     'A dedicated high-resolution view of MA(5) member trajectories for each feature cluster.',\n",
    "     'Each subplot represents one feature cluster with thin lines for member books and a bold centroid trajectory.',\n",
    "     'This figure provides direct visual evidence of pacing archetypes that define the final feature-cluster interpretation.'),\n",
    "]\n",
    "\n",
    "for title, fig_name, what_txt, how_txt, insight_txt in fig_sections:\n",
    "    final_report_lines.append(f'### {title}')\n",
    "    final_report_lines.append(f'![{title}](../outputs/final_report/figures/{fig_name})')\n",
    "    final_report_lines.append('')\n",
    "    final_report_lines.append(f'- What this figure shows: {what_txt}')\n",
    "    final_report_lines.append(f'- How to read it: {how_txt}')\n",
    "    final_report_lines.append(f'- Interpretation and insight: {insight_txt}')\n",
    "    final_report_lines.append('')\n",
    "\n",
    "final_report_lines.append('## 11. Utility, Generalization, and Deployment Scenarios')\n",
    "final_report_lines.append('The method is useful when a team needs interpretable, chunk-level semantic trajectories from high-dimensional embeddings and cannot afford heavy black-box sequence models. Since the student is linear and trained with explicit supervision, each run produces a transparent semantic basis that is fast to apply to new data.')\n",
    "final_report_lines.append('')\n",
    "final_report_lines.append('Potential use scenarios:')\n",
    "final_report_lines.append('1. Narrative pacing analytics for editorial workflow support.')\n",
    "final_report_lines.append('2. Cross-book comparative profiling for literary or media research.')\n",
    "final_report_lines.append('3. Teacher-student distillation pipeline for other abstract constructs (for example suspense, urgency, or emotional intensity).')\n",
    "final_report_lines.append('4. Lightweight semantic monitoring where interpretability and reproducibility are mandatory.')\n",
    "final_report_lines.append('')\n",
    "\n",
    "final_report_lines.append('## 12. Limitations and Threats to Validity')\n",
    "final_report_lines.append('1. Teacher labels are pseudo-ground truth and can contain systematic LLM bias.')\n",
    "final_report_lines.append('2. Corpus size is small (`20` novels), limiting external validity.')\n",
    "final_report_lines.append('3. The student is linear, so nonlinear semantic structure may be underfit.')\n",
    "final_report_lines.append('4. Cluster structure is sensitive to feature design and sample size.')\n",
    "final_report_lines.append('5. Correlation and R2 behavior can diverge from trend-error objectives, so objective choice must be explicit.')\n",
    "final_report_lines.append('')\n",
    "\n",
    "final_report_lines.append('## 13. Reproducibility and Artifact Guide')\n",
    "final_report_lines.append('Generated in stage `10_final_teacher_guided_semantic_basis_report.ipynb` with deterministic configuration (`SEED=42`, `MA_WINDOW=5`).')\n",
    "final_report_lines.append('')\n",
    "final_report_lines.append('Core evidence artifacts:')\n",
    "final_report_lines.append('1. `../outputs/final_report/tables/variant_selection_summary.csv`')\n",
    "final_report_lines.append('2. `../outputs/final_report/tables/variant_selection_diagnostics.csv`')\n",
    "final_report_lines.append('3. `../outputs/final_report/tables/dataset_profile_for_report.csv`')\n",
    "final_report_lines.append('4. `../outputs/final_report/tables/ciw5_per_book_deepdive.csv`')\n",
    "final_report_lines.append('5. `../outputs/final_report/tables/cluster_summary_for_report.csv`')\n",
    "final_report_lines.append('6. `../outputs/final_report/tables/key_results_registry.csv`')\n",
    "final_report_lines.append('7. `../outputs/final_report/tables/method_claims_checklist.csv`')\n",
    "final_report_lines.append('8. `../outputs/final_report/tables/report_integrity_checks.csv`')\n",
    "final_report_lines.append('')\n",
    "final_report_lines.append('Related appendix: `docs/OTHER_EXPERIMENTS.md` documents Twist Signal and PCA tracks as secondary experiments, intentionally separated from the main claim path.')\n",
    "final_report_lines.append('')\n",
    "\n",
    "final_report_lines.append('## 14. Conclusion')\n",
    "final_report_lines.append('This project demonstrates a concrete path for converting abstract semantics into interpretable time series by combining embedding trajectories, LLM teacher supervision, and linear semantic basis extraction. In this case study, CIW-5 is the most suitable teacher protocol under trend-first selection. The resulting signal supports meaningful clustering and practical downstream interpretation. More broadly, the workflow provides a reusable pattern for teacher-guided semantic projection where transparency, reproducibility, and analytical utility are first-class goals.')\n",
    "final_report_lines.append('')\n",
    "\n",
    "final_report_lines.append('## Claim Provenance')\n",
    "final_report_lines.append('- Core registry: `../outputs/final_report/tables/key_results_registry.csv`')\n",
    "final_report_lines.append('- Claim checklist: `../outputs/final_report/tables/method_claims_checklist.csv`')\n",
    "final_report_lines.append('- Report integrity: `../outputs/final_report/tables/report_integrity_checks.csv`')\n",
    "\n",
    "final_report_path = DOCS_ROOT / 'FINAL_REPORT.md'\n",
    "final_report_path.write_text(chr(10).join(final_report_lines) + chr(10), encoding='utf-8')\n",
    "\n",
    "# Keep the appendix narrative isolated\n",
    "other_lines = []\n",
    "other_lines.append('# Other Experiments: Twist Signal and PCA Tracks')\n",
    "other_lines.append('')\n",
    "other_lines.append('## Scope')\n",
    "other_lines.append('This document summarizes additional experiments completed in the project that are not part of the primary final claim. The primary claim is centered on Teacher-Guided Semantic Basis Projection. The experiments below remain valuable, but they are intentionally separated to keep narrative focus clear.')\n",
    "other_lines.append('')\n",
    "other_lines.append('## Twist Signal Track')\n",
    "other_lines.append('Twist Signal experiments model local novelty dynamics from embedding trajectories using `s_t` and acceleration `a_t`. This branch supports exploratory narrative-change analysis and peak detection.')\n",
    "other_lines.append('')\n",
    "other_lines.append('Key outputs:')\n",
    "other_lines.append('- `../outputs/features.csv`')\n",
    "other_lines.append('- `../outputs/clusters_kmeans.csv`')\n",
    "other_lines.append('- `../outputs/clusters_hier.csv`')\n",
    "other_lines.append('- `../outputs/dtw_distance_k7.npy`')\n",
    "other_lines.append('- `../outputs/eda/`')\n",
    "other_lines.append('')\n",
    "other_lines.append('## PCA Component Track')\n",
    "other_lines.append('PCA experiments analyze unsupervised axes of variance and their temporal behavior across books. This is useful for structural diagnostics and exploratory component interpretation.')\n",
    "other_lines.append('')\n",
    "other_lines.append('Key outputs:')\n",
    "other_lines.append('- `../outputs/pca/global_pca_fit.npz`')\n",
    "other_lines.append('- `../outputs/pca/global_pca_fit_meta.json`')\n",
    "other_lines.append('- `../outputs/pca/global_pca_variance_summary.csv`')\n",
    "other_lines.append('- `../outputs/pca_analysis/`')\n",
    "other_lines.append('')\n",
    "other_lines.append('## Why This Is Secondary in the Final Narrative')\n",
    "other_lines.append('The final narrative aims to evaluate supervised extraction of one specific semantic basis from embeddings using teacher labels. Twist Signal and PCA branches address different questions. They are retained as supporting evidence of broad project exploration and as future integration candidates, but they are not used as primary evidence for the teacher-guided semantic basis claim.')\n",
    "\n",
    "other_report_path = DOCS_ROOT / 'OTHER_EXPERIMENTS.md'\n",
    "other_report_path.write_text(chr(10).join(other_lines) + chr(10), encoding='utf-8')\n",
    "\n",
    "# Integrity checks for generated docs and image links\n",
    "fig_files = sorted([p.name for p in FINAL_FIG.glob('*.png')])\n",
    "required_figs = [\n",
    "    'fig01_pipeline_overview.png',\n",
    "    'fig02_variant_comparison_test_metrics.png',\n",
    "    'fig03_ciw5_model_behavior.png',\n",
    "    'fig04_ciw5_test_overlays_reference.png',\n",
    "    'fig05_feature_cluster_map.png',\n",
    "    'fig06_cluster_genre_composition.png',\n",
    "    'fig07_cluster_signatures_and_agreement.png',\n",
    "    'fig08_variant_rank_sensitivity.png',\n",
    "    'fig09_ciw5_per_book_test_breakdown.png',\n",
    "    'fig10_contribution_and_use_cases_map.png',\n",
    "    'fig11_feature_cluster_member_trajectories_ma5.png',\n",
    "]\n",
    "missing_figs = [f for f in required_figs if f not in fig_files]\n",
    "\n",
    "required_tables = [\n",
    "    'dataset_profile_for_report.csv',\n",
    "    'variant_selection_summary.csv',\n",
    "    'variant_selection_diagnostics.csv',\n",
    "    'ciw5_per_book_deepdive.csv',\n",
    "    'cluster_summary_for_report.csv',\n",
    "    'key_results_registry.csv',\n",
    "    'method_claims_checklist.csv',\n",
    "]\n",
    "missing_tables = [t for t in required_tables if not (FINAL_TAB / t).exists()]\n",
    "\n",
    "final_text = final_report_path.read_text(encoding='utf-8')\n",
    "other_text = other_report_path.read_text(encoding='utf-8')\n",
    "emdash_present = ('—' in final_text) or ('—' in other_text)\n",
    "\n",
    "# Validate embedded image links\n",
    "image_refs = re.findall(r'!\\[[^\\]]*\\]\\(([^)]+)\\)', final_text)\n",
    "missing_image_links = []\n",
    "for ref in image_refs:\n",
    "    resolved = (final_report_path.parent / ref).resolve()\n",
    "    if not resolved.exists():\n",
    "        missing_image_links.append(ref)\n",
    "\n",
    "# Claims completeness\n",
    "claims_df = pd.read_csv(FINAL_TAB / 'method_claims_checklist.csv')\n",
    "claims_all_mapped = bool((claims_df['status'] == 'mapped').all())\n",
    "\n",
    "# CIW-5 rank checks\n",
    "variant_diag_df = pd.read_csv(FINAL_TAB / 'variant_selection_diagnostics.csv')\n",
    "ciw5_rank = int(variant_diag_df[(variant_diag_df['split'] == 'test') & (variant_diag_df['variant_code'] == 'CIW-5')]['rank_trend_primary'].iloc[0])\n",
    "\n",
    "split_profile = pd.read_csv(FINAL_TAB / 'dataset_profile_for_report.csv')\n",
    "train_count = int((split_profile['split'] == 'train').sum())\n",
    "test_count = int((split_profile['split'] == 'test').sum())\n",
    "\n",
    "integrity_rows = []\n",
    "integrity_rows.extend(source_checks)\n",
    "integrity_rows.extend([\n",
    "    {\n",
    "        'check': 'dataset_profile_for_report_exists',\n",
    "        'expected': True,\n",
    "        'actual': (FINAL_TAB / 'dataset_profile_for_report.csv').exists(),\n",
    "        'pass': (FINAL_TAB / 'dataset_profile_for_report.csv').exists(),\n",
    "    },\n",
    "    {\n",
    "        'check': 'variant_selection_diagnostics_exists',\n",
    "        'expected': True,\n",
    "        'actual': (FINAL_TAB / 'variant_selection_diagnostics.csv').exists(),\n",
    "        'pass': (FINAL_TAB / 'variant_selection_diagnostics.csv').exists(),\n",
    "    },\n",
    "    {\n",
    "        'check': 'ciw5_per_book_deepdive_exists',\n",
    "        'expected': True,\n",
    "        'actual': (FINAL_TAB / 'ciw5_per_book_deepdive.csv').exists(),\n",
    "        'pass': (FINAL_TAB / 'ciw5_per_book_deepdive.csv').exists(),\n",
    "    },\n",
    "    {\n",
    "        'check': 'method_claims_checklist_exists',\n",
    "        'expected': True,\n",
    "        'actual': (FINAL_TAB / 'method_claims_checklist.csv').exists(),\n",
    "        'pass': (FINAL_TAB / 'method_claims_checklist.csv').exists(),\n",
    "    },\n",
    "    {\n",
    "        'check': 'required_table_count',\n",
    "        'expected': len(required_tables),\n",
    "        'actual': len(required_tables) - len(missing_tables),\n",
    "        'pass': len(missing_tables) == 0,\n",
    "    },\n",
    "    {\n",
    "        'check': 'curated_figure_count',\n",
    "        'expected': len(required_figs),\n",
    "        'actual': len(required_figs) - len(missing_figs),\n",
    "        'pass': len(missing_figs) == 0,\n",
    "    },\n",
    "    {\n",
    "        'check': 'final_report_exists',\n",
    "        'expected': True,\n",
    "        'actual': final_report_path.exists(),\n",
    "        'pass': final_report_path.exists(),\n",
    "    },\n",
    "    {\n",
    "        'check': 'other_experiments_exists',\n",
    "        'expected': True,\n",
    "        'actual': other_report_path.exists(),\n",
    "        'pass': other_report_path.exists(),\n",
    "    },\n",
    "    {\n",
    "        'check': 'embedded_image_links_exist',\n",
    "        'expected': True,\n",
    "        'actual': len(missing_image_links) == 0,\n",
    "        'pass': len(missing_image_links) == 0,\n",
    "    },\n",
    "    {\n",
    "        'check': 'claims_all_mapped',\n",
    "        'expected': True,\n",
    "        'actual': claims_all_mapped,\n",
    "        'pass': claims_all_mapped,\n",
    "    },\n",
    "    {\n",
    "        'check': 'no_emdash_in_docs',\n",
    "        'expected': True,\n",
    "        'actual': not emdash_present,\n",
    "        'pass': not emdash_present,\n",
    "    },\n",
    "    {\n",
    "        'check': 'ciw5_rank_1_in_diagnostics',\n",
    "        'expected': 1,\n",
    "        'actual': ciw5_rank,\n",
    "        'pass': ciw5_rank == 1,\n",
    "    },\n",
    "    {\n",
    "        'check': 'split_counts_are_16_4',\n",
    "        'expected': 'train=16,test=4',\n",
    "        'actual': f'train={train_count},test={test_count}',\n",
    "        'pass': train_count == 16 and test_count == 4,\n",
    "    },\n",
    "])\n",
    "\n",
    "integrity_df = pd.DataFrame(integrity_rows)\n",
    "integrity_df.to_csv(FINAL_TAB / 'report_integrity_checks.csv', index=False)\n",
    "\n",
    "print('Saved:', final_report_path)\n",
    "print('Saved:', other_report_path)\n",
    "print('Saved:', FINAL_TAB / 'report_integrity_checks.csv')\n",
    "print('Integrity all-pass:', bool(integrity_df['pass'].all()))\n",
    "if missing_figs:\n",
    "    print('Missing required figures:', missing_figs)\n",
    "if missing_tables:\n",
    "    print('Missing required tables:', missing_tables)\n",
    "if missing_image_links:\n",
    "    print('Broken image refs:', missing_image_links)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

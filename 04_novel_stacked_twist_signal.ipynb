{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 04 - All-Novel Stacked Twist Signal Visuals\n\nThis notebook generates **20 per-novel stacked figures** where each figure contains 3 vertical panels for `k=5,7,11`, plotting:\n- `s_t` (Twist Signal)\n- `a_t` (Twist acceleration)\n\nIt also exports:\n- clean grouped outputs under `outputs/eda/novel_stacks/`\n- summary tables for stats/highlights/manifest\n- consolidated interpretation markdown at `docs/NOVEL_STACKED_OUTPUT_INTERPRETATION.md`\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Install required packages if missing\nimport importlib\nimport subprocess\nimport sys\n\nREQUIRED_PACKAGES = [\n    (\"pandas\", \"pandas\"),\n    (\"numpy\", \"numpy\"),\n    (\"matplotlib\", \"matplotlib\"),\n]\n\nfor module_name, pip_name in REQUIRED_PACKAGES:\n    try:\n        importlib.import_module(module_name)\n    except ImportError:\n        print(f\"Installing {pip_name} ...\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name])\n\nprint(\"Dependency check complete.\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Setup"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from pathlib import Path\nimport json\nimport textwrap\nimport re\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\n\ntry:\n    from IPython.display import display\nexcept Exception:\n    def display(x):\n        print(x)\n\nSEED = 42\nnp.random.seed(SEED)\n\nK_VALUES = [5, 7, 11]\nPRIMARY_K = 7\nFIGSIZE = (16, 14)\n\nPROJECT_ROOT = Path(\".\").resolve()\nDATA_DIR = PROJECT_ROOT / \"data\"\nPROCESSED_DIR = DATA_DIR / \"processed\"\nMETADATA_PATH = DATA_DIR / \"metadata.csv\"\n\nOUTPUT_ROOT = PROJECT_ROOT / \"outputs\" / \"eda\" / \"novel_stacks\"\nFIG_DIR = OUTPUT_ROOT / \"figures\"\nTABLE_DIR = OUTPUT_ROOT / \"tables\"\nDOC_PATH = PROJECT_ROOT / \"docs\" / \"NOVEL_STACKED_OUTPUT_INTERPRETATION.md\"\n\nfor d in [OUTPUT_ROOT, FIG_DIR, TABLE_DIR, DOC_PATH.parent]:\n    d.mkdir(parents=True, exist_ok=True)\n\nprint(f\"Project root: {PROJECT_ROOT}\")\nprint(f\"Managed output root: {OUTPUT_ROOT}\")\nprint(f\"Interpretation doc path: {DOC_PATH}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Clean Managed Output Folder"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def clean_managed_output_dir(fig_dir: Path, table_dir: Path):\n    removed = []\n    for d in [fig_dir, table_dir]:\n        for p in d.glob(\"*\"):\n            if p.is_file():\n                p.unlink()\n                removed.append(str(p))\n    return removed\n\nremoved_files = clean_managed_output_dir(FIG_DIR, TABLE_DIR)\nprint(f\"Removed {len(removed_files)} managed files from previous run.\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Load and Validate Inputs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "metadata_df = pd.read_csv(METADATA_PATH)\nif \"id\" not in metadata_df.columns and \"pg_id\" in metadata_df.columns:\n    metadata_df[\"id\"] = metadata_df[\"pg_id\"]\n\nrequired_cols = [\"id\", \"title\", \"processed_dir\"]\nmissing_cols = [c for c in required_cols if c not in metadata_df.columns]\nif missing_cols:\n    raise ValueError(f\"metadata.csv missing required columns: {missing_cols}\")\n\nmetadata_df = metadata_df.sort_values(\"id\").reset_index(drop=True)\nunique_books = metadata_df[\"id\"].nunique()\nif unique_books != 20:\n    raise RuntimeError(f\"Expected 20 unique books, found {unique_books}\")\n\nnovel_payloads = []\n\nfor row in metadata_df.to_dict(orient=\"records\"):\n    book_id = int(row[\"id\"])\n    title = str(row[\"title\"])\n    processed_dir = str(row[\"processed_dir\"])\n    base = PROCESSED_DIR / processed_dir\n\n    if not base.exists():\n        raise FileNotFoundError(f\"Processed folder missing for book {book_id}: {base}\")\n\n    by_k = {}\n    lengths = []\n\n    for k in K_VALUES:\n        signal_path = base / f\"signals_k{k}.npz\"\n        if not signal_path.exists():\n            raise FileNotFoundError(f\"Missing signal file: {signal_path}\")\n\n        npz = np.load(signal_path)\n        if \"s\" not in npz.files or \"a\" not in npz.files:\n            raise RuntimeError(f\"signals_k{k}.npz must contain keys 's' and 'a' for book {book_id}\")\n\n        s = np.asarray(npz[\"s\"], dtype=np.float32)\n        a = np.asarray(npz[\"a\"], dtype=np.float32)\n\n        if s.shape[0] != a.shape[0] or s.shape[0] == 0:\n            raise RuntimeError(f\"Invalid signal lengths for book {book_id}, k={k}: len(s)={len(s)}, len(a)={len(a)}\")\n        if not np.isfinite(s).all() or not np.isfinite(a).all():\n            raise RuntimeError(f\"Non-finite values in signals for book {book_id}, k={k}\")\n\n        T = int(s.shape[0])\n        t = np.arange(T, dtype=np.int32)\n        t_norm = t / (T - 1) if T > 1 else np.zeros(T, dtype=np.float32)\n\n        peaks_path = base / f\"peaks_k{k}.json\"\n        if peaks_path.exists():\n            peaks_obj = json.loads(peaks_path.read_text(encoding=\"utf-8\"))\n            peak_indices = [int(x) for x in peaks_obj.get(\"peak_indices\", [])]\n        else:\n            peak_indices = []\n\n        peak_indices = [idx for idx in peak_indices if 0 <= idx < T]\n        peak_pos_norm = [float(idx / (T - 1)) if T > 1 else 0.0 for idx in peak_indices]\n\n        by_k[k] = {\n            \"s\": s,\n            \"a\": a,\n            \"T\": T,\n            \"chunk_index\": t,\n            \"t_norm\": t_norm,\n            \"peak_indices\": peak_indices,\n            \"peak_pos_norm\": peak_pos_norm,\n        }\n        lengths.append(T)\n\n    if len(set(lengths)) != 1:\n        raise RuntimeError(f\"Chunk lengths differ across k for book {book_id}: {lengths}\")\n\n    novel_payloads.append({\n        \"book_id\": book_id,\n        \"title\": title,\n        \"processed_dir\": processed_dir,\n        \"signals\": by_k,\n    })\n\nprint(f\"Validated books: {len(novel_payloads)}\")\nprint(\"Sample books:\", [(n[\"book_id\"], n[\"processed_dir\"]) for n in novel_payloads[:5]])\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Extract Stats, Manifest, and Highlights Tables"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def pad_peak_positions(positions, target=3):\n    out = list(positions[:target])\n    while len(out) < target:\n        out.append(-1.0)\n    return out\n\nstats_rows = []\nmanifest_rows = []\n\nfor novel in novel_payloads:\n    book_id = novel[\"book_id\"]\n    title = novel[\"title\"]\n    processed_dir = novel[\"processed_dir\"]\n\n    fig_name = f\"novel_{book_id}_{processed_dir}_stacked_k5_k7_k11.png\"\n    fig_rel = f\"outputs/eda/novel_stacks/figures/{fig_name}\"\n\n    manifest_rows.append({\n        \"book_id\": book_id,\n        \"title\": title,\n        \"processed_dir\": processed_dir,\n        \"figure_path\": fig_rel,\n        \"T_k5\": int(novel[\"signals\"][5][\"T\"]),\n        \"T_k7\": int(novel[\"signals\"][7][\"T\"]),\n        \"T_k11\": int(novel[\"signals\"][11][\"T\"]),\n    })\n\n    for k in K_VALUES:\n        sig = novel[\"signals\"][k]\n        s = sig[\"s\"]\n        a = sig[\"a\"]\n        T = sig[\"T\"]\n\n        peak_pos = pad_peak_positions(sig[\"peak_pos_norm\"], target=3)\n\n        stats_rows.append({\n            \"book_id\": int(book_id),\n            \"title\": title,\n            \"processed_dir\": processed_dir,\n            \"k\": int(k),\n            \"T\": int(T),\n            \"mean_s\": float(np.mean(s)),\n            \"std_s\": float(np.std(s)),\n            \"max_s\": float(np.max(s)),\n            \"mean_a\": float(np.mean(a)),\n            \"std_a\": float(np.std(a)),\n            \"max_a\": float(np.max(a)),\n            \"num_peaks\": int(len(sig[\"peak_indices\"])),\n            \"peak_pos_1\": float(peak_pos[0]),\n            \"peak_pos_2\": float(peak_pos[1]),\n            \"peak_pos_3\": float(peak_pos[2]),\n        })\n\nstats_df = pd.DataFrame(stats_rows).sort_values([\"book_id\", \"k\"]).reset_index(drop=True)\nmanifest_df = pd.DataFrame(manifest_rows).sort_values(\"book_id\").reset_index(drop=True)\n\nstats_path = TABLE_DIR / \"novel_stacked_stats.csv\"\nmanifest_path = TABLE_DIR / \"novel_stacked_manifest.csv\"\n\nstats_df.to_csv(stats_path, index=False)\nmanifest_df.to_csv(manifest_path, index=False)\n\nk5 = stats_df[stats_df[\"k\"] == 5][[\"book_id\", \"mean_s\", \"max_a\"]].rename(columns={\"mean_s\": \"mean_s_k5\", \"max_a\": \"max_a_k5\"})\nk7 = stats_df[stats_df[\"k\"] == 7][[\"book_id\", \"mean_s\", \"max_a\"]].rename(columns={\"mean_s\": \"mean_s_k7\", \"max_a\": \"max_a_k7\"})\nk11 = stats_df[stats_df[\"k\"] == 11][[\"book_id\", \"mean_s\", \"max_a\"]].rename(columns={\"mean_s\": \"mean_s_k11\", \"max_a\": \"max_a_k11\"})\n\nhighlights_df = (\n    metadata_df[[\"id\", \"title\", \"processed_dir\", \"genre_primary\", \"format\", \"twist_peak_rank\"]]\n    .rename(columns={\"id\": \"book_id\"})\n    .merge(k5, on=\"book_id\", how=\"left\")\n    .merge(k7, on=\"book_id\", how=\"left\")\n    .merge(k11, on=\"book_id\", how=\"left\")\n)\n\nhighlights_df[\"delta_mean_s_k11_k5\"] = highlights_df[\"mean_s_k11\"] - highlights_df[\"mean_s_k5\"]\nhighlights_df[\"delta_max_a_k11_k5\"] = highlights_df[\"max_a_k11\"] - highlights_df[\"max_a_k5\"]\n\nhighlights_df[\"rank_mean_s_k7_desc\"] = highlights_df[\"mean_s_k7\"].rank(ascending=False, method=\"min\").astype(int)\nhighlights_df[\"rank_max_a_k7_desc\"] = highlights_df[\"max_a_k7\"].rank(ascending=False, method=\"min\").astype(int)\n\nhighlights_df[\"is_top3_mean_s_k7\"] = highlights_df[\"rank_mean_s_k7_desc\"] <= 3\nhighlights_df[\"is_top3_max_a_k7\"] = highlights_df[\"rank_max_a_k7_desc\"] <= 3\n\nhighlights_df = highlights_df.sort_values(\"book_id\").reset_index(drop=True)\nhighlights_path = TABLE_DIR / \"novel_stacked_highlights.csv\"\nhighlights_df.to_csv(highlights_path, index=False)\n\nprint(f\"Saved {stats_path}\")\nprint(f\"Saved {manifest_path}\")\nprint(f\"Saved {highlights_path}\")\nprint(f\"Stats rows: {len(stats_df)} | Manifest rows: {len(manifest_df)} | Highlight rows: {len(highlights_df)}\")\n\ndisplay(highlights_df.head(8))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Generate 20 Stacked Figures (k=5,7,11)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def build_figure_filename(book_id: int, processed_dir: str):\n    safe_dir = re.sub(r\"[^a-zA-Z0-9_\\-]+\", \"_\", processed_dir).strip(\"_\")\n    return f\"novel_{book_id}_{safe_dir}_stacked_k5_k7_k11.png\"\n\n\ndef subplot_title(k: int, s: np.ndarray, a: np.ndarray, peak_count: int, T: int):\n    return (\n        f\"k={k} | T={T} | mean_s={np.mean(s):.3f} max_s={np.max(s):.3f} | \"\n        f\"mean_a={np.mean(a):.3f} max_a={np.max(a):.3f} | peaks={peak_count}\"\n    )\n\nsaved_fig_paths = []\n\nfor novel in novel_payloads:\n    book_id = novel[\"book_id\"]\n    title = novel[\"title\"]\n    processed_dir = novel[\"processed_dir\"]\n\n    fig_name = build_figure_filename(book_id, processed_dir)\n    fig_path = FIG_DIR / fig_name\n\n    # Keep a shared y-range within a book for k-to-k comparability.\n    all_values = []\n    for k in K_VALUES:\n        all_values.extend(novel[\"signals\"][k][\"s\"].tolist())\n        all_values.extend(novel[\"signals\"][k][\"a\"].tolist())\n    y_min = float(np.min(all_values))\n    y_max = float(np.max(all_values))\n    margin = 0.05 * (y_max - y_min if y_max > y_min else 1.0)\n    y_low, y_high = y_min - margin, y_max + margin\n\n    fig, axes = plt.subplots(nrows=3, ncols=1, figsize=FIGSIZE, sharex=True)\n\n    for ax, k in zip(axes, K_VALUES):\n        data = novel[\"signals\"][k]\n        t = data[\"chunk_index\"]\n        s = data[\"s\"]\n        a = data[\"a\"]\n        peak_indices = data[\"peak_indices\"]\n\n        ax.plot(t, s, color=\"#1f77b4\", linewidth=1.6, label=\"s_t (Twist Signal)\")\n        ax.plot(t, a, color=\"#d62728\", linewidth=1.2, alpha=0.9, label=\"a_t (Twist Acceleration)\")\n\n        if peak_indices:\n            ax.scatter(\n                np.array(peak_indices, dtype=int),\n                a[np.array(peak_indices, dtype=int)],\n                color=\"#2ca02c\",\n                s=28,\n                zorder=4,\n                label=\"Detected Peaks\",\n            )\n\n        ax.set_ylim(y_low, y_high)\n        ax.set_ylabel(\"Signal\")\n        ax.set_title(subplot_title(k, s, a, len(peak_indices), data[\"T\"]), fontsize=11)\n        ax.grid(alpha=0.28)\n\n    axes[-1].set_xlabel(\"Chunk Index\")\n    axes[0].legend(loc=\"upper right\", fontsize=9)\n\n    fig.suptitle(f\"{book_id} | {title} | processed_dir={processed_dir}\", fontsize=14)\n    fig.tight_layout(rect=[0, 0.02, 1, 0.97])\n    fig.savefig(fig_path, dpi=180, bbox_inches=\"tight\")\n    plt.close(fig)\n\n    saved_fig_paths.append(fig_path)\n\nprint(f\"Saved stacked figures: {len(saved_fig_paths)}\")\nprint(\"Example:\", saved_fig_paths[0] if saved_fig_paths else \"<none>\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Generate Consolidated Interpretation Markdown"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "k7_stats = stats_df[stats_df[\"k\"] == PRIMARY_K].copy().sort_values(\"book_id\").reset_index(drop=True)\nreport_df = highlights_df.merge(\n    k7_stats[[\"book_id\", \"num_peaks\", \"peak_pos_1\", \"peak_pos_2\", \"peak_pos_3\", \"std_a\"]],\n    on=\"book_id\",\n    how=\"left\",\n)\n\n# Quantile thresholds for deterministic, relative labeling.\nq_mean_lo = report_df[\"mean_s_k7\"].quantile(0.33)\nq_mean_hi = report_df[\"mean_s_k7\"].quantile(0.67)\nq_acc_lo = report_df[\"max_a_k7\"].quantile(0.33)\nq_acc_hi = report_df[\"max_a_k7\"].quantile(0.67)\n\nq_delta_mean_abs = report_df[\"delta_mean_s_k11_k5\"].abs().quantile(0.40)\nq_delta_maxa_abs = report_df[\"delta_max_a_k11_k5\"].abs().quantile(0.40)\n\n\ndef classify_level(v, low_thr, high_thr):\n    if v <= low_thr:\n        return \"Low\"\n    if v >= high_thr:\n        return \"High\"\n    return \"Medium\"\n\n\ndef classify_peak_timing(row):\n    vals = [row[\"peak_pos_1\"], row[\"peak_pos_2\"], row[\"peak_pos_3\"]]\n    vals = [float(v) for v in vals if float(v) >= 0]\n    if not vals:\n        return \"No detected top-3 peaks\"\n\n    vmin, vmax, vmean = min(vals), max(vals), float(np.mean(vals))\n    if vmin < 0.33 and vmax > 0.66:\n        return f\"Distributed across early-to-late arc (avg={vmean:.2f})\"\n    if vmean < 0.33:\n        return f\"Early-weighted (avg={vmean:.2f})\"\n    if vmean < 0.66:\n        return f\"Mid-story weighted (avg={vmean:.2f})\"\n    return f\"Late-weighted (avg={vmean:.2f})\"\n\n\ndef classify_k_dependence(row):\n    dm = float(row[\"delta_mean_s_k11_k5\"])\n    da = float(row[\"delta_max_a_k11_k5\"])\n\n    if abs(dm) <= q_delta_mean_abs and abs(da) <= q_delta_maxa_abs:\n        return \"Stable across k\"\n    if dm > q_delta_mean_abs and da > q_delta_maxa_abs:\n        return \"Increasing with larger k\"\n    if dm < -q_delta_mean_abs and da < -q_delta_maxa_abs:\n        return \"Decreasing with larger k\"\n    return \"Mixed sensitivity across k\"\n\n\ndef standout_sentence(novelty_level, accel_level, k_pattern, row):\n    title = row[\"title\"]\n    if novelty_level == \"High\" and accel_level == \"High\":\n        return f\"{title} combines high novelty and sharp shifts, forming a jagged trajectory profile.\"\n    if novelty_level == \"Low\" and accel_level == \"Low\":\n        return f\"{title} shows a smoother, lower-volatility progression relative to the corpus.\"\n    if novelty_level == \"High\" and accel_level != \"High\":\n        return f\"{title} is consistently novel but less spike-driven than the most volatile books.\"\n    if novelty_level == \"Low\" and accel_level == \"High\":\n        return f\"{title} has a calmer baseline punctuated by concentrated bursts of change.\"\n    return f\"{title} sits in a middle regime with {k_pattern.lower()} behavior across window sizes.\"\n\n\n# Global highlights\nk7_sorted_mean_desc = report_df.sort_values(\"mean_s_k7\", ascending=False)\nk7_sorted_mean_asc = report_df.sort_values(\"mean_s_k7\", ascending=True)\nk7_sorted_maxa_desc = report_df.sort_values(\"max_a_k7\", ascending=False)\n\nsensitivity_mean = report_df.reindex(report_df[\"delta_mean_s_k11_k5\"].abs().sort_values(ascending=False).index)\nsensitivity_maxa = report_df.reindex(report_df[\"delta_max_a_k11_k5\"].abs().sort_values(ascending=False).index)\n\nlines = []\nlines.append(\"# Novel Stacked Twist Signal Interpretation\")\nlines.append(\"\")\nlines.append(\"## Overview\")\nlines.append(\"This document interprets 20 stacked per-novel plots generated from `k=5,7,11`. Each figure has three vertical panels for one novel, and each panel overlays `s_t` (Twist Signal) and `a_t` (Twist Acceleration).\")\nlines.append(\"\")\nlines.append(\"How to read each panel:\")\nlines.append(\"- `s_t` tracks novelty versus recent narrative context.\")\nlines.append(\"- `a_t` tracks local novelty acceleration between consecutive chunks.\")\nlines.append(\"- Peak markers indicate top acceleration points for that `k`.\")\nlines.append(\"\")\nlines.append(\"## Global Highlights\")\nlines.append(\"\")\nlines.append(\"Top 3 by novelty (`mean_s`, k=7):\")\nfor _, r in k7_sorted_mean_desc.head(3).iterrows():\n    lines.append(f\"- {int(r['book_id'])} | {r['title']} | mean_s_k7={r['mean_s_k7']:.3f}\")\nlines.append(\"\")\nlines.append(\"Lowest 3 by novelty (`mean_s`, k=7):\")\nfor _, r in k7_sorted_mean_asc.head(3).iterrows():\n    lines.append(f\"- {int(r['book_id'])} | {r['title']} | mean_s_k7={r['mean_s_k7']:.3f}\")\nlines.append(\"\")\nlines.append(\"Top 3 by acceleration spikes (`max_a`, k=7):\")\nfor _, r in k7_sorted_maxa_desc.head(3).iterrows():\n    lines.append(f\"- {int(r['book_id'])} | {r['title']} | max_a_k7={r['max_a_k7']:.3f}\")\nlines.append(\"\")\nlines.append(\"Strongest k-sensitivity (`|delta_mean_s_k11_k5|`):\")\nfor _, r in sensitivity_mean.head(3).iterrows():\n    lines.append(f\"- {int(r['book_id'])} | {r['title']} | delta_mean_s_k11_k5={r['delta_mean_s_k11_k5']:.3f}\")\nlines.append(\"\")\nlines.append(\"Strongest k-sensitivity (`|delta_max_a_k11_k5|`):\")\nfor _, r in sensitivity_maxa.head(3).iterrows():\n    lines.append(f\"- {int(r['book_id'])} | {r['title']} | delta_max_a_k11_k5={r['delta_max_a_k11_k5']:.3f}\")\nlines.append(\"\")\nlines.append(\"Caveats:\")\nlines.append(\"- Labels are relative to this 20-book corpus and current embedding/signal settings.\")\nlines.append(\"- Peak extraction uses top-3 acceleration peaks and minimum separation defaults from the pipeline.\")\nlines.append(\"- Interpretive statements are descriptive and should be validated with additional settings/checks.\")\nlines.append(\"\")\nlines.append(\"## Per-Novel Interpretations\")\nlines.append(\"\")\n\nfor _, row in report_df.sort_values(\"book_id\").iterrows():\n    book_id = int(row[\"book_id\"])\n    title = str(row[\"title\"])\n    processed_dir = str(row[\"processed_dir\"])\n\n    fig_name = f\"novel_{book_id}_{processed_dir}_stacked_k5_k7_k11.png\"\n    fig_rel = f\"../outputs/eda/novel_stacks/figures/{fig_name}\"\n\n    novelty_level = classify_level(float(row[\"mean_s_k7\"]), q_mean_lo, q_mean_hi)\n    accel_level = classify_level(float(row[\"max_a_k7\"]), q_acc_lo, q_acc_hi)\n    peak_profile = classify_peak_timing(row)\n    k_pattern = classify_k_dependence(row)\n    standout = standout_sentence(novelty_level, accel_level, k_pattern, row)\n\n    lines.append(f\"### [{book_id}] {title}\")\n    lines.append(\"\")\n    lines.append(f\"![{title} stacked Twist Signal](%s)\" % fig_rel)\n    lines.append(\"\")\n    lines.append(f\"- Novelty level (k=7 mean_s={row['mean_s_k7']:.3f}): **{novelty_level}**\")\n    lines.append(f\"- Acceleration/volatility level (k=7 max_a={row['max_a_k7']:.3f}): **{accel_level}**\")\n    lines.append(f\"- Peak timing profile (k=7): {peak_profile}\")\n    lines.append(f\"- k-dependence pattern: {k_pattern} (delta_mean_s={row['delta_mean_s_k11_k5']:.3f}, delta_max_a={row['delta_max_a_k11_k5']:.3f})\")\n    lines.append(f\"- What stands out: {standout}\")\n    lines.append(\"\")\n\nmarkdown_text = \"\\n\".join(lines) + \"\\n\"\nDOC_PATH.write_text(markdown_text, encoding=\"utf-8\")\n\nprint(f\"Saved interpretation markdown: {DOC_PATH}\")\nprint(\"Preview:\")\nprint(\"\\n\".join(lines[:32]))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7) Validation Checks"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "fig_paths = sorted([p for p in FIG_DIR.glob(\"*.png\") if p.is_file()])\n\nstats_df_check = pd.read_csv(TABLE_DIR / \"novel_stacked_stats.csv\")\nmanifest_df_check = pd.read_csv(TABLE_DIR / \"novel_stacked_manifest.csv\")\nhighlights_df_check = pd.read_csv(TABLE_DIR / \"novel_stacked_highlights.csv\")\n\nchecks = []\nexpected_books = metadata_df[\"id\"].nunique()\n\nchecks.append({\"check\": \"book_count_is_20\", \"expected\": 20, \"actual\": int(expected_books), \"pass\": int(expected_books) == 20})\nchecks.append({\"check\": \"figure_count\", \"expected\": int(expected_books), \"actual\": len(fig_paths), \"pass\": len(fig_paths) == int(expected_books)})\nchecks.append({\"check\": \"stats_rows\", \"expected\": int(expected_books) * len(K_VALUES), \"actual\": len(stats_df_check), \"pass\": len(stats_df_check) == int(expected_books) * len(K_VALUES)})\nchecks.append({\"check\": \"manifest_rows\", \"expected\": int(expected_books), \"actual\": len(manifest_df_check), \"pass\": len(manifest_df_check) == int(expected_books)})\nchecks.append({\"check\": \"highlights_rows\", \"expected\": int(expected_books), \"actual\": len(highlights_df_check), \"pass\": len(highlights_df_check) == int(expected_books)})\n\ninterp_text = DOC_PATH.read_text(encoding=\"utf-8\")\nsection_count = interp_text.count(\"### [\")\nimage_count = interp_text.count(\"../outputs/eda/novel_stacks/figures/\")\nchecks.append({\"check\": \"interpretation_sections\", \"expected\": int(expected_books), \"actual\": int(section_count), \"pass\": int(section_count) == int(expected_books)})\nchecks.append({\"check\": \"interpretation_image_links\", \"expected\": int(expected_books), \"actual\": int(image_count), \"pass\": int(image_count) == int(expected_books)})\n\n# Verify image links resolve.\nmissing_links = []\nfor line in interp_text.splitlines():\n    if \"../outputs/eda/novel_stacks/figures/\" in line:\n        rel = line.split(\"(\", 1)[1].rsplit(\")\", 1)[0]\n        target = (DOC_PATH.parent / rel).resolve()\n        if not target.exists():\n            missing_links.append(str(target))\n\nchecks.append({\"check\": \"image_link_targets_exist\", \"expected\": 0, \"actual\": len(missing_links), \"pass\": len(missing_links) == 0})\n\nvalidation_df = pd.DataFrame(checks)\nvalidation_df.to_csv(TABLE_DIR / \"novel_stacked_validation_checks.csv\", index=False)\n\nif not validation_df[\"pass\"].all():\n    failed = validation_df[~validation_df[\"pass\"]]\n    raise RuntimeError(f\"Validation failed:\\n{failed}\")\n\nprint(\"All validation checks passed.\")\ndisplay(validation_df)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8) Outputs\nGenerated outputs are grouped and cleaned under:\n- `outputs/eda/novel_stacks/figures/` (20 stacked PNGs)\n- `outputs/eda/novel_stacks/tables/` (stats, manifest, highlights, validation)\n\nGenerated interpretation markdown:\n- `docs/NOVEL_STACKED_OUTPUT_INTERPRETATION.md`\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
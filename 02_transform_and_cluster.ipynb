{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 02 - Transform, Signals, Features, and Clustering\n\nThis notebook loads per-book embeddings, projects them with PCA, computes the Option B novelty/twist signal, extracts story-level features, runs clustering, and optionally computes DTW distance-based similarity.\n\n## Outputs\n- `./data/processed/{book_id}/pca_d2.npy`\n- `./data/processed/{book_id}/pca_d5.npy`\n- `./data/processed/{book_id}/signals_k{K}.npz`\n- `./data/processed/{book_id}/peaks_k{K}.json`\n- `./outputs/features.csv`\n- `./outputs/clusters_kmeans.csv`\n- `./outputs/clusters_hier.csv`\n- `./outputs/dtw_distance_k7.npy`\n- `./outputs/summary.md`\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Install required packages if missing\nimport importlib\nimport subprocess\nimport sys\n\nREQUIRED_PACKAGES = [\n    (\"numpy\", \"numpy\"),\n    (\"pandas\", \"pandas\"),\n    (\"sklearn\", \"scikit-learn\"),\n    (\"matplotlib\", \"matplotlib\"),\n]\n\nfor module_name, pip_name in REQUIRED_PACKAGES:\n    try:\n        importlib.import_module(module_name)\n    except ImportError:\n        print(f\"Installing {pip_name} ...\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name])\n\n# Try DTW package but allow fallback if unavailable.\ntry:\n    importlib.import_module(\"dtaidistance\")\n    print(\"dtaidistance available.\")\nexcept ImportError:\n    print(\"Attempting to install dtaidistance (optional) ...\")\n    try:\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"dtaidistance\"])\n    except Exception:\n        print(\"dtaidistance installation failed; pure-Python DTW fallback will be used.\")\n\nprint(\"Dependency check complete.\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from pathlib import Path\nimport ast\nimport json\nimport random\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import AgglomerativeClustering, KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\nPROJECT_ROOT = Path(\".\").resolve()\nDATA_DIR = PROJECT_ROOT / \"data\"\nPROCESSED_DIR = DATA_DIR / \"processed\"\nOUTPUTS_DIR = PROJECT_ROOT / \"outputs\"\nMETADATA_PATH = DATA_DIR / \"metadata.csv\"\n\nOUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n\nk_values = [7, 5, 11]\nprimary_k = 7\npca_dims = [2, 5]\ntop_K_peaks = 3\nmin_peak_separation = 3\nn_clusters_default = 4\nresample_len = 200\nmax_pca_fit_rows = 100_000\n\nprint(f\"Processed dir: {PROCESSED_DIR}\")\nprint(f\"k values: {k_values}\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "metadata_df = None\nbook_embeddings = {}\nbook_artifact_dirs = {}\n\nif METADATA_PATH.exists():\n    metadata_df = pd.read_csv(METADATA_PATH)\n    if \"id\" not in metadata_df.columns and \"pg_id\" in metadata_df.columns:\n        metadata_df[\"id\"] = metadata_df[\"pg_id\"]\n\n    if \"raw_filename\" not in metadata_df.columns:\n        metadata_df[\"raw_filename\"] = \"\"\n    if \"processed_dir\" not in metadata_df.columns:\n        metadata_df[\"processed_dir\"] = \"\"\n\n    for col in [\"genre_secondary\", \"short_tags\", \"citations\"]:\n        if col in metadata_df.columns:\n            def _parse_listlike(v):\n                if not isinstance(v, str) or not v.strip().startswith(\"[\"):\n                    return v\n                try:\n                    parsed = json.loads(v)\n                except Exception:\n                    try:\n                        parsed = ast.literal_eval(v)\n                    except Exception:\n                        return v\n                if isinstance(parsed, list):\n                    return \", \".join(str(x) for x in parsed)\n                return v\n            metadata_df[col] = metadata_df[col].apply(_parse_listlike)\n\n    for row in metadata_df.sort_values(\"id\").to_dict(orient=\"records\"):\n        book_id = str(int(row[\"id\"]))\n        processed_dir = str(row.get(\"processed_dir\", \"\") or \"\").strip()\n        if not processed_dir:\n            raw_filename = str(row.get(\"raw_filename\", \"\") or \"\").strip()\n            processed_dir = Path(raw_filename).stem if raw_filename else book_id\n\n        candidates = [PROCESSED_DIR / processed_dir, PROCESSED_DIR / book_id]\n        chosen_dir = None\n        for d in candidates:\n            emb_path = d / \"embeddings.npy\"\n            idx_path = d / \"index.json\"\n            if emb_path.exists() and idx_path.exists():\n                chosen_dir = d\n                break\n\n        if chosen_dir is None:\n            continue\n\n        emb = np.load(chosen_dir / \"embeddings.npy\")\n        if emb.ndim != 2 or emb.shape[0] < 2:\n            continue\n\n        book_embeddings[book_id] = emb.astype(np.float32, copy=False)\n        book_artifact_dirs[book_id] = chosen_dir\n\nelse:\n    # Fallback mode if metadata is absent\n    book_dirs = sorted([p for p in PROCESSED_DIR.iterdir() if p.is_dir()])\n    for d in book_dirs:\n        emb_path = d / \"embeddings.npy\"\n        idx_path = d / \"index.json\"\n        if not emb_path.exists() or not idx_path.exists():\n            continue\n\n        idx = json.loads(idx_path.read_text(encoding=\"utf-8\"))\n        book_id = str(idx.get(\"book_id\", d.name))\n        emb = np.load(emb_path)\n        if emb.ndim != 2 or emb.shape[0] < 2:\n            continue\n\n        book_embeddings[book_id] = emb.astype(np.float32, copy=False)\n        book_artifact_dirs[book_id] = d\n\nif not book_embeddings:\n    raise RuntimeError(\"No embeddings found under data/processed/* for the current metadata list\")\n\nprint(f\"Books with embeddings: {len(book_embeddings)}\")\nprint(\"Sample IDs:\", list(book_embeddings.keys())[:8])\nif metadata_df is not None:\n    print(f\"Metadata rows available: {len(metadata_df)}\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def compute_novelty_signal(emb: np.ndarray, k: int, eps: float = 1e-8):\n    T, D = emb.shape\n    s = np.zeros(T, dtype=np.float32)\n\n    csum = np.cumsum(emb, axis=0, dtype=np.float64)\n\n    for t in range(T):\n        if t == 0:\n            context = emb[0]\n        else:\n            start = max(0, t - k)\n            count = t - start\n            context_sum = csum[t - 1].copy()\n            if start > 0:\n                context_sum -= csum[start - 1]\n            context = context_sum / max(count, 1)\n\n        dot_val = float(np.dot(emb[t], context))\n        denom = float(np.linalg.norm(emb[t]) * np.linalg.norm(context) + eps)\n        cos_sim = dot_val / denom if denom > 0 else 0.0\n        s[t] = np.float32(1.0 - cos_sim)\n\n    a = np.zeros(T, dtype=np.float32)\n    if T > 1:\n        a[1:] = np.abs(np.diff(s)).astype(np.float32)\n\n    return s, a\n\n\ndef pick_top_peaks(signal: np.ndarray, top_k: int = 3, min_separation: int = 3):\n    if signal.size == 0:\n        return []\n\n    ranked = np.argsort(signal)[::-1]\n    selected = []\n\n    for idx in ranked:\n        idx = int(idx)\n        if any(abs(idx - s) < min_separation for s in selected):\n            continue\n        selected.append(idx)\n        if len(selected) >= top_k:\n            break\n\n    selected.sort()\n    return selected\n\n\ndef peak_positions_normalized(peak_indices, T: int):\n    if T <= 1:\n        return [0.0 for _ in peak_indices]\n    return [float(idx / (T - 1)) for idx in peak_indices]\n\n\ndef build_feature_row(book_id: str, k: int, s: np.ndarray, a: np.ndarray, peak_indices):\n    T = len(s)\n    peak_pos = peak_positions_normalized(peak_indices, T)\n\n    while len(peak_pos) < 3:\n        peak_pos.append(-1.0)\n\n    return {\n        \"book_id\": int(book_id),\n        \"k\": int(k),\n        \"T\": int(T),\n        \"mean_s\": float(np.mean(s)),\n        \"std_s\": float(np.std(s)),\n        \"max_s\": float(np.max(s)),\n        \"mean_a\": float(np.mean(a)),\n        \"std_a\": float(np.std(a)),\n        \"max_a\": float(np.max(a)),\n        \"num_peaks\": int(len(peak_indices)),\n        \"peak_pos_1\": float(peak_pos[0]),\n        \"peak_pos_2\": float(peak_pos[1]),\n        \"peak_pos_3\": float(peak_pos[2]),\n        \"auc_proxy_mean_s\": float(np.mean(s)),\n    }\n\n\ndef resample_signal(signal: np.ndarray, target_len: int = 200):\n    if signal.size == 0:\n        return np.zeros(target_len, dtype=np.float32)\n    if signal.size == 1:\n        return np.full(target_len, float(signal[0]), dtype=np.float32)\n\n    old_x = np.linspace(0.0, 1.0, num=signal.size)\n    new_x = np.linspace(0.0, 1.0, num=target_len)\n    return np.interp(new_x, old_x, signal).astype(np.float32)\n\n\ndef dtw_distance_py(x: np.ndarray, y: np.ndarray):\n    n = len(x)\n    m = len(y)\n    prev = np.full(m + 1, np.inf, dtype=np.float64)\n    prev[0] = 0.0\n\n    for i in range(1, n + 1):\n        curr = np.full(m + 1, np.inf, dtype=np.float64)\n        xi = x[i - 1]\n        for j in range(1, m + 1):\n            cost = abs(float(xi) - float(y[j - 1]))\n            curr[j] = cost + min(curr[j - 1], prev[j], prev[j - 1])\n        prev = curr\n\n    return float(prev[m])\n\n\ndef compute_dtw_distance_matrix(series_by_book: dict):\n    book_ids = sorted(series_by_book.keys(), key=lambda x: int(x))\n    n = len(book_ids)\n    dist = np.zeros((n, n), dtype=np.float32)\n\n    dtw_backend = \"python\"\n    dtw_lib = None\n    try:\n        from dtaidistance import dtw as dtw_lib\n        dtw_backend = \"dtaidistance\"\n    except Exception:\n        dtw_backend = \"python\"\n\n    for i in range(n):\n        xi = series_by_book[book_ids[i]]\n        for j in range(i + 1, n):\n            xj = series_by_book[book_ids[j]]\n            if dtw_backend == \"dtaidistance\":\n                try:\n                    d = float(dtw_lib.distance_fast(xi.astype(np.double), xj.astype(np.double), use_pruning=True))\n                except Exception:\n                    try:\n                        d = float(dtw_lib.distance(xi.astype(np.double), xj.astype(np.double)))\n                    except Exception:\n                        d = dtw_distance_py(xi, xj)\n            else:\n                d = dtw_distance_py(xi, xj)\n\n            dist[i, j] = dist[j, i] = np.float32(d)\n\n    return book_ids, dist, dtw_backend\n\n\ndef agglomerative_precomputed(dist_matrix: np.ndarray, n_clusters: int):\n    try:\n        model = AgglomerativeClustering(\n            n_clusters=n_clusters,\n            metric=\"precomputed\",\n            linkage=\"average\",\n        )\n    except TypeError:\n        model = AgglomerativeClustering(\n            n_clusters=n_clusters,\n            affinity=\"precomputed\",\n            linkage=\"average\",\n        )\n    return model.fit_predict(dist_matrix)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Fit PCA on pooled embeddings (or sample if large), then save per-book PCA projections\nall_book_ids = sorted(book_embeddings.keys(), key=lambda x: int(x))\nall_emb_list = [book_embeddings[b] for b in all_book_ids]\nall_emb = np.vstack(all_emb_list)\n\nprint(f\"Total embedding rows across all books: {all_emb.shape[0]}\")\n\nfit_matrix = all_emb\nif all_emb.shape[0] > max_pca_fit_rows:\n    rng = np.random.default_rng(SEED)\n    sample_idx = rng.choice(all_emb.shape[0], size=max_pca_fit_rows, replace=False)\n    fit_matrix = all_emb[sample_idx]\n    print(f\"Using sampled rows for PCA fit: {fit_matrix.shape[0]}\")\n\npca = PCA(n_components=5, svd_solver=\"randomized\", random_state=SEED)\npca.fit(fit_matrix)\n\nfor book_id in all_book_ids:\n    emb = book_embeddings[book_id]\n    z5 = pca.transform(emb).astype(np.float32)\n    z2 = z5[:, :2].astype(np.float32)\n\n    book_dir = book_artifact_dirs[book_id]\n    np.save(book_dir / \"pca_d5.npy\", z5)\n    np.save(book_dir / \"pca_d2.npy\", z2)\n\nprint(\"Saved PCA projections per book (d=2 and d=5).\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Compute novelty/twist signals, peaks, and story-level features\nfeature_rows = []\nsignal_store = {k: {} for k in k_values}\n\nfor book_id in all_book_ids:\n    emb = book_embeddings[book_id]\n    T = emb.shape[0]\n    book_dir = book_artifact_dirs[book_id]\n\n    for k in k_values:\n        s, a = compute_novelty_signal(emb, k=k)\n\n        if not np.isfinite(s).all() or not np.isfinite(a).all():\n            raise RuntimeError(f\"Non-finite values in signal for book {book_id}, k={k}\")\n        if len(s) != T or len(a) != T:\n            raise RuntimeError(f\"Signal length mismatch for book {book_id}, k={k}\")\n\n        np.savez(book_dir / f\"signals_k{k}.npz\", s=s.astype(np.float32), a=a.astype(np.float32))\n\n        peaks = pick_top_peaks(a, top_k=top_K_peaks, min_separation=min_peak_separation)\n        peak_pos = peak_positions_normalized(peaks, T)\n\n        peaks_payload = {\n            \"book_id\": int(book_id),\n            \"k\": int(k),\n            \"top_K\": int(top_K_peaks),\n            \"separation\": int(min_peak_separation),\n            \"peak_indices\": [int(x) for x in peaks],\n            \"peak_positions_norm\": [float(x) for x in peak_pos],\n            \"signal\": \"a_t\",\n        }\n        (book_dir / f\"peaks_k{k}.json\").write_text(json.dumps(peaks_payload, indent=2), encoding=\"utf-8\")\n\n        feature_rows.append(build_feature_row(book_id=book_id, k=k, s=s, a=a, peak_indices=peaks))\n        signal_store[k][book_id] = s.astype(np.float32)\n\nfeatures_df = pd.DataFrame(feature_rows)\nif features_df.empty:\n    raise RuntimeError(\"No feature rows generated.\")\n\nfeatures_df = features_df.sort_values([\"k\", \"book_id\"]).reset_index(drop=True)\n\nif metadata_df is not None and \"id\" in metadata_df.columns:\n    merge_cols = [\n        \"id\", \"title\", \"author\", \"first_publication_year\", \"origin_country\", \"original_language\",\n        \"format\", \"genre_primary\", \"genre_secondary\", \"short_tags\", \"recognizability_rank\",\n        \"genre_clarity_rank\", \"twist_peak_rank\", \"twist_peak_reason\", \"notes\", \"ebook_page_url\",\n        \"plain_text_utf8_url\", \"raw_filename\", \"processed_dir\", \"length\", \"status\", \"citations\",\n    ]\n    meta_subset = metadata_df[[c for c in merge_cols if c in metadata_df.columns]].copy()\n    meta_subset = meta_subset.rename(columns={\"id\": \"book_id\"})\n    features_df = features_df.merge(meta_subset, on=\"book_id\", how=\"left\")\n\nfeatures_path = OUTPUTS_DIR / \"features.csv\"\nfeatures_df.to_csv(features_path, index=False)\nprint(f\"Saved features: {features_path}\")\ndisplay(features_df.head(10))\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Feature-based clustering + DTW-based clustering (k=7)\nfeature_cols = [\n    \"mean_s\", \"std_s\", \"max_s\",\n    \"mean_a\", \"std_a\", \"max_a\",\n    \"num_peaks\", \"peak_pos_1\", \"peak_pos_2\", \"peak_pos_3\",\n    \"auc_proxy_mean_s\", \"T\",\n]\n\nkmeans_rows = []\nhier_rows = []\n\nfor k in sorted(features_df[\"k\"].unique()):\n    subset = features_df[features_df[\"k\"] == k].sort_values(\"book_id\").reset_index(drop=True)\n    X = subset[feature_cols].values\n\n    scaler = StandardScaler()\n    Xs = scaler.fit_transform(X)\n\n    km = KMeans(n_clusters=n_clusters_default, random_state=SEED, n_init=20)\n    km_labels = km.fit_predict(Xs)\n\n    ward = AgglomerativeClustering(n_clusters=n_clusters_default, linkage=\"ward\")\n    ward_labels = ward.fit_predict(Xs)\n\n    for i, book_id in enumerate(subset[\"book_id\"].tolist()):\n        kmeans_rows.append({\n            \"book_id\": int(book_id),\n            \"k\": int(k),\n            \"n_clusters\": int(n_clusters_default),\n            \"cluster\": int(km_labels[i]),\n        })\n        hier_rows.append({\n            \"book_id\": int(book_id),\n            \"k\": int(k),\n            \"mode\": \"feature_ward\",\n            \"n_clusters\": int(n_clusters_default),\n            \"cluster\": int(ward_labels[i]),\n        })\n\n# DTW matrix and clustering for primary k\nif primary_k not in signal_store or not signal_store[primary_k]:\n    raise RuntimeError(f\"No signals found for primary_k={primary_k}\")\n\nresampled = {\n    book_id: resample_signal(sig, target_len=resample_len)\n    for book_id, sig in signal_store[primary_k].items()\n}\n\nbook_ids_dtw, dtw_matrix, dtw_backend = compute_dtw_distance_matrix(resampled)\nif dtw_matrix.shape[0] != dtw_matrix.shape[1]:\n    raise RuntimeError(\"DTW distance matrix must be square\")\nif not np.allclose(dtw_matrix, dtw_matrix.T, atol=1e-6):\n    raise RuntimeError(\"DTW distance matrix must be symmetric\")\nif not np.allclose(np.diag(dtw_matrix), 0.0, atol=1e-6):\n    raise RuntimeError(\"DTW distance matrix must have zero diagonal\")\n\nnp.save(OUTPUTS_DIR / \"dtw_distance_k7.npy\", dtw_matrix)\n\ndtw_labels = agglomerative_precomputed(dtw_matrix, n_clusters=n_clusters_default)\nfor i, book_id in enumerate(book_ids_dtw):\n    hier_rows.append({\n        \"book_id\": int(book_id),\n        \"k\": int(primary_k),\n        \"mode\": \"dtw_average\",\n        \"n_clusters\": int(n_clusters_default),\n        \"cluster\": int(dtw_labels[i]),\n    })\n\nclusters_kmeans_df = pd.DataFrame(kmeans_rows).sort_values([\"k\", \"book_id\"]).reset_index(drop=True)\nclusters_hier_df = pd.DataFrame(hier_rows).sort_values([\"mode\", \"k\", \"book_id\"]).reset_index(drop=True)\n\nif metadata_df is not None and \"id\" in metadata_df.columns:\n    meta_cols = [\"id\", \"title\", \"author\", \"genre_primary\", \"format\", \"processed_dir\", \"first_publication_year\", \"recognizability_rank\", \"twist_peak_rank\"]\n    meta_subset = metadata_df[[c for c in meta_cols if c in metadata_df.columns]].copy().rename(columns={\"id\": \"book_id\"})\n    clusters_kmeans_df = clusters_kmeans_df.merge(meta_subset, on=\"book_id\", how=\"left\")\n    clusters_hier_df = clusters_hier_df.merge(meta_subset, on=\"book_id\", how=\"left\")\n\nclusters_kmeans_df.to_csv(OUTPUTS_DIR / \"clusters_kmeans.csv\", index=False)\nclusters_hier_df.to_csv(OUTPUTS_DIR / \"clusters_hier.csv\", index=False)\n\nprint(f\"DTW backend used: {dtw_backend}\")\nprint(f\"Saved KMeans clusters: {OUTPUTS_DIR / 'clusters_kmeans.csv'}\")\nprint(f\"Saved hierarchical clusters: {OUTPUTS_DIR / 'clusters_hier.csv'}\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Minimal sanity plots and summary report\nexample_ids = sorted(signal_store[primary_k].keys(), key=lambda x: int(x))[:2]\n\nif example_ids:\n    fig, axes = plt.subplots(len(example_ids), 1, figsize=(12, 4 * len(example_ids)), sharex=False)\n    if len(example_ids) == 1:\n        axes = [axes]\n\n    for ax, book_id in zip(axes, example_ids):\n        s = signal_store[primary_k][book_id]\n        emb = book_embeddings[book_id]\n        _, a = compute_novelty_signal(emb, k=primary_k)\n\n        ax.plot(s, label=\"s_t (novelty)\", linewidth=1.3)\n        ax.plot(a, label=\"a_t (acceleration)\", linewidth=1.0, alpha=0.8)\n        ax.set_title(f\"Book {book_id} signals (k={primary_k})\")\n        ax.set_xlabel(\"Chunk index\")\n        ax.set_ylabel(\"Signal value\")\n        ax.legend(loc=\"upper right\")\n\n    fig.tight_layout()\n    sanity_plot_path = OUTPUTS_DIR / \"sanity_signal_examples.png\"\n    fig.savefig(sanity_plot_path, dpi=140)\n    plt.show()\nelse:\n    sanity_plot_path = None\n\nsummary_lines = [\n    \"# Story Trajectory Pipeline Summary\",\n    \"\",\n    \"## Created artifacts\",\n    f\"- Books processed: {len(all_book_ids)}\",\n    f\"- Feature rows: {len(features_df)}\",\n    f\"- k values: {k_values}\",\n    f\"- PCA dimensions saved per book: {pca_dims}\",\n    f\"- KMeans clusters file: {OUTPUTS_DIR / 'clusters_kmeans.csv'}\",\n    f\"- Hierarchical clusters file: {OUTPUTS_DIR / 'clusters_hier.csv'}\",\n    f\"- DTW distance matrix: {OUTPUTS_DIR / 'dtw_distance_k7.npy'}\",\n    f\"- DTW resample length: {resample_len}\",\n    f\"- Metadata file: {METADATA_PATH} (exists={METADATA_PATH.exists()})\",\n]\n\nif sanity_plot_path is not None:\n    summary_lines.append(f\"- Sanity plot: {sanity_plot_path}\")\n\nsummary_text = \"\\n\".join(summary_lines) + \"\\n\"\nsummary_path = OUTPUTS_DIR / \"summary.md\"\nsummary_path.write_text(summary_text, encoding=\"utf-8\")\n\nprint(f\"Saved summary: {summary_path}\")\nprint(summary_text)\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
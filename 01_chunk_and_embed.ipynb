{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 - Chunk and Embed Cleaned Books\n",
        "\n",
        "This notebook loads cleaned Gutenberg text files, chunks each book into overlapping word windows, and computes sentence-transformer embeddings.\n",
        "\n",
        "## Outputs per book\n",
        "- `./data/processed/{book_id}/chunks.jsonl`\n",
        "- `./data/processed/{book_id}/embeddings.npy`\n",
        "- `./data/processed/{book_id}/index.json`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "65089c17",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kongfha/miniconda3/envs/work313/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dependency check complete.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages if missing\n",
        "import importlib\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "REQUIRED_PACKAGES = [\n",
        "    (\"numpy\", \"numpy\"),\n",
        "    (\"pandas\", \"pandas\"),\n",
        "    (\"sentence_transformers\", \"sentence-transformers\"),\n",
        "]\n",
        "\n",
        "for module_name, pip_name in REQUIRED_PACKAGES:\n",
        "    try:\n",
        "        importlib.import_module(module_name)\n",
        "    except ImportError:\n",
        "        print(f\"Installing {pip_name} ...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name])\n",
        "\n",
        "print(\"Dependency check complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a88c4698",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "window_words=300, stride_words=100\n",
            "model=sentence-transformers/all-mpnet-base-v2, batch_size=64\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import json\n",
        "import random\n",
        "import shutil\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "PROJECT_ROOT = Path(\".\").resolve()\n",
        "DATA_DIR = PROJECT_ROOT / \"data\"\n",
        "RAW_DIR = DATA_DIR / \"raw\"\n",
        "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
        "METADATA_PATH = DATA_DIR / \"metadata.csv\"\n",
        "\n",
        "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "window_words = 300\n",
        "stride_words = 100\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "batch_size = 64\n",
        "force_recompute = False\n",
        "text_preview_chars = 220\n",
        "show_progress_bar = True\n",
        "\n",
        "print(f\"window_words={window_words}, stride_words={stride_words}\")\n",
        "print(f\"model={model_name}, batch_size={batch_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "537fca13",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Books to process: 20\n"
          ]
        }
      ],
      "source": [
        "if not METADATA_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Missing metadata file: {METADATA_PATH}\")\n",
        "\n",
        "metadata_df = pd.read_csv(METADATA_PATH)\n",
        "if \"id\" not in metadata_df.columns:\n",
        "    if \"pg_id\" in metadata_df.columns:\n",
        "        metadata_df[\"id\"] = metadata_df[\"pg_id\"]\n",
        "    else:\n",
        "        raise ValueError(\"metadata.csv must contain an 'id' or 'pg_id' column.\")\n",
        "\n",
        "if \"raw_filename\" not in metadata_df.columns:\n",
        "    metadata_df[\"raw_filename\"] = \"\"\n",
        "if \"processed_dir\" not in metadata_df.columns:\n",
        "    metadata_df[\"processed_dir\"] = \"\"\n",
        "\n",
        "book_records = []\n",
        "missing_raw = []\n",
        "\n",
        "for row in metadata_df.sort_values(\"id\").to_dict(orient=\"records\"):\n",
        "    book_id = int(row[\"id\"])\n",
        "    raw_filename = str(row.get(\"raw_filename\", \"\") or \"\").strip()\n",
        "\n",
        "    candidate_paths = []\n",
        "    if raw_filename:\n",
        "        candidate_paths.append(RAW_DIR / raw_filename)\n",
        "    candidate_paths.append(RAW_DIR / f\"{book_id}.txt\")\n",
        "\n",
        "    resolved_path = None\n",
        "    for p in candidate_paths:\n",
        "        if p.exists():\n",
        "            resolved_path = p\n",
        "            break\n",
        "\n",
        "    if resolved_path is None:\n",
        "        missing_raw.append({\"book_id\": book_id, \"checked\": [str(p) for p in candidate_paths]})\n",
        "        continue\n",
        "\n",
        "    processed_dir = str(row.get(\"processed_dir\", \"\") or \"\").strip()\n",
        "    if not processed_dir:\n",
        "        processed_dir = Path(resolved_path).stem\n",
        "    processed_dir = re.sub(r\"[^a-zA-Z0-9_\\-]+\", \"_\", processed_dir).strip(\"_\")\n",
        "    if not processed_dir:\n",
        "        processed_dir = str(book_id)\n",
        "\n",
        "    book_records.append({\n",
        "        \"book_id\": book_id,\n",
        "        \"title\": row.get(\"title\", f\"Book_{book_id}\"),\n",
        "        \"text_path\": resolved_path,\n",
        "        \"raw_filename\": resolved_path.name,\n",
        "        \"processed_dir\": processed_dir,\n",
        "    })\n",
        "\n",
        "if not book_records:\n",
        "    raise ValueError(\"No readable raw text files found from metadata.csv\")\n",
        "\n",
        "if missing_raw:\n",
        "    missing_ids = [m[\"book_id\"] for m in missing_raw[:10]]\n",
        "    raise FileNotFoundError(f\"Missing raw text files for IDs: {missing_ids}\")\n",
        "\n",
        "book_ids = [r[\"book_id\"] for r in book_records]\n",
        "print(f\"Books to process: {len(book_records)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d1b7c034",
      "metadata": {},
      "outputs": [],
      "source": [
        "def chunk_words(text: str, window_words: int = 300, stride_words: int = 100):\n",
        "    words = re.findall(r\"\\S+\", text)\n",
        "    n = len(words)\n",
        "    if n == 0:\n",
        "        return []\n",
        "\n",
        "    chunks = []\n",
        "    min_tail_words = max(80, window_words // 3)\n",
        "\n",
        "    if n <= window_words:\n",
        "        chunk_text = \" \".join(words)\n",
        "        chunks.append({\n",
        "            \"chunk_index\": 0,\n",
        "            \"start_word\": 0,\n",
        "            \"end_word\": n,\n",
        "            \"text\": chunk_text,\n",
        "            \"text_preview\": chunk_text[:text_preview_chars],\n",
        "        })\n",
        "        return chunks\n",
        "\n",
        "    chunk_index = 0\n",
        "    for start in range(0, n, stride_words):\n",
        "        end = min(start + window_words, n)\n",
        "        current_len = end - start\n",
        "\n",
        "        if start > 0 and end == n and current_len < min_tail_words:\n",
        "            break\n",
        "\n",
        "        chunk_text = \" \".join(words[start:end])\n",
        "        chunks.append({\n",
        "            \"chunk_index\": chunk_index,\n",
        "            \"start_word\": int(start),\n",
        "            \"end_word\": int(end),\n",
        "            \"text\": chunk_text,\n",
        "            \"text_preview\": chunk_text[:text_preview_chars],\n",
        "        })\n",
        "        chunk_index += 1\n",
        "\n",
        "        if end >= n:\n",
        "            break\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def artifact_paths(book_id: int, processed_dir_name: str):\n",
        "    book_dir = PROCESSED_DIR / processed_dir_name\n",
        "    legacy_dir = PROCESSED_DIR / str(book_id)\n",
        "\n",
        "    return {\n",
        "        \"book_dir\": book_dir,\n",
        "        \"chunks\": book_dir / \"chunks.jsonl\",\n",
        "        \"embeddings\": book_dir / \"embeddings.npy\",\n",
        "        \"index\": book_dir / \"index.json\",\n",
        "        \"legacy_dir\": legacy_dir,\n",
        "        \"legacy_chunks\": legacy_dir / \"chunks.jsonl\",\n",
        "        \"legacy_embeddings\": legacy_dir / \"embeddings.npy\",\n",
        "        \"legacy_index\": legacy_dir / \"index.json\",\n",
        "    }\n",
        "\n",
        "\n",
        "def _load_jsonl(path: Path):\n",
        "    records = []\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            records.append(json.loads(line))\n",
        "    return records\n",
        "\n",
        "\n",
        "def _is_cache_valid(index_obj: dict):\n",
        "    expected = {\n",
        "        \"window_words\": window_words,\n",
        "        \"stride_words\": stride_words,\n",
        "        \"model_name\": model_name,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"dtype\": \"float32\",\n",
        "    }\n",
        "    for k, v in expected.items():\n",
        "        if index_obj.get(k) != v:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def _maybe_migrate_legacy_cache(paths: dict):\n",
        "    if paths[\"book_dir\"].exists():\n",
        "        return\n",
        "\n",
        "    if not paths[\"legacy_dir\"].exists():\n",
        "        return\n",
        "\n",
        "    needed = [\"legacy_chunks\", \"legacy_embeddings\", \"legacy_index\"]\n",
        "    if not all(paths[k].exists() for k in needed):\n",
        "        return\n",
        "\n",
        "    paths[\"book_dir\"].mkdir(parents=True, exist_ok=True)\n",
        "    shutil.copy2(paths[\"legacy_chunks\"], paths[\"chunks\"])\n",
        "    shutil.copy2(paths[\"legacy_embeddings\"], paths[\"embeddings\"])\n",
        "    shutil.copy2(paths[\"legacy_index\"], paths[\"index\"])\n",
        "\n",
        "\n",
        "def load_or_embed_chunks(book_id: int, processed_dir_name: str, text: str, model):\n",
        "    paths = artifact_paths(book_id=book_id, processed_dir_name=processed_dir_name)\n",
        "    _maybe_migrate_legacy_cache(paths)\n",
        "    paths[\"book_dir\"].mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if (\n",
        "        not force_recompute\n",
        "        and paths[\"chunks\"].exists()\n",
        "        and paths[\"embeddings\"].exists()\n",
        "        and paths[\"index\"].exists()\n",
        "    ):\n",
        "        try:\n",
        "            index_obj = json.loads(paths[\"index\"].read_text(encoding=\"utf-8\"))\n",
        "            if _is_cache_valid(index_obj):\n",
        "                cached_chunks = _load_jsonl(paths[\"chunks\"])\n",
        "                cached_embeddings = np.load(paths[\"embeddings\"])\n",
        "                if (\n",
        "                    cached_embeddings.dtype == np.float32\n",
        "                    and cached_embeddings.ndim == 2\n",
        "                    and cached_embeddings.shape[0] == len(cached_chunks)\n",
        "                ):\n",
        "                    return cached_chunks, cached_embeddings, True\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    chunks = chunk_words(text=text, window_words=window_words, stride_words=stride_words)\n",
        "    if not chunks:\n",
        "        raise ValueError(f\"No chunks generated for book {book_id}\")\n",
        "\n",
        "    chunk_texts = [c[\"text\"] for c in chunks]\n",
        "    embeddings = model.encode(\n",
        "        chunk_texts,\n",
        "        batch_size=batch_size,\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=False,\n",
        "        show_progress_bar=show_progress_bar,\n",
        "    )\n",
        "    embeddings = np.asarray(embeddings, dtype=np.float32)\n",
        "\n",
        "    if embeddings.shape[0] != len(chunks):\n",
        "        raise RuntimeError(\n",
        "            f\"Embedding row mismatch for book {book_id}: {embeddings.shape[0]} vs {len(chunks)}\"\n",
        "        )\n",
        "\n",
        "    with paths[\"chunks\"].open(\"w\", encoding=\"utf-8\") as f:\n",
        "        for c in chunks:\n",
        "            row = {\n",
        "                \"chunk_index\": int(c[\"chunk_index\"]),\n",
        "                \"start_word\": int(c[\"start_word\"]),\n",
        "                \"end_word\": int(c[\"end_word\"]),\n",
        "                \"text_preview\": c[\"text_preview\"],\n",
        "            }\n",
        "            f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    np.save(paths[\"embeddings\"], embeddings)\n",
        "\n",
        "    index_obj = {\n",
        "        \"book_id\": int(book_id),\n",
        "        \"processed_dir\": str(processed_dir_name),\n",
        "        \"T\": int(embeddings.shape[0]),\n",
        "        \"D\": int(embeddings.shape[1]),\n",
        "        \"window_words\": int(window_words),\n",
        "        \"stride_words\": int(stride_words),\n",
        "        \"model_name\": model_name,\n",
        "        \"batch_size\": int(batch_size),\n",
        "        \"dtype\": \"float32\",\n",
        "        \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    }\n",
        "    paths[\"index\"].write_text(json.dumps(index_obj, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "    lightweight_chunks = [\n",
        "        {\n",
        "            \"chunk_index\": int(c[\"chunk_index\"]),\n",
        "            \"start_word\": int(c[\"start_word\"]),\n",
        "            \"end_word\": int(c[\"end_word\"]),\n",
        "            \"text_preview\": c[\"text_preview\"],\n",
        "        }\n",
        "        for c in chunks\n",
        "    ]\n",
        "    return lightweight_chunks, embeddings, False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a27cfdc0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading embedding model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 1992.47it/s, Materializing param=pooler.dense.weight]                        \n",
            "\u001b[1mMPNetModel LOAD REPORT\u001b[0m from: sentence-transformers/all-mpnet-base-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
            "Batches: 100%|██████████| 5/5 [00:33<00:00,  6.74s/it]\n",
            "/var/folders/2m/my6rfccj2lqgy6_6y84lqwsw0000gn/T/ipykernel_57843/60254591.py:144: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[computed] 11 (alice_s_adventures_wonderland.txt): T=264, D=768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 8/8 [01:32<00:00, 11.55s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[computed] 16 (peter_pan.txt): T=471, D=768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 6/6 [00:43<00:00,  7.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[computed] 35 (time_machine.txt): T=323, D=768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 10/10 [01:45<00:00, 10.56s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[computed] 36 (war_worlds.txt): T=599, D=768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 4/4 [00:53<00:00, 13.44s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[computed] 43 (strange_case_dr_jekyll.txt): T=255, D=768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 7/7 [00:39<00:00,  5.58s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[computed] 55 (wonderful_wizard_oz.txt): T=395, D=768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 12/12 [01:10<00:00,  5.88s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[computed] 84 (frankenstein_modern_prometheus.txt): T=749, D=768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches:  20%|██        | 2/10 [06:16<24:32, 184.05s/it]"
          ]
        }
      ],
      "source": [
        "print(\"Loading embedding model...\")\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "book_stats = []\n",
        "total_chunks = 0\n",
        "processed_books = 0\n",
        "\n",
        "for rec in book_records:\n",
        "    book_id = int(rec[\"book_id\"])\n",
        "    title = rec.get(\"title\", f\"Book_{book_id}\")\n",
        "    processed_dir = rec.get(\"processed_dir\", str(book_id))\n",
        "    text_path = Path(rec[\"text_path\"])\n",
        "\n",
        "    if not text_path.exists():\n",
        "        print(f\"[skip] missing raw text for {book_id}: {text_path.name}\")\n",
        "        continue\n",
        "\n",
        "    text = text_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "    chunks, embeddings, used_cache = load_or_embed_chunks(\n",
        "        book_id=book_id,\n",
        "        processed_dir_name=processed_dir,\n",
        "        text=text,\n",
        "        model=model,\n",
        "    )\n",
        "\n",
        "    if embeddings.shape[0] != len(chunks):\n",
        "        raise RuntimeError(f\"Validation failed for {book_id}: embeddings/chunks mismatch\")\n",
        "\n",
        "    total_chunks += embeddings.shape[0]\n",
        "    processed_books += 1\n",
        "\n",
        "    book_stats.append({\n",
        "        \"book_id\": int(book_id),\n",
        "        \"title\": title,\n",
        "        \"raw_filename\": text_path.name,\n",
        "        \"processed_dir\": processed_dir,\n",
        "        \"chunks\": int(embeddings.shape[0]),\n",
        "        \"dim\": int(embeddings.shape[1]),\n",
        "        \"cached\": bool(used_cache),\n",
        "    })\n",
        "\n",
        "    cache_tag = \"cached\" if used_cache else \"computed\"\n",
        "    print(f\"[{cache_tag}] {book_id} ({processed_dir}): T={embeddings.shape[0]}, D={embeddings.shape[1]}\")\n",
        "\n",
        "if processed_books == 0:\n",
        "    raise RuntimeError(\"No books were processed in notebook 1.\")\n",
        "\n",
        "stats_df = pd.DataFrame(book_stats)\n",
        "print(\"\\nEmbedding summary\")\n",
        "print(f\"Processed books: {processed_books}\")\n",
        "print(f\"Total chunks: {total_chunks}\")\n",
        "print(f\"Average chunks/book: {total_chunks / processed_books:.2f}\")\n",
        "display(stats_df.head(10))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "work313",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 09 indep_winsize_5 Clustering and Insight Workflow\n\nUnsupervised clustering workflow focused on `label_indep_winsize_5.npy`.\n\nApproach:\n- Feature-based clustering (`KMeans` + `Agglomerative Ward`) with model selection on `k=2..6`.\n- DTW-based clustering on resampled trajectories for shape validation.\n- Cross-method agreement + representative books + narrative insights.\n\nThis run uses `MA_WINDOW=5` for smoothing-derived features and centroid trajectory plots.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from __future__ import annotations\n\nfrom pathlib import Path\nfrom itertools import combinations\nimport json\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import (\n    silhouette_score,\n    davies_bouldin_score,\n    calinski_harabasz_score,\n    adjusted_rand_score,\n    normalized_mutual_info_score,\n)\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nfrom scipy.spatial.distance import squareform\n\nSEED = 42\nrng = np.random.default_rng(SEED)\nnp.random.seed(SEED)\n\nPROJECT_ROOT = Path('.').resolve()\nDATA_DIR = PROJECT_ROOT / 'data'\nPROCESSED_DIR = DATA_DIR / 'processed'\nMETADATA_PATH = DATA_DIR / 'metadata.csv'\n\nOUT_ROOT = PROJECT_ROOT / 'outputs' / 'excitement_indep_clustering'\nFIG_DIR = OUT_ROOT / 'figures'\nTABLE_DIR = OUT_ROOT / 'tables'\nfor d in [OUT_ROOT, FIG_DIR, TABLE_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n\nLABEL_FILE = 'label_indep_winsize_5.npy'\nEMB_FILE = 'embeddings.npy'\n\nK_RANGE = list(range(2, 7))\nN_STABILITY_SEEDS = 20\nL_DTW = 200\nMA_WINDOW = 5\nEPS = 1e-8\n\nprint(f'PROJECT_ROOT={PROJECT_ROOT}')\nprint(f'OUT_ROOT={OUT_ROOT}')\nprint(f'LABEL_FILE={LABEL_FILE}, K_RANGE={K_RANGE}, L_DTW={L_DTW}, MA_WINDOW={MA_WINDOW}')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label_shape(y: np.ndarray, label_path: Path) -> np.ndarray:\n",
    "    y = np.asarray(y)\n",
    "    if y.ndim == 1:\n",
    "        return y\n",
    "    if y.ndim == 2 and 1 in y.shape:\n",
    "        return y.reshape(-1)\n",
    "    raise ValueError(f'Unsupported label shape {y.shape} at {label_path}')\n",
    "\n",
    "\n",
    "def moving_average_1d(x: np.ndarray, window: int) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float64).reshape(-1)\n",
    "    if window <= 1:\n",
    "        return x.copy()\n",
    "    if window % 2 == 0:\n",
    "        raise ValueError(f'MA window must be odd, got {window}')\n",
    "    return pd.Series(x).rolling(window=window, center=True, min_periods=1).mean().to_numpy()\n",
    "\n",
    "\n",
    "def safe_corr(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    a = np.asarray(a, dtype=np.float64).reshape(-1)\n",
    "    b = np.asarray(b, dtype=np.float64).reshape(-1)\n",
    "    if len(a) < 2 or len(b) < 2:\n",
    "        return 0.0\n",
    "    if np.std(a) < EPS or np.std(b) < EPS:\n",
    "        return 0.0\n",
    "    c = np.corrcoef(a, b)[0, 1]\n",
    "    if not np.isfinite(c):\n",
    "        return 0.0\n",
    "    return float(c)\n",
    "\n",
    "\n",
    "def dtw_distance(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    a = np.asarray(a, dtype=np.float64).reshape(-1)\n",
    "    b = np.asarray(b, dtype=np.float64).reshape(-1)\n",
    "    n, m = len(a), len(b)\n",
    "    dp = np.full((n + 1, m + 1), np.inf, dtype=np.float64)\n",
    "    dp[0, 0] = 0.0\n",
    "    for i in range(1, n + 1):\n",
    "        ai = a[i - 1]\n",
    "        for j in range(1, m + 1):\n",
    "            cost = abs(ai - b[j - 1])\n",
    "            dp[i, j] = cost + min(dp[i - 1, j], dp[i, j - 1], dp[i - 1, j - 1])\n",
    "    return float(dp[n, m])\n",
    "\n",
    "\n",
    "def resample_to_len(y: np.ndarray, L: int) -> np.ndarray:\n",
    "    y = np.asarray(y, dtype=np.float64).reshape(-1)\n",
    "    if len(y) == L:\n",
    "        return y.copy()\n",
    "    x_old = np.linspace(0.0, 1.0, len(y))\n",
    "    x_new = np.linspace(0.0, 1.0, L)\n",
    "    return np.interp(x_new, x_old, y)\n",
    "\n",
    "\n",
    "def compute_features(y: np.ndarray, ma_window: int) -> dict:\n",
    "    y = np.asarray(y, dtype=np.float64).reshape(-1)\n",
    "    T = len(y)\n",
    "    d = np.diff(y)\n",
    "    abs_d = np.abs(d) if T > 1 else np.array([], dtype=np.float64)\n",
    "\n",
    "    p10 = float(np.quantile(y, 0.10))\n",
    "    p90 = float(np.quantile(y, 0.90))\n",
    "    q25 = float(np.quantile(y, 0.25))\n",
    "    q75 = float(np.quantile(y, 0.75))\n",
    "\n",
    "    counts = np.bincount(y.astype(int), minlength=5)\n",
    "    probs = counts / counts.sum()\n",
    "    entropy = float(-np.sum(probs[probs > 0] * np.log2(probs[probs > 0])))\n",
    "\n",
    "    if T > 1:\n",
    "        up_rate = float(np.mean(d > 0))\n",
    "        down_rate = float(np.mean(d < 0))\n",
    "        flat_rate = float(np.mean(d == 0))\n",
    "        jump_ge_2_rate = float(np.mean(abs_d >= 2))\n",
    "        lag1_autocorr = safe_corr(y[:-1], y[1:])\n",
    "\n",
    "        nz = d != 0\n",
    "        if np.sum(nz) >= 2:\n",
    "            signs = np.sign(d[nz])\n",
    "            sign_change_rate = float(np.mean(signs[1:] != signs[:-1]))\n",
    "        else:\n",
    "            sign_change_rate = 0.0\n",
    "    else:\n",
    "        up_rate = down_rate = flat_rate = 0.0\n",
    "        jump_ge_2_rate = 0.0\n",
    "        lag1_autocorr = 0.0\n",
    "        sign_change_rate = 0.0\n",
    "\n",
    "    t_norm = np.linspace(0.0, 1.0, T) if T > 1 else np.zeros(T, dtype=np.float64)\n",
    "    corr_with_position = safe_corr(y, t_norm)\n",
    "    if T > 1:\n",
    "        slope_position = float(np.polyfit(t_norm, y, deg=1)[0])\n",
    "    else:\n",
    "        slope_position = 0.0\n",
    "\n",
    "    b0, b1, b2, b3 = np.linspace(0, T, 4, dtype=int)\n",
    "    seg_early = y[b0:b1] if b1 > b0 else y\n",
    "    seg_mid = y[b1:b2] if b2 > b1 else y\n",
    "    seg_late = y[b2:b3] if b3 > b2 else y\n",
    "\n",
    "    y_ma = moving_average_1d(y, ma_window)\n",
    "\n",
    "    out = {\n",
    "        'T': int(T),\n",
    "        'mean_y': float(np.mean(y)),\n",
    "        'std_y': float(np.std(y)),\n",
    "        'median_y': float(np.median(y)),\n",
    "        'iqr_y': float(q75 - q25),\n",
    "        'min_y': float(np.min(y)),\n",
    "        'max_y': float(np.max(y)),\n",
    "        'p10_y': p10,\n",
    "        'p90_y': p90,\n",
    "        'range_y': float(np.max(y) - np.min(y)),\n",
    "        'prop_label_0': float(probs[0]),\n",
    "        'prop_label_1': float(probs[1]),\n",
    "        'prop_label_2': float(probs[2]),\n",
    "        'prop_label_3': float(probs[3]),\n",
    "        'prop_label_4': float(probs[4]),\n",
    "        'entropy_labels': entropy,\n",
    "        'mean_abs_diff': float(np.mean(abs_d)) if len(abs_d) else 0.0,\n",
    "        'std_diff': float(np.std(d)) if len(d) else 0.0,\n",
    "        'p95_abs_diff': float(np.quantile(abs_d, 0.95)) if len(abs_d) else 0.0,\n",
    "        'jump_ge_2_rate': jump_ge_2_rate,\n",
    "        'up_rate': up_rate,\n",
    "        'down_rate': down_rate,\n",
    "        'flat_rate': flat_rate,\n",
    "        'lag1_autocorr': lag1_autocorr,\n",
    "        'sign_change_rate': sign_change_rate,\n",
    "        'corr_with_position': corr_with_position,\n",
    "        'slope_position': slope_position,\n",
    "        'mean_early': float(np.mean(seg_early)),\n",
    "        'mean_mid': float(np.mean(seg_mid)),\n",
    "        'mean_late': float(np.mean(seg_late)),\n",
    "        'mean_ma5': float(np.mean(y_ma)),\n",
    "        'std_ma5': float(np.std(y_ma)),\n",
    "        'p95_ma5': float(np.quantile(y_ma, 0.95)),\n",
    "    }\n",
    "    return out\n",
    "\n",
    "\n",
    "def expected_outputs() -> list[Path]:\n",
    "    files = [\n",
    "        TABLE_DIR / 'indep_book_features.csv',\n",
    "        TABLE_DIR / 'indep_book_features_zscore.csv',\n",
    "        TABLE_DIR / 'cluster_quality_by_method.csv',\n",
    "        TABLE_DIR / 'cluster_assignments_feature.csv',\n",
    "        TABLE_DIR / 'cluster_assignments_dtw.csv',\n",
    "        TABLE_DIR / 'cluster_profile_summary.csv',\n",
    "        TABLE_DIR / 'cluster_representatives.csv',\n",
    "        TABLE_DIR / 'cluster_method_agreement.csv',\n",
    "        TABLE_DIR / 'kmeans_elbow_curve.csv',\n",
    "        TABLE_DIR / 'genre_by_feature_cluster_counts.csv',\n",
    "        TABLE_DIR / 'genre_by_feature_cluster_proportions.csv',\n",
    "        TABLE_DIR / 'feature_cluster_signature_top_features.csv',\n",
    "        TABLE_DIR / 'figure_legend_checks.csv',\n",
    "        TABLE_DIR / 'integrity_checks.csv',\n",
    "        FIG_DIR / 'feature_correlation_heatmap.png',\n",
    "        FIG_DIR / 'feature_pca_scatter_feature_clusters.png',\n",
    "        FIG_DIR / 'feature_elbow_kmeans_inertia.png',\n",
    "        FIG_DIR / 'feature_k_sweep_quality_metrics.png',\n",
    "        FIG_DIR / 'dtw_k_sweep_silhouette.png',\n",
    "        FIG_DIR / 'genre_by_feature_cluster_counts.png',\n",
    "        FIG_DIR / 'genre_by_feature_cluster_proportions.png',\n",
    "        FIG_DIR / 'cluster_feature_signature_heatmap_top12.png',\n",
    "        FIG_DIR / 'cluster_method_contingency_heatmap.png',\n",
    "        FIG_DIR / 'feature_cluster_member_trajectories_ma5.png',\n",
    "        FIG_DIR / 'cluster_centroid_trajectories_raw.png',\n",
    "        FIG_DIR / 'cluster_centroid_trajectories_ma5.png',\n",
    "        FIG_DIR / 'dtw_distance_heatmap.png',\n",
    "        FIG_DIR / 'dtw_dendrogram.png',\n",
    "        FIG_DIR / 'cluster_size_comparison.png',\n",
    "        OUT_ROOT / 'insights.md',\n",
    "        OUT_ROOT / 'cluster_report.md',\n",
    "    ]\n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load indep labels and validate integrity\nmeta = pd.read_csv(METADATA_PATH)\nrequired_cols = {'id', 'title', 'processed_dir'}\nif not required_cols.issubset(meta.columns):\n    raise ValueError(f'metadata missing required columns: {required_cols - set(meta.columns)}')\n\nmeta = meta.sort_values('id').reset_index(drop=True)\n\nintegrity_rows = []\npayloads = []\n\nfor r in meta.itertuples(index=False):\n    book_id = int(r.id)\n    title = str(r.title)\n    processed_dir = str(r.processed_dir)\n    base = PROCESSED_DIR / processed_dir\n\n    label_path = base / LABEL_FILE\n    emb_path = base / EMB_FILE\n\n    if not label_path.exists():\n        raise FileNotFoundError(f'Missing indep label file: {label_path}')\n    if not emb_path.exists():\n        raise FileNotFoundError(f'Missing embeddings file: {emb_path}')\n\n    y_raw = np.load(label_path)\n    y = normalize_label_shape(y_raw, label_path).astype(np.float64)\n\n    emb_shape = np.load(emb_path, mmap_mode='r').shape\n    if len(emb_shape) != 2:\n        raise ValueError(f'Expected 2D embeddings at {emb_path}, got {emb_shape}')\n    T_emb = int(emb_shape[0])\n    T = int(len(y))\n\n    int_like = bool(np.allclose(y, np.round(y)))\n    in_range = bool(np.all((y >= 0) & (y <= 4)))\n    aligned = bool(T == T_emb)\n\n    integrity_rows.append({\n        'check': f'book_{book_id}_indep_alignment',\n        'expected': 'len==emb_T, integer-like labels in [0,4]',\n        'actual': f'len={T}, emb_T={T_emb}, min={float(np.min(y)):.3f}, max={float(np.max(y)):.3f}, int_like={int_like}',\n        'pass': bool(aligned and in_range and int_like),\n    })\n\n    if not aligned:\n        raise ValueError(f'Length mismatch for {label_path}: label={T}, emb_T={T_emb}')\n    if not in_range:\n        raise ValueError(f'Out-of-range labels for {label_path}')\n    if not int_like:\n        raise ValueError(f'Non-integer-like labels for {label_path}')\n\n    payloads.append({\n        'book_id': book_id,\n        'title': title,\n        'processed_dir': processed_dir,\n        'T': T,\n        'y': y,\n        'y_ma5': moving_average_1d(y, MA_WINDOW),\n        'y_resampled': resample_to_len(y, L_DTW),\n        'y_ma5_resampled': resample_to_len(moving_average_1d(y, MA_WINDOW), L_DTW),\n    })\n\nif len(payloads) != 20:\n    raise RuntimeError(f'Expected 20 books, found {len(payloads)}')\n\nprint('Loaded books:', len(payloads))\nprint('Sample:', [(p['book_id'], p['processed_dir'], p['T']) for p in payloads[:5]])\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Feature extraction + zscore + correlation figure\nfeature_rows = []\nfor p in payloads:\n    feats = compute_features(p['y'], ma_window=MA_WINDOW)\n    feature_rows.append({\n        'book_id': int(p['book_id']),\n        'title': p['title'],\n        'processed_dir': p['processed_dir'],\n        **feats,\n    })\n\nfeatures_df = pd.DataFrame(feature_rows).sort_values('book_id').reset_index(drop=True)\nid_cols = ['book_id', 'title', 'processed_dir']\nnumeric_cols = [c for c in features_df.columns if c not in id_cols]\n\nmeans = features_df[numeric_cols].mean(axis=0)\nstds = features_df[numeric_cols].std(axis=0, ddof=0).replace(0.0, 1.0)\nz_df = features_df.copy()\nz_df[numeric_cols] = (features_df[numeric_cols] - means) / stds\n\nfeatures_df.to_csv(TABLE_DIR / 'indep_book_features.csv', index=False)\nz_df.to_csv(TABLE_DIR / 'indep_book_features_zscore.csv', index=False)\n\n# Integrity checks for feature matrix\nintegrity_rows.extend([\n    {'check': 'feature_row_count', 'expected': 20, 'actual': len(features_df), 'pass': len(features_df) == 20},\n    {'check': 'feature_no_nan', 'expected': True, 'actual': bool(np.isfinite(features_df[numeric_cols].to_numpy()).all()), 'pass': bool(np.isfinite(features_df[numeric_cols].to_numpy()).all())},\n    {'check': 'feature_no_inf_zscore', 'expected': True, 'actual': bool(np.isfinite(z_df[numeric_cols].to_numpy()).all()), 'pass': bool(np.isfinite(z_df[numeric_cols].to_numpy()).all())},\n])\n\n# Feature correlation heatmap\ncorr = features_df[numeric_cols].corr(numeric_only=True).to_numpy()\nfig, ax = plt.subplots(figsize=(16, 13))\nim = ax.imshow(corr, cmap='coolwarm', vmin=-1, vmax=1, aspect='auto')\nax.set_xticks(np.arange(len(numeric_cols)))\nax.set_xticklabels(numeric_cols, rotation=90, fontsize=8)\nax.set_yticks(np.arange(len(numeric_cols)))\nax.set_yticklabels(numeric_cols, fontsize=8)\nax.set_title('Feature correlation heatmap (indep_winsize_5)')\nfig.colorbar(im, ax=ax, fraction=0.025, pad=0.01)\nfig.tight_layout()\nfig.savefig(FIG_DIR / 'feature_correlation_heatmap.png', dpi=180, bbox_inches='tight')\nplt.close(fig)\n\nprint('Saved feature tables and correlation figure.')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-branch clustering sweep (kmeans + ward) and selection\n",
    "Xz = z_df[numeric_cols].to_numpy(dtype=np.float64)\n",
    "book_ids = z_df['book_id'].to_numpy(dtype=int)\n",
    "\n",
    "quality_rows = []\n",
    "feature_assign_cache = {}\n",
    "\n",
    "# KMeans elbow diagnostics (k=1..10)\n",
    "elbow_rows = []\n",
    "prev_inertia = None\n",
    "for k in range(1, 11):\n",
    "    km = KMeans(n_clusters=k, random_state=SEED, n_init=50)\n",
    "    km.fit(Xz)\n",
    "    inertia = float(km.inertia_)\n",
    "    if prev_inertia is None:\n",
    "        delta = np.nan\n",
    "        pct_drop = np.nan\n",
    "    else:\n",
    "        delta = prev_inertia - inertia\n",
    "        pct_drop = (delta / prev_inertia * 100.0) if prev_inertia > EPS else np.nan\n",
    "    elbow_rows.append({\n",
    "        'k': int(k),\n",
    "        'inertia': inertia,\n",
    "        'delta_inertia': delta,\n",
    "        'pct_drop_from_prev': pct_drop,\n",
    "    })\n",
    "    prev_inertia = inertia\n",
    "\n",
    "elbow_df = pd.DataFrame(elbow_rows)\n",
    "elbow_df.to_csv(TABLE_DIR / 'kmeans_elbow_curve.csv', index=False)\n",
    "\n",
    "# Feature clustering sweep (k=2..6)\n",
    "for method in ['kmeans', 'ward']:\n",
    "    for k in K_RANGE:\n",
    "        if method == 'kmeans':\n",
    "            model = KMeans(n_clusters=k, random_state=SEED, n_init=50)\n",
    "        else:\n",
    "            model = AgglomerativeClustering(n_clusters=k, linkage='ward')\n",
    "\n",
    "        labels0 = model.fit_predict(Xz)\n",
    "        feature_assign_cache[(method, k)] = labels0\n",
    "\n",
    "        sil = float(silhouette_score(Xz, labels0, metric='euclidean'))\n",
    "        db = float(davies_bouldin_score(Xz, labels0))\n",
    "        ch = float(calinski_harabasz_score(Xz, labels0))\n",
    "\n",
    "        stability = np.nan\n",
    "        if method == 'kmeans':\n",
    "            run_labels = []\n",
    "            for s in range(N_STABILITY_SEEDS):\n",
    "                km = KMeans(n_clusters=k, random_state=s, n_init=20)\n",
    "                run_labels.append(km.fit_predict(Xz))\n",
    "            aris = []\n",
    "            for a, b in combinations(range(N_STABILITY_SEEDS), 2):\n",
    "                aris.append(adjusted_rand_score(run_labels[a], run_labels[b]))\n",
    "            stability = float(np.mean(aris)) if len(aris) else np.nan\n",
    "\n",
    "        quality_rows.append({\n",
    "            'branch': 'feature',\n",
    "            'method': method,\n",
    "            'k': int(k),\n",
    "            'silhouette': sil,\n",
    "            'davies_bouldin': db,\n",
    "            'calinski_harabasz': ch,\n",
    "            'kmeans_stability_ari': stability,\n",
    "        })\n",
    "\n",
    "feat_quality = pd.DataFrame([r for r in quality_rows if r['branch'] == 'feature']).copy()\n",
    "feat_quality['stability_rank'] = feat_quality['kmeans_stability_ari'].fillna(-1e9)\n",
    "feat_quality = feat_quality.sort_values(['silhouette', 'stability_rank', 'davies_bouldin'], ascending=[False, False, True]).reset_index(drop=True)\n",
    "\n",
    "best_feat = feat_quality.iloc[0]\n",
    "selected_feature_method = str(best_feat['method'])\n",
    "selected_feature_k = int(best_feat['k'])\n",
    "feature_labels0 = feature_assign_cache[(selected_feature_method, selected_feature_k)]\n",
    "feature_labels = feature_labels0.astype(int) + 1\n",
    "\n",
    "feature_cluster_ids = sorted(np.unique(feature_labels))\n",
    "feature_cluster_sizes = {int(c): int(np.sum(feature_labels == c)) for c in feature_cluster_ids}\n",
    "feature_palette = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "feature_color_map = {int(c): feature_palette[i % len(feature_palette)] for i, c in enumerate(feature_cluster_ids)}\n",
    "\n",
    "integrity_rows.append({\n",
    "    'check': 'selected_feature_k_in_range',\n",
    "    'expected': '[2,6]',\n",
    "    'actual': selected_feature_k,\n",
    "    'pass': 2 <= selected_feature_k <= 6,\n",
    "})\n",
    "\n",
    "# Elbow integrity + figure\n",
    "elbow_monotonic = bool(np.all(np.diff(elbow_df['inertia'].to_numpy(dtype=np.float64)) <= 1e-9))\n",
    "inertia_selected = float(elbow_df.loc[elbow_df['k'] == selected_feature_k, 'inertia'].iloc[0])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8.5, 5.5))\n",
    "ax.plot(elbow_df['k'], elbow_df['inertia'], marker='o', linewidth=2.0, color='#1f77b4')\n",
    "ax.scatter([selected_feature_k], [inertia_selected], s=70, color='#d62728', zorder=5, label=f'selected k={selected_feature_k}')\n",
    "ax.axvline(selected_feature_k, linestyle='--', color='#d62728', alpha=0.7)\n",
    "ax.set_title('KMeans elbow curve (diagnostic only)')\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('Inertia')\n",
    "ax.grid(alpha=0.25)\n",
    "ax.legend(loc='upper right')\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'feature_elbow_kmeans_inertia.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Feature PCA scatter with robust legend handling\n",
    "pca = PCA(n_components=2, random_state=SEED)\n",
    "X2 = pca.fit_transform(Xz)\n",
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "legend_handles = []\n",
    "legend_labels = []\n",
    "for c in feature_cluster_ids:\n",
    "    idx = np.where(feature_labels == c)[0]\n",
    "    color = feature_color_map[int(c)]\n",
    "    sc = ax.scatter(\n",
    "        X2[idx, 0],\n",
    "        X2[idx, 1],\n",
    "        s=85,\n",
    "        alpha=0.9,\n",
    "        color=color,\n",
    "        edgecolor='black',\n",
    "        linewidth=0.35,\n",
    "    )\n",
    "    legend_handles.append(sc)\n",
    "    legend_labels.append(f'Cluster {int(c)} (n={feature_cluster_sizes[int(c)]})')\n",
    "    for j in idx:\n",
    "        ax.text(X2[j, 0], X2[j, 1], str(int(book_ids[j])), fontsize=8, alpha=0.82)\n",
    "\n",
    "ax.set_title(f'Feature PCA scatter (method={selected_feature_method}, k={selected_feature_k})')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.grid(alpha=0.2)\n",
    "legend = ax.legend(\n",
    "    handles=legend_handles,\n",
    "    labels=legend_labels,\n",
    "    title='Feature clusters',\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(1.02, 1.0),\n",
    "    borderaxespad=0.0,\n",
    "    frameon=True,\n",
    ")\n",
    "fig.tight_layout(rect=[0, 0, 0.80, 1])\n",
    "fig.savefig(FIG_DIR / 'feature_pca_scatter_feature_clusters.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "legend_actual = len(legend.texts) if legend is not None else 0\n",
    "legend_df = pd.DataFrame([\n",
    "    {\n",
    "        'figure': 'feature_pca_scatter_feature_clusters.png',\n",
    "        'expected_entries': int(len(feature_cluster_ids)),\n",
    "        'actual_entries': int(legend_actual),\n",
    "        'pass': bool(legend_actual == len(feature_cluster_ids)),\n",
    "        'labels': ' | '.join(legend_labels),\n",
    "    }\n",
    "])\n",
    "legend_df.to_csv(TABLE_DIR / 'figure_legend_checks.csv', index=False)\n",
    "\n",
    "integrity_rows.extend([\n",
    "    {'check': 'elbow_k_coverage', 'expected': '1..10', 'actual': f\"{int(elbow_df['k'].min())}..{int(elbow_df['k'].max())}\", 'pass': int(elbow_df['k'].min()) == 1 and int(elbow_df['k'].max()) == 10},\n",
    "    {'check': 'elbow_inertia_nonincreasing', 'expected': True, 'actual': elbow_monotonic, 'pass': elbow_monotonic},\n",
    "    {'check': 'feature_pca_legend_entries_match_clusters', 'expected': int(len(feature_cluster_ids)), 'actual': int(legend_actual), 'pass': bool(legend_actual == len(feature_cluster_ids))},\n",
    "])\n",
    "\n",
    "print('Selected feature clustering:', selected_feature_method, selected_feature_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DTW branch + method agreement + cluster tables + representative/profile tables\n",
    "\n",
    "# Build DTW distance matrix from resampled trajectories\n",
    "Y_res = np.vstack([p['y_resampled'] for p in payloads])\n",
    "N = Y_res.shape[0]\n",
    "D = np.zeros((N, N), dtype=np.float64)\n",
    "for i in range(N):\n",
    "    for j in range(i + 1, N):\n",
    "        d = dtw_distance(Y_res[i], Y_res[j])\n",
    "        D[i, j] = d\n",
    "        D[j, i] = d\n",
    "\n",
    "# DTW clustering sweep\n",
    "DTW_METHOD = 'average'\n",
    "dtw_assign_cache = {}\n",
    "for k in K_RANGE:\n",
    "    model = AgglomerativeClustering(n_clusters=k, metric='precomputed', linkage=DTW_METHOD)\n",
    "    labels0 = model.fit_predict(D)\n",
    "    dtw_assign_cache[k] = labels0\n",
    "    sil = float(silhouette_score(D, labels0, metric='precomputed'))\n",
    "    quality_rows.append({\n",
    "        'branch': 'dtw',\n",
    "        'method': DTW_METHOD,\n",
    "        'k': int(k),\n",
    "        'silhouette': sil,\n",
    "        'davies_bouldin': np.nan,\n",
    "        'calinski_harabasz': np.nan,\n",
    "        'kmeans_stability_ari': np.nan,\n",
    "    })\n",
    "\n",
    "quality_df = pd.DataFrame(quality_rows).sort_values(['branch', 'method', 'k']).reset_index(drop=True)\n",
    "dtw_quality = quality_df[quality_df['branch'] == 'dtw'].sort_values('silhouette', ascending=False).reset_index(drop=True)\n",
    "best_dtw = dtw_quality.iloc[0]\n",
    "selected_dtw_k = int(best_dtw['k'])\n",
    "dtw_labels0 = dtw_assign_cache[selected_dtw_k]\n",
    "dtw_labels = dtw_labels0.astype(int) + 1\n",
    "\n",
    "dtw_cluster_ids = sorted(np.unique(dtw_labels))\n",
    "dtw_cluster_sizes = {int(c): int(np.sum(dtw_labels == c)) for c in dtw_cluster_ids}\n",
    "dtw_palette = plt.cm.Set2(np.linspace(0, 1, max(3, len(dtw_cluster_ids))))\n",
    "dtw_color_map = {int(c): dtw_palette[i % len(dtw_palette)] for i, c in enumerate(dtw_cluster_ids)}\n",
    "\n",
    "integrity_rows.append({\n",
    "    'check': 'selected_dtw_k_in_range',\n",
    "    'expected': '[2,6]',\n",
    "    'actual': selected_dtw_k,\n",
    "    'pass': 2 <= selected_dtw_k <= 6,\n",
    "})\n",
    "\n",
    "# Assignments tables\n",
    "feature_assign_df = z_df[id_cols].copy()\n",
    "feature_assign_df['method'] = selected_feature_method\n",
    "feature_assign_df['k'] = selected_feature_k\n",
    "feature_assign_df['cluster'] = feature_labels\n",
    "feature_assign_df = feature_assign_df.sort_values('book_id').reset_index(drop=True)\n",
    "\n",
    "dtw_assign_df = z_df[id_cols].copy()\n",
    "dtw_assign_df['method'] = DTW_METHOD\n",
    "dtw_assign_df['k'] = selected_dtw_k\n",
    "dtw_assign_df['cluster'] = dtw_labels\n",
    "dtw_assign_df = dtw_assign_df.sort_values('book_id').reset_index(drop=True)\n",
    "\n",
    "# Agreement table with metrics + contingency\n",
    "ari = float(adjusted_rand_score(feature_labels, dtw_labels))\n",
    "nmi = float(normalized_mutual_info_score(feature_labels, dtw_labels))\n",
    "ctab = pd.crosstab(feature_labels, dtw_labels)\n",
    "\n",
    "agree_rows = [\n",
    "    {'row_type': 'metric', 'metric': 'ari', 'value': ari, 'feature_cluster': np.nan, 'dtw_cluster': np.nan, 'count': np.nan},\n",
    "    {'row_type': 'metric', 'metric': 'nmi', 'value': nmi, 'feature_cluster': np.nan, 'dtw_cluster': np.nan, 'count': np.nan},\n",
    "]\n",
    "for fc in ctab.index:\n",
    "    for dc in ctab.columns:\n",
    "        agree_rows.append({\n",
    "            'row_type': 'contingency',\n",
    "            'metric': 'count',\n",
    "            'value': np.nan,\n",
    "            'feature_cluster': int(fc),\n",
    "            'dtw_cluster': int(dc),\n",
    "            'count': int(ctab.loc[fc, dc]),\n",
    "        })\n",
    "agreement_df = pd.DataFrame(agree_rows)\n",
    "\n",
    "# Quality metric visualizations\n",
    "feature_quality = quality_df[quality_df['branch'] == 'feature'].copy()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "for method in ['kmeans', 'ward']:\n",
    "    s = feature_quality[feature_quality['method'] == method].sort_values('k')\n",
    "    axes[0, 0].plot(s['k'], s['silhouette'], marker='o', linewidth=2, label=method)\n",
    "axes[0, 0].set_title('Silhouette vs k (feature branch)')\n",
    "axes[0, 0].set_xlabel('k')\n",
    "axes[0, 0].set_ylabel('Silhouette')\n",
    "axes[0, 0].grid(alpha=0.25)\n",
    "axes[0, 0].legend()\n",
    "\n",
    "for method in ['kmeans', 'ward']:\n",
    "    s = feature_quality[feature_quality['method'] == method].sort_values('k')\n",
    "    axes[0, 1].plot(s['k'], s['davies_bouldin'], marker='o', linewidth=2, label=method)\n",
    "axes[0, 1].set_title('Davies-Bouldin vs k (lower is better)')\n",
    "axes[0, 1].set_xlabel('k')\n",
    "axes[0, 1].set_ylabel('Davies-Bouldin')\n",
    "axes[0, 1].grid(alpha=0.25)\n",
    "\n",
    "for method in ['kmeans', 'ward']:\n",
    "    s = feature_quality[feature_quality['method'] == method].sort_values('k')\n",
    "    axes[1, 0].plot(s['k'], s['calinski_harabasz'], marker='o', linewidth=2, label=method)\n",
    "axes[1, 0].set_title('Calinski-Harabasz vs k (higher is better)')\n",
    "axes[1, 0].set_xlabel('k')\n",
    "axes[1, 0].set_ylabel('Calinski-Harabasz')\n",
    "axes[1, 0].grid(alpha=0.25)\n",
    "\n",
    "km_stability = feature_quality[feature_quality['method'] == 'kmeans'].sort_values('k')\n",
    "axes[1, 1].plot(km_stability['k'], km_stability['kmeans_stability_ari'], marker='o', linewidth=2, color='#9467bd')\n",
    "axes[1, 1].set_title('KMeans stability ARI vs k')\n",
    "axes[1, 1].set_xlabel('k')\n",
    "axes[1, 1].set_ylabel('Mean pairwise ARI')\n",
    "axes[1, 1].grid(alpha=0.25)\n",
    "\n",
    "fig.suptitle('Feature branch quality diagnostics')\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'feature_k_sweep_quality_metrics.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8.5, 5.5))\n",
    "s = dtw_quality.sort_values('k')\n",
    "ax.plot(s['k'], s['silhouette'], marker='o', linewidth=2, color='#2ca02c')\n",
    "ax.scatter([selected_dtw_k], [float(s[s['k'] == selected_dtw_k]['silhouette'].iloc[0])], color='#d62728', s=70, zorder=5, label=f'selected k={selected_dtw_k}')\n",
    "ax.axvline(selected_dtw_k, linestyle='--', color='#d62728', alpha=0.7)\n",
    "ax.set_title('DTW branch silhouette vs k')\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('Silhouette (precomputed DTW distance)')\n",
    "ax.grid(alpha=0.25)\n",
    "ax.legend(loc='upper right')\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'dtw_k_sweep_silhouette.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Genre by feature cluster analysis\n",
    "genre_map = (\n",
    "    meta[['id', 'genre_primary']]\n",
    "    .drop_duplicates(subset=['id'])\n",
    "    .rename(columns={'id': 'book_id'})\n",
    ")\n",
    "feature_genre_df = feature_assign_df.merge(genre_map, on='book_id', how='left')\n",
    "missing_genre = int(feature_genre_df['genre_primary'].isna().sum())\n",
    "feature_genre_df['genre_primary'] = feature_genre_df['genre_primary'].fillna('Unknown')\n",
    "\n",
    "genre_count_df = pd.crosstab(feature_genre_df['cluster'], feature_genre_df['genre_primary']).sort_index()\n",
    "genre_prop_df = genre_count_df.div(genre_count_df.sum(axis=1), axis=0).fillna(0.0)\n",
    "\n",
    "genre_count_df.to_csv(TABLE_DIR / 'genre_by_feature_cluster_counts.csv', index_label='cluster')\n",
    "genre_prop_df.to_csv(TABLE_DIR / 'genre_by_feature_cluster_proportions.csv', index_label='cluster')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "genre_count_df.plot(kind='bar', stacked=True, ax=ax, colormap='tab20')\n",
    "ax.set_title('Genre composition by feature cluster (counts)')\n",
    "ax.set_xlabel('Feature cluster')\n",
    "ax.set_ylabel('Books')\n",
    "ax.grid(axis='y', alpha=0.25)\n",
    "ax.legend(title='genre_primary', bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "fig.tight_layout(rect=[0, 0, 0.80, 1])\n",
    "fig.savefig(FIG_DIR / 'genre_by_feature_cluster_counts.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 5.5))\n",
    "mat = genre_prop_df.to_numpy(dtype=np.float64)\n",
    "im = ax.imshow(mat, cmap='YlGnBu', aspect='auto', vmin=0.0, vmax=max(0.25, float(np.max(mat))))\n",
    "ax.set_xticks(np.arange(genre_prop_df.shape[1]))\n",
    "ax.set_xticklabels(genre_prop_df.columns, rotation=45, ha='right')\n",
    "ax.set_yticks(np.arange(genre_prop_df.shape[0]))\n",
    "ax.set_yticklabels([str(int(i)) for i in genre_prop_df.index])\n",
    "ax.set_xlabel('genre_primary')\n",
    "ax.set_ylabel('Feature cluster')\n",
    "ax.set_title('Genre composition by feature cluster (row-normalized)')\n",
    "for r in range(mat.shape[0]):\n",
    "    for c in range(mat.shape[1]):\n",
    "        if mat[r, c] > 0:\n",
    "            ax.text(c, r, f'{mat[r, c]:.2f}', ha='center', va='center', fontsize=8, color='black')\n",
    "fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'genre_by_feature_cluster_proportions.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Profile summary table\n",
    "profile_rows = []\n",
    "rep_rows = []\n",
    "\n",
    "Xz_df = z_df.copy()\n",
    "Xz_df['cluster_feature'] = feature_labels\n",
    "Xz_df['cluster_dtw'] = dtw_labels\n",
    "\n",
    "# Feature-branch representatives (centroid nearest + high/low volatility)\n",
    "for c in feature_cluster_ids:\n",
    "    idx = np.where(feature_labels == c)[0]\n",
    "    Xc = Xz[idx]\n",
    "    centroid = Xc.mean(axis=0)\n",
    "    dists = np.linalg.norm(Xc - centroid, axis=1)\n",
    "    medoid_local = idx[int(np.argmin(dists))]\n",
    "\n",
    "    raw_sub = features_df.iloc[idx].copy()\n",
    "    high_idx = raw_sub['std_diff'].idxmax()\n",
    "    low_idx = raw_sub['std_diff'].idxmin()\n",
    "\n",
    "    for role, ridx, sval, sname in [\n",
    "        ('centroid_medoid', medoid_local, float(np.min(dists)), 'euclidean_to_centroid'),\n",
    "        ('high_volatility', int(high_idx), float(features_df.loc[high_idx, 'std_diff']), 'std_diff'),\n",
    "        ('low_volatility', int(low_idx), float(features_df.loc[low_idx, 'std_diff']), 'std_diff'),\n",
    "    ]:\n",
    "        rep_rows.append({\n",
    "            'branch': 'feature',\n",
    "            'cluster': int(c),\n",
    "            'role': role,\n",
    "            'book_id': int(features_df.loc[ridx, 'book_id']),\n",
    "            'title': str(features_df.loc[ridx, 'title']),\n",
    "            'processed_dir': str(features_df.loc[ridx, 'processed_dir']),\n",
    "            'score_name': sname,\n",
    "            'score_value': float(sval),\n",
    "        })\n",
    "\n",
    "# DTW-branch representatives (medoid via mean intra-cluster DTW + high/low volatility)\n",
    "for c in dtw_cluster_ids:\n",
    "    idx = np.where(dtw_labels == c)[0]\n",
    "    Dc = D[np.ix_(idx, idx)]\n",
    "    mean_d = Dc.mean(axis=1)\n",
    "    medoid_local = idx[int(np.argmin(mean_d))]\n",
    "\n",
    "    raw_sub = features_df.iloc[idx].copy()\n",
    "    high_idx = raw_sub['std_diff'].idxmax()\n",
    "    low_idx = raw_sub['std_diff'].idxmin()\n",
    "\n",
    "    for role, ridx, sval, sname in [\n",
    "        ('cluster_medoid', medoid_local, float(np.min(mean_d)), 'mean_intra_cluster_dtw'),\n",
    "        ('high_volatility', int(high_idx), float(features_df.loc[high_idx, 'std_diff']), 'std_diff'),\n",
    "        ('low_volatility', int(low_idx), float(features_df.loc[low_idx, 'std_diff']), 'std_diff'),\n",
    "    ]:\n",
    "        rep_rows.append({\n",
    "            'branch': 'dtw',\n",
    "            'cluster': int(c),\n",
    "            'role': role,\n",
    "            'book_id': int(features_df.loc[ridx, 'book_id']),\n",
    "            'title': str(features_df.loc[ridx, 'title']),\n",
    "            'processed_dir': str(features_df.loc[ridx, 'processed_dir']),\n",
    "            'score_name': sname,\n",
    "            'score_value': float(sval),\n",
    "        })\n",
    "\n",
    "# Profile rows (feature + dtw branches)\n",
    "for branch_name, cluster_col in [('feature', 'cluster_feature'), ('dtw', 'cluster_dtw')]:\n",
    "    for c in sorted(Xz_df[cluster_col].unique()):\n",
    "        mask = Xz_df[cluster_col] == c\n",
    "        n_c = int(mask.sum())\n",
    "        for feat in numeric_cols:\n",
    "            mean_raw = float(features_df.loc[mask, feat].mean())\n",
    "            med_raw = float(features_df.loc[mask, feat].median())\n",
    "            mean_z = float(z_df.loc[mask, feat].mean())\n",
    "            profile_rows.append({\n",
    "                'branch': branch_name,\n",
    "                'cluster': int(c),\n",
    "                'cluster_size': n_c,\n",
    "                'feature': feat,\n",
    "                'mean_raw': mean_raw,\n",
    "                'median_raw': med_raw,\n",
    "                'mean_z': mean_z,\n",
    "                'delta_z_vs_corpus': mean_z,\n",
    "            })\n",
    "\n",
    "profile_df = pd.DataFrame(profile_rows)\n",
    "profile_df['abs_delta_z'] = profile_df['delta_z_vs_corpus'].abs()\n",
    "profile_df['rank_abs_delta'] = (\n",
    "    profile_df.sort_values(['branch', 'cluster', 'abs_delta_z'], ascending=[True, True, False])\n",
    "    .groupby(['branch', 'cluster'])\n",
    "    .cumcount() + 1\n",
    ")\n",
    "\n",
    "representatives_df = pd.DataFrame(rep_rows).sort_values(['branch', 'cluster', 'role']).reset_index(drop=True)\n",
    "\n",
    "# Feature signature heatmap/table (top 12 by max absolute delta)\n",
    "feature_profile = profile_df[profile_df['branch'] == 'feature'].copy()\n",
    "feature_max_abs = (\n",
    "    feature_profile.groupby('feature', as_index=False)['abs_delta_z']\n",
    "    .max()\n",
    "    .sort_values('abs_delta_z', ascending=False)\n",
    ")\n",
    "top_signature_features = feature_max_abs.head(12)['feature'].tolist()\n",
    "feature_rank_map = {f: i + 1 for i, f in enumerate(top_signature_features)}\n",
    "feature_signature_df = feature_profile[feature_profile['feature'].isin(top_signature_features)].copy()\n",
    "feature_signature_df['global_feature_rank'] = feature_signature_df['feature'].map(feature_rank_map)\n",
    "feature_signature_df = feature_signature_df.sort_values(['global_feature_rank', 'cluster']).reset_index(drop=True)\n",
    "feature_signature_df.to_csv(TABLE_DIR / 'feature_cluster_signature_top_features.csv', index=False)\n",
    "\n",
    "sig_pivot = (\n",
    "    feature_signature_df\n",
    "    .pivot_table(index='cluster', columns='feature', values='delta_z_vs_corpus', aggfunc='mean')\n",
    "    .reindex(sorted(feature_signature_df['cluster'].unique()), axis=0)\n",
    ")\n",
    "sig_pivot = sig_pivot[top_signature_features]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5.5))\n",
    "mat = sig_pivot.to_numpy(dtype=np.float64)\n",
    "vmax = float(np.max(np.abs(mat))) if np.isfinite(mat).all() else 1.0\n",
    "vmax = max(vmax, 0.1)\n",
    "im = ax.imshow(mat, cmap='coolwarm', aspect='auto', vmin=-vmax, vmax=vmax)\n",
    "ax.set_xticks(np.arange(sig_pivot.shape[1]))\n",
    "ax.set_xticklabels(sig_pivot.columns, rotation=45, ha='right')\n",
    "ax.set_yticks(np.arange(sig_pivot.shape[0]))\n",
    "ax.set_yticklabels([str(int(i)) for i in sig_pivot.index])\n",
    "ax.set_xlabel('Feature')\n",
    "ax.set_ylabel('Feature cluster')\n",
    "ax.set_title('Top-12 feature signatures by cluster (delta z vs corpus)')\n",
    "for r in range(mat.shape[0]):\n",
    "    for c in range(mat.shape[1]):\n",
    "        ax.text(c, r, f'{mat[r, c]:+.2f}', ha='center', va='center', fontsize=8)\n",
    "fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'cluster_feature_signature_heatmap_top12.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Contingency heatmap (feature vs DTW)\n",
    "fig, ax = plt.subplots(figsize=(7, 5.5))\n",
    "ctab_mat = ctab.to_numpy(dtype=np.float64)\n",
    "im = ax.imshow(ctab_mat, cmap='Blues', aspect='auto')\n",
    "ax.set_xticks(np.arange(len(ctab.columns)))\n",
    "ax.set_xticklabels([f'DTW {int(c)}' for c in ctab.columns])\n",
    "ax.set_yticks(np.arange(len(ctab.index)))\n",
    "ax.set_yticklabels([f'Feat {int(c)}' for c in ctab.index])\n",
    "ax.set_title('Feature vs DTW cluster contingency')\n",
    "ax.set_xlabel('DTW cluster')\n",
    "ax.set_ylabel('Feature cluster')\n",
    "for r in range(ctab_mat.shape[0]):\n",
    "    for c in range(ctab_mat.shape[1]):\n",
    "        ax.text(c, r, f'{int(ctab_mat[r, c])}', ha='center', va='center', fontsize=10)\n",
    "fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'cluster_method_contingency_heatmap.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Save tables\n",
    "quality_df.to_csv(TABLE_DIR / 'cluster_quality_by_method.csv', index=False)\n",
    "feature_assign_df.to_csv(TABLE_DIR / 'cluster_assignments_feature.csv', index=False)\n",
    "dtw_assign_df.to_csv(TABLE_DIR / 'cluster_assignments_dtw.csv', index=False)\n",
    "profile_df.to_csv(TABLE_DIR / 'cluster_profile_summary.csv', index=False)\n",
    "representatives_df.to_csv(TABLE_DIR / 'cluster_representatives.csv', index=False)\n",
    "agreement_df.to_csv(TABLE_DIR / 'cluster_method_agreement.csv', index=False)\n",
    "\n",
    "# Integrity checks for cluster assignments + quality table + genre checks\n",
    "row_sum_dev = float(np.max(np.abs(genre_prop_df.sum(axis=1).to_numpy(dtype=np.float64) - 1.0))) if len(genre_prop_df) else 0.0\n",
    "integrity_rows.extend([\n",
    "    {'check': 'cluster_assignments_feature_rows', 'expected': 20, 'actual': len(feature_assign_df), 'pass': len(feature_assign_df) == 20},\n",
    "    {'check': 'cluster_assignments_dtw_rows', 'expected': 20, 'actual': len(dtw_assign_df), 'pass': len(dtw_assign_df) == 20},\n",
    "    {'check': 'cluster_quality_rows', 'expected': len(K_RANGE) * 3, 'actual': len(quality_df), 'pass': len(quality_df) == len(K_RANGE) * 3},\n",
    "    {'check': 'agreement_ari_range', 'expected': '[0,1]', 'actual': ari, 'pass': 0.0 <= ari <= 1.0},\n",
    "    {'check': 'agreement_nmi_range', 'expected': '[0,1]', 'actual': nmi, 'pass': 0.0 <= nmi <= 1.0},\n",
    "    {'check': 'genre_join_missing_count', 'expected': 0, 'actual': missing_genre, 'pass': missing_genre == 0},\n",
    "    {'check': 'genre_cluster_count_sum', 'expected': len(feature_assign_df), 'actual': int(genre_count_df.to_numpy().sum()), 'pass': int(genre_count_df.to_numpy().sum()) == len(feature_assign_df)},\n",
    "    {'check': 'genre_proportion_rowsum_max_deviation', 'expected': '<=1e-6', 'actual': row_sum_dev, 'pass': row_sum_dev <= 1e-6},\n",
    "])\n",
    "\n",
    "print('Selected feature:', selected_feature_method, selected_feature_k)\n",
    "print('Selected DTW:', selected_dtw_k)\n",
    "print('ARI/NMI:', ari, nmi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figures: DTW + centroids + cluster sizes + member panels\n",
    "\n",
    "# DTW distance heatmap (ordered by dtw cluster)\n",
    "order = np.argsort(dtw_labels)\n",
    "D_ord = D[np.ix_(order, order)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 8))\n",
    "im = ax.imshow(D_ord, cmap='viridis', aspect='auto')\n",
    "ax.set_title('DTW distance heatmap (ordered by DTW clusters)')\n",
    "ax.set_xlabel('Books')\n",
    "ax.set_ylabel('Books')\n",
    "fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'dtw_distance_heatmap.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# DTW dendrogram\n",
    "condensed = squareform(D, checks=False)\n",
    "Z = linkage(condensed, method=DTW_METHOD)\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "dendrogram(Z, labels=[str(int(b)) for b in book_ids], leaf_rotation=90, ax=ax)\n",
    "ax.set_title('DTW hierarchical dendrogram (average linkage)')\n",
    "ax.set_xlabel('Book ID')\n",
    "ax.set_ylabel('Distance')\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'dtw_dendrogram.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Cluster centroid trajectories (raw and MA5), feature vs dtw\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 5.5), sharey=True)\n",
    "for ax, branch, labels, color_map in [\n",
    "    (axes[0], 'feature', feature_labels, feature_color_map),\n",
    "    (axes[1], 'dtw', dtw_labels, dtw_color_map),\n",
    "]:\n",
    "    for c in sorted(np.unique(labels)):\n",
    "        idx = np.where(labels == c)[0]\n",
    "        Ys = np.vstack([payloads[i]['y_resampled'] for i in idx])\n",
    "        centroid = Ys.mean(axis=0)\n",
    "        color = color_map[int(c)]\n",
    "        for row in Ys:\n",
    "            ax.plot(np.linspace(0, 1, L_DTW), row, color=color, alpha=0.10, linewidth=0.8)\n",
    "        ax.plot(np.linspace(0, 1, L_DTW), centroid, color=color, linewidth=2.2, label=f'Cluster {int(c)} (n={len(idx)})')\n",
    "    ax.set_title(f'{branch} clusters (raw)')\n",
    "    ax.set_xlabel('Normalized position')\n",
    "    ax.grid(alpha=0.2)\n",
    "axes[0].set_ylabel('Excitement')\n",
    "axes[0].legend(loc='upper right', fontsize=8)\n",
    "axes[1].legend(loc='upper right', fontsize=8)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'cluster_centroid_trajectories_raw.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 5.5), sharey=True)\n",
    "for ax, branch, labels, color_map in [\n",
    "    (axes[0], 'feature', feature_labels, feature_color_map),\n",
    "    (axes[1], 'dtw', dtw_labels, dtw_color_map),\n",
    "]:\n",
    "    for c in sorted(np.unique(labels)):\n",
    "        idx = np.where(labels == c)[0]\n",
    "        Ys = np.vstack([payloads[i]['y_ma5_resampled'] for i in idx])\n",
    "        centroid = Ys.mean(axis=0)\n",
    "        color = color_map[int(c)]\n",
    "        for row in Ys:\n",
    "            ax.plot(np.linspace(0, 1, L_DTW), row, color=color, alpha=0.10, linewidth=0.8)\n",
    "        ax.plot(np.linspace(0, 1, L_DTW), centroid, color=color, linewidth=2.2, label=f'Cluster {int(c)} (n={len(idx)})')\n",
    "    ax.set_title(f'{branch} clusters (MA5)')\n",
    "    ax.set_xlabel('Normalized position')\n",
    "    ax.grid(alpha=0.2)\n",
    "axes[0].set_ylabel('Excitement')\n",
    "axes[0].legend(loc='upper right', fontsize=8)\n",
    "axes[1].legend(loc='upper right', fontsize=8)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'cluster_centroid_trajectories_ma5.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Member trajectories by feature cluster (MA5)\n",
    "feature_clusters = sorted(np.unique(feature_labels))\n",
    "n_clusters = len(feature_clusters)\n",
    "ncols = 2\n",
    "nrows = int(np.ceil(n_clusters / ncols))\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, 3.8 * nrows), sharex=True, sharey=True)\n",
    "axes = np.atleast_1d(axes).reshape(nrows, ncols)\n",
    "for i, c in enumerate(feature_clusters):\n",
    "    ax = axes[i // ncols, i % ncols]\n",
    "    idx = np.where(feature_labels == c)[0]\n",
    "    color = feature_color_map[int(c)]\n",
    "    Ys = np.vstack([payloads[j]['y_ma5_resampled'] for j in idx])\n",
    "    centroid = Ys.mean(axis=0)\n",
    "    for row in Ys:\n",
    "        ax.plot(np.linspace(0, 1, L_DTW), row, color=color, alpha=0.18, linewidth=0.9)\n",
    "    ax.plot(np.linspace(0, 1, L_DTW), centroid, color=color, linewidth=2.4)\n",
    "    ax.set_title(f'Feature Cluster {int(c)} (n={len(idx)})')\n",
    "    ax.grid(alpha=0.2)\n",
    "for j in range(n_clusters, nrows * ncols):\n",
    "    axes[j // ncols, j % ncols].axis('off')\n",
    "for ax in axes[-1, :]:\n",
    "    ax.set_xlabel('Normalized position')\n",
    "for r in range(nrows):\n",
    "    axes[r, 0].set_ylabel('MA5 excitement')\n",
    "fig.suptitle('Feature cluster member trajectories (MA5)')\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'feature_cluster_member_trajectories_ma5.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Cluster size comparison\n",
    "feat_counts = pd.Series(feature_labels).value_counts().sort_index()\n",
    "dtw_counts = pd.Series(dtw_labels).value_counts().sort_index()\n",
    "all_clusters = sorted(set(feat_counts.index.tolist()) | set(dtw_counts.index.tolist()))\n",
    "feat_vals = np.array([int(feat_counts.get(c, 0)) for c in all_clusters])\n",
    "dtw_vals = np.array([int(dtw_counts.get(c, 0)) for c in all_clusters])\n",
    "\n",
    "x = np.arange(len(all_clusters))\n",
    "bw = 0.35\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(x - bw/2, feat_vals, width=bw, label=f'feature ({selected_feature_method}, k={selected_feature_k})')\n",
    "ax.bar(x + bw/2, dtw_vals, width=bw, label=f'dtw (k={selected_dtw_k})')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([str(int(c)) for c in all_clusters])\n",
    "ax.set_xlabel('Cluster id')\n",
    "ax.set_ylabel('Books')\n",
    "ax.set_title('Cluster size comparison: feature vs DTW')\n",
    "ax.grid(axis='y', alpha=0.25)\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'cluster_size_comparison.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "print('Saved DTW/centroid/size/member figures.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insights markdown + cluster report + finalize integrity checks\n",
    "\n",
    "# Build top distinguishing feature helper\n",
    "feature_top = (\n",
    "    profile_df[profile_df['branch'] == 'feature']\n",
    "    .sort_values(['cluster', 'rank_abs_delta'])\n",
    ")\n",
    "dtw_top = (\n",
    "    profile_df[profile_df['branch'] == 'dtw']\n",
    "    .sort_values(['cluster', 'rank_abs_delta'])\n",
    ")\n",
    "\n",
    "rep_feature = representatives_df[(representatives_df['branch'] == 'feature') & (representatives_df['role'] == 'centroid_medoid')]\n",
    "rep_dtw = representatives_df[(representatives_df['branch'] == 'dtw') & (representatives_df['role'] == 'cluster_medoid')]\n",
    "\n",
    "# Existing concise insights.md\n",
    "lines = []\n",
    "lines.append('# indep_winsize_5 Clustering Insights (MA(W), current run W=5)')\n",
    "lines.append('')\n",
    "lines.append('## 1) Setup and Integrity')\n",
    "lines.append(f'- Books analyzed: **{len(payloads)}**')\n",
    "lines.append('- Input checks passed: label existence, shape normalization, range `[0,4]`, integer-like, and embedding length alignment.')\n",
    "lines.append(f'- Feature set size: **{len(numeric_cols)}** numeric features per book.')\n",
    "lines.append('')\n",
    "\n",
    "feat_row = quality_df[(quality_df['branch']=='feature') & (quality_df['method']==selected_feature_method) & (quality_df['k']==selected_feature_k)].iloc[0]\n",
    "dtw_row = quality_df[(quality_df['branch']=='dtw') & (quality_df['k']==selected_dtw_k)].iloc[0]\n",
    "\n",
    "lines.append('## 2) Model Selection Results')\n",
    "lines.append(f\"- Feature branch selected: `{selected_feature_method}` with `k={selected_feature_k}` (silhouette={feat_row['silhouette']:.3f}, DB={feat_row['davies_bouldin']:.3f}, CH={feat_row['calinski_harabasz']:.1f}).\")\n",
    "if selected_feature_method == 'kmeans':\n",
    "    lines.append(f\"- KMeans stability (mean pairwise ARI across seeds): {feat_row['kmeans_stability_ari']:.3f}.\")\n",
    "lines.append(f\"- DTW branch selected: `average-linkage` with `k={selected_dtw_k}` (silhouette={dtw_row['silhouette']:.3f}).\")\n",
    "lines.append('')\n",
    "\n",
    "lines.append('## 3) Cross-Method Agreement')\n",
    "lines.append(f'- Adjusted Rand Index (feature vs DTW): **{ari:.3f}**')\n",
    "lines.append(f'- Normalized Mutual Information: **{nmi:.3f}**')\n",
    "lines.append('- Agreement table: `outputs/excitement_indep_clustering/tables/cluster_method_agreement.csv`.')\n",
    "lines.append('')\n",
    "\n",
    "lines.append('## 4) Feature-Branch Cluster Archetypes')\n",
    "for c in sorted(np.unique(feature_labels)):\n",
    "    top_feats = feature_top[(feature_top['cluster']==c) & (feature_top['rank_abs_delta']<=3)]\n",
    "    top_txt = '; '.join([f\"{r.feature} (delta_z={r.delta_z_vs_corpus:+.2f})\" for r in top_feats.itertuples(index=False)])\n",
    "    rep = rep_feature[rep_feature['cluster']==c].iloc[0]\n",
    "    csize = int(np.sum(feature_labels == c))\n",
    "    lines.append(f\"- Cluster {int(c)} (n={csize}): {top_txt}. Representative: {int(rep['book_id'])} | {rep['title']}.\")\n",
    "lines.append('')\n",
    "\n",
    "lines.append('## 5) DTW-Branch Shape Archetypes')\n",
    "for c in sorted(np.unique(dtw_labels)):\n",
    "    top_feats = dtw_top[(dtw_top['cluster']==c) & (dtw_top['rank_abs_delta']<=3)]\n",
    "    top_txt = '; '.join([f\"{r.feature} (delta_z={r.delta_z_vs_corpus:+.2f})\" for r in top_feats.itertuples(index=False)])\n",
    "    rep = rep_dtw[rep_dtw['cluster']==c].iloc[0]\n",
    "    csize = int(np.sum(dtw_labels == c))\n",
    "    lines.append(f\"- Cluster {int(c)} (n={csize}): {top_txt}. DTW medoid: {int(rep['book_id'])} | {rep['title']}.\")\n",
    "lines.append('')\n",
    "\n",
    "lines.append('## 6) Practical Reading')\n",
    "lines.append('- Use feature clusters as primary interpretable archetypes.')\n",
    "lines.append('- Use DTW clusters as trajectory-shape validation; disagreement flags uncertain archetypes.')\n",
    "lines.append('- MA5 centroid plots are better for coarse pacing patterns than raw per-chunk spikes.')\n",
    "lines.append('')\n",
    "\n",
    "lines.append('## 7) Next Steps')\n",
    "lines.append('1. Add bootstrap stability over book-resampling for selected k.')\n",
    "lines.append('2. Compare clustering using raw vs MA5-only feature subsets.')\n",
    "lines.append('3. Add external validation against genre/twist metadata as weak labels.')\n",
    "lines.append('')\n",
    "\n",
    "lines.append('## Provenance')\n",
    "lines.append('- Figures: `outputs/excitement_indep_clustering/figures/*.png`')\n",
    "lines.append('- Tables: `outputs/excitement_indep_clustering/tables/*.csv`')\n",
    "\n",
    "(OUT_ROOT / 'insights.md').write_text(chr(10).join(lines) + chr(10), encoding='utf-8')\n",
    "\n",
    "# New extended cluster report with embedded figures\n",
    "dtw_sweep = quality_df[quality_df['branch'] == 'dtw'].sort_values('k').reset_index(drop=True)\n",
    "feature_sweep = quality_df[quality_df['branch'] == 'feature'].sort_values(['method', 'k']).reset_index(drop=True)\n",
    "\n",
    "elbow_valid = elbow_df.dropna(subset=['delta_inertia']).copy()\n",
    "if len(elbow_valid):\n",
    "    elbow_best_idx = int(elbow_valid['delta_inertia'].idxmax())\n",
    "    elbow_best_k = int(elbow_df.loc[elbow_best_idx, 'k'])\n",
    "    elbow_best_drop = float(elbow_df.loc[elbow_best_idx, 'delta_inertia'])\n",
    "else:\n",
    "    elbow_best_k = selected_feature_k\n",
    "    elbow_best_drop = float('nan')\n",
    "\n",
    "genre_count_df = pd.read_csv(TABLE_DIR / 'genre_by_feature_cluster_counts.csv', index_col='cluster')\n",
    "genre_prop_df = pd.read_csv(TABLE_DIR / 'genre_by_feature_cluster_proportions.csv', index_col='cluster')\n",
    "feature_signature_df = pd.read_csv(TABLE_DIR / 'feature_cluster_signature_top_features.csv')\n",
    "\n",
    "report_lines = []\n",
    "report_lines.append('# indep_winsize_5 Clustering Report (Feature-Primary, MA(W) with W=5)')\n",
    "report_lines.append('')\n",
    "report_lines.append('## Executive Verdict')\n",
    "report_lines.append(f\"- The **feature branch** is primary for interpretation: selected `{selected_feature_method}` with `k={selected_feature_k}`.\")\n",
    "report_lines.append(f\"- Feature quality at selected solution: silhouette `{feat_row['silhouette']:.3f}`, Davies-Bouldin `{feat_row['davies_bouldin']:.3f}`, Calinski-Harabasz `{feat_row['calinski_harabasz']:.2f}`.\")\n",
    "if selected_feature_method == 'kmeans':\n",
    "    report_lines.append(f\"- KMeans seed-stability is high (mean pairwise ARI `{feat_row['kmeans_stability_ari']:.3f}`).\")\n",
    "report_lines.append(f\"- DTW validation selected `k={selected_dtw_k}` with silhouette `{dtw_row['silhouette']:.3f}`; agreement with feature clusters is limited (ARI `{ari:.3f}`, NMI `{nmi:.3f}`).\")\n",
    "report_lines.append('')\n",
    "\n",
    "report_lines.append('## Model Selection Diagnostics')\n",
    "report_lines.append(f\"- Elbow diagnostics (`k=1..10`) show largest inertia drop at `k={elbow_best_k}` (drop `{elbow_best_drop:.3f}`).\")\n",
    "report_lines.append('- Final model-selection policy remains silhouette-first over `k=2..6` for comparability with prior runs.')\n",
    "report_lines.append('- DTW branch is used as trajectory-shape validation rather than the primary cluster definition.')\n",
    "report_lines.append('')\n",
    "\n",
    "report_lines.append('## Feature Cluster Interpretation')\n",
    "for c in sorted(np.unique(feature_labels)):\n",
    "    csize = int(np.sum(feature_labels == c))\n",
    "    top_feats = feature_top[(feature_top['cluster'] == c) & (feature_top['rank_abs_delta'] <= 3)]\n",
    "    top_txt = '; '.join([f\"{r.feature} ({r.delta_z_vs_corpus:+.2f})\" for r in top_feats.itertuples(index=False)])\n",
    "    rep = rep_feature[rep_feature['cluster'] == c].iloc[0]\n",
    "    report_lines.append(f\"- Cluster {int(c)} (n={csize}): top signatures `{top_txt}`; representative `{int(rep['book_id'])} | {rep['title']}`.\")\n",
    "report_lines.append('')\n",
    "\n",
    "report_lines.append('## Genre Composition by Cluster')\n",
    "for c in genre_count_df.index:\n",
    "    row_counts = genre_count_df.loc[c]\n",
    "    top_genre = str(row_counts.idxmax())\n",
    "    top_count = int(row_counts.max())\n",
    "    top_prop = float(genre_prop_df.loc[c, top_genre])\n",
    "    report_lines.append(f\"- Cluster {int(c)} is most concentrated in `{top_genre}` ({top_count} books, {top_prop:.2%} of cluster).\")\n",
    "report_lines.append('')\n",
    "\n",
    "report_lines.append('## Feature vs DTW Agreement')\n",
    "report_lines.append(f'- ARI: `{ari:.3f}`')\n",
    "report_lines.append(f'- NMI: `{nmi:.3f}`')\n",
    "report_lines.append('- Interpretation: DTW captures broad trajectory shape, but does not strongly reproduce the feature-space grouping in this run.')\n",
    "report_lines.append('')\n",
    "\n",
    "report_lines.append('## Figure Gallery (Embedded)')\n",
    "embedded_figures = [\n",
    "    ('Feature PCA scatter (legend-checked)', 'feature_pca_scatter_feature_clusters.png'),\n",
    "    ('KMeans elbow diagnostics', 'feature_elbow_kmeans_inertia.png'),\n",
    "    ('Feature k-sweep quality metrics', 'feature_k_sweep_quality_metrics.png'),\n",
    "    ('DTW k-sweep silhouette', 'dtw_k_sweep_silhouette.png'),\n",
    "    ('Genre counts by feature cluster', 'genre_by_feature_cluster_counts.png'),\n",
    "    ('Genre proportions by feature cluster', 'genre_by_feature_cluster_proportions.png'),\n",
    "    ('Top feature signatures heatmap', 'cluster_feature_signature_heatmap_top12.png'),\n",
    "    ('Feature vs DTW contingency heatmap', 'cluster_method_contingency_heatmap.png'),\n",
    "    ('Feature cluster member trajectories (MA5)', 'feature_cluster_member_trajectories_ma5.png'),\n",
    "    ('Feature/DTW centroid trajectories (raw)', 'cluster_centroid_trajectories_raw.png'),\n",
    "    ('Feature/DTW centroid trajectories (MA5)', 'cluster_centroid_trajectories_ma5.png'),\n",
    "    ('DTW distance heatmap', 'dtw_distance_heatmap.png'),\n",
    "    ('DTW dendrogram', 'dtw_dendrogram.png'),\n",
    "    ('Cluster size comparison', 'cluster_size_comparison.png'),\n",
    "]\n",
    "for title, fig_name in embedded_figures:\n",
    "    report_lines.append(f'### {title}')\n",
    "    report_lines.append(f'![{title}](figures/{fig_name})')\n",
    "    report_lines.append('')\n",
    "\n",
    "report_lines.append('## Practical Use / Caveats / Next Steps')\n",
    "report_lines.append('- Use now: interpretable archetype grouping from feature clusters and MA5 pacing profiles.')\n",
    "report_lines.append('- Use with caution: DTW-driven hard grouping, because selected DTW split is coarse in this run.')\n",
    "report_lines.append('- Caveat: small sample size (20 books) means cluster boundaries should be treated as exploratory.')\n",
    "report_lines.append('1. Add bootstrap re-sampling of books to quantify cluster stability confidence intervals.')\n",
    "report_lines.append('2. Add constrained model selection that penalizes singleton clusters for DTW branch.')\n",
    "report_lines.append('3. Evaluate agreement with external weak labels (genre/twist ranks) for external validity.')\n",
    "\n",
    "cluster_report_path = OUT_ROOT / 'cluster_report.md'\n",
    "cluster_report_path.write_text(chr(10).join(report_lines) + chr(10), encoding='utf-8')\n",
    "\n",
    "# Write interim integrity so self-file existence can pass\n",
    "pd.DataFrame(integrity_rows).to_csv(TABLE_DIR / 'integrity_checks.csv', index=False)\n",
    "\n",
    "# Report image-link existence check\n",
    "missing_report_figs = [name for _, name in embedded_figures if not (FIG_DIR / name).exists()]\n",
    "integrity_rows.append({\n",
    "    'check': 'cluster_report_embedded_figures_exist',\n",
    "    'expected': len(embedded_figures),\n",
    "    'actual': len(embedded_figures) - len(missing_report_figs),\n",
    "    'pass': len(missing_report_figs) == 0,\n",
    "})\n",
    "\n",
    "# Output completeness\n",
    "expected = expected_outputs()\n",
    "missing = [str(p) for p in expected if not p.exists()]\n",
    "\n",
    "integrity_rows.extend([\n",
    "    {'check': 'output_files_complete', 'expected': len(expected), 'actual': len(expected) - len(missing), 'pass': len(missing) == 0},\n",
    "    {'check': 'feature_assign_unique_books', 'expected': 20, 'actual': int(feature_assign_df['book_id'].nunique()), 'pass': int(feature_assign_df['book_id'].nunique()) == 20},\n",
    "    {'check': 'dtw_assign_unique_books', 'expected': 20, 'actual': int(dtw_assign_df['book_id'].nunique()), 'pass': int(dtw_assign_df['book_id'].nunique()) == 20},\n",
    "    {'check': 'ma_window_is_5', 'expected': 5, 'actual': MA_WINDOW, 'pass': MA_WINDOW == 5},\n",
    "])\n",
    "\n",
    "integrity_df = pd.DataFrame(integrity_rows)\n",
    "integrity_df.to_csv(TABLE_DIR / 'integrity_checks.csv', index=False)\n",
    "\n",
    "print('Saved insights:', OUT_ROOT / 'insights.md')\n",
    "print('Saved cluster report:', cluster_report_path)\n",
    "print('Saved integrity:', TABLE_DIR / 'integrity_checks.csv')\n",
    "print('Integrity all-pass:', bool(integrity_df['pass'].all()))\n",
    "if missing_report_figs:\n",
    "    print('Missing report figures:', missing_report_figs)\n",
    "if missing:\n",
    "    print('Missing files:')\n",
    "    for m in missing:\n",
    "        print('-', m)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 - LLM Excitement Signal + Linear Projection from Embeddings\n",
    "\n",
    "This notebook:\n",
    "1. Visualizes `label.npy` excitement signals for all 20 novels.\n",
    "2. Trains a 1-layer linear perceptron (NumPy GD, MSE) from chunk embeddings to excitement.\n",
    "3. Uses a novel-level deterministic 80:20 split (16 train novels, 4 test novels).\n",
    "4. Produces train/test diagnostics and per-novel overlay plots.\n",
    "\n",
    "## Outputs\n",
    "- `outputs/excitement_linear/figures/labels_all_20_novels_grid.png`\n",
    "- `outputs/excitement_linear/figures/labels_all_20_novels_overlay_normpos.png`\n",
    "- `outputs/excitement_linear/figures/train_loss_curve.png`\n",
    "- `outputs/excitement_linear/figures/prediction_scatter_train_test.png`\n",
    "- `outputs/excitement_linear/figures/residual_hist_train_test.png`\n",
    "- `outputs/excitement_linear/figures/novel_overlay_test_{book_id}.png` (4 files)\n",
    "- `outputs/excitement_linear/figures/novel_overlay_train_{book_id}.png` (2 files)\n",
    "- `outputs/excitement_linear/tables/split_manifest.csv`\n",
    "- `outputs/excitement_linear/tables/global_metrics.csv`\n",
    "- `outputs/excitement_linear/tables/per_novel_metrics.csv`\n",
    "- `outputs/excitement_linear/tables/integrity_checks.csv`\n",
    "- `outputs/excitement_linear/model/linear_weights.npz`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/kongfha/Desktop/Time_Series_Mining/story-trajectory-analysis\n",
      "Processed dir: /Users/kongfha/Desktop/Time_Series_Mining/story-trajectory-analysis/data/processed\n",
      "Output root: /Users/kongfha/Desktop/Time_Series_Mining/story-trajectory-analysis/outputs/excitement_linear\n",
      "SEED=42, EPOCHS=200, BATCH_SIZE=4096, LR=0.01, WEIGHT_DECAY=0.0001, MA_WINDOW=9\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "PROJECT_ROOT = Path('.').resolve()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "METADATA_PATH = DATA_DIR / 'metadata.csv'\n",
    "\n",
    "OUT_ROOT = PROJECT_ROOT / 'outputs' / 'excitement_linear'\n",
    "FIG_DIR = OUT_ROOT / 'figures'\n",
    "TABLE_DIR = OUT_ROOT / 'tables'\n",
    "MODEL_DIR = OUT_ROOT / 'model'\n",
    "for d in [OUT_ROOT, FIG_DIR, TABLE_DIR, MODEL_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Optimization defaults\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 4096\n",
    "LR = 1e-2\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPS = 1e-8\n",
    "\n",
    "# Presentation smoothing\n",
    "MA_WINDOW = 9\n",
    "\n",
    "print(f'Project root: {PROJECT_ROOT}')\n",
    "print(f'Processed dir: {PROCESSED_DIR}')\n",
    "print(f'Output root: {OUT_ROOT}')\n",
    "print(f'SEED={SEED}, EPOCHS={EPOCHS}, BATCH_SIZE={BATCH_SIZE}, LR={LR}, WEIGHT_DECAY={WEIGHT_DECAY}, MA_WINDOW={MA_WINDOW}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label_shape(y: np.ndarray, label_path: Path) -> np.ndarray:\n",
    "    y = np.asarray(y)\n",
    "    if y.ndim == 1:\n",
    "        out = y\n",
    "    elif y.ndim == 2 and 1 in y.shape:\n",
    "        out = y.reshape(-1)\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported label shape {y.shape} at {label_path}')\n",
    "    return out\n",
    "\n",
    "\n",
    "def moving_average_1d(x: np.ndarray, window: int) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float64).reshape(-1)\n",
    "    if window <= 1:\n",
    "        return x.copy()\n",
    "    if window % 2 == 0:\n",
    "        raise ValueError(f'MA window must be odd for centered smoothing, got {window}')\n",
    "    return pd.Series(x).rolling(window=window, center=True, min_periods=1).mean().to_numpy()\n",
    "\n",
    "\n",
    "def metric_dict(y_true: np.ndarray, y_pred: np.ndarray) -> dict:\n",
    "    y_true = np.asarray(y_true, dtype=np.float64).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64).reshape(-1)\n",
    "    err = y_pred - y_true\n",
    "    mse = float(np.mean(err ** 2))\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    mae = float(np.mean(np.abs(err)))\n",
    "\n",
    "    denom = float(np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    if denom <= 0:\n",
    "        r2 = np.nan\n",
    "    else:\n",
    "        r2 = float(1 - np.sum(err ** 2) / denom)\n",
    "\n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "    }\n",
    "\n",
    "\n",
    "def iter_minibatches(n: int, batch_size: int, rng: np.random.Generator):\n",
    "    idx = rng.permutation(n)\n",
    "    for start in range(0, n, batch_size):\n",
    "        yield idx[start:start + batch_size]\n",
    "\n",
    "\n",
    "def required_output_files(test_ids: list[int], train_ids: list[int]) -> list[Path]:\n",
    "    files = [\n",
    "        FIG_DIR / 'labels_all_20_novels_grid.png',\n",
    "        FIG_DIR / 'labels_all_20_novels_overlay_normpos.png',\n",
    "        FIG_DIR / 'train_loss_curve.png',\n",
    "        FIG_DIR / 'prediction_scatter_train_test.png',\n",
    "        FIG_DIR / 'residual_hist_train_test.png',\n",
    "        FIG_DIR / 'mae_raw_vs_moving_average.png',\n",
    "        TABLE_DIR / 'split_manifest.csv',\n",
    "        TABLE_DIR / 'global_metrics.csv',\n",
    "        TABLE_DIR / 'per_novel_metrics.csv',\n",
    "        TABLE_DIR / 'presentation_mae.csv',\n",
    "        MODEL_DIR / 'linear_weights.npz',\n",
    "    ]\n",
    "    files.extend([FIG_DIR / f'novel_overlay_test_{bid}.png' for bid in test_ids])\n",
    "    files.extend([FIG_DIR / f'novel_overlay_train_{bid}.png' for bid in train_ids])\n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded books: 20\n",
      "Embedding dimension: 768\n",
      "Total chunks: 21337\n"
     ]
    }
   ],
   "source": [
    "# 1) Load data + fail-fast validation\n",
    "meta = pd.read_csv(METADATA_PATH)\n",
    "if 'id' not in meta.columns and 'pg_id' in meta.columns:\n",
    "    meta['id'] = meta['pg_id']\n",
    "required_cols = ['id', 'title', 'processed_dir']\n",
    "missing_cols = [c for c in required_cols if c not in meta.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f'metadata.csv missing required columns: {missing_cols}')\n",
    "\n",
    "meta = meta.sort_values('id').reset_index(drop=True)\n",
    "if meta['id'].nunique() != 20:\n",
    "    raise RuntimeError(f'Expected 20 unique books, found {meta[\"id\"].nunique()}')\n",
    "\n",
    "payloads = []\n",
    "integrity_rows = []\n",
    "\n",
    "for row in meta.to_dict(orient='records'):\n",
    "    book_id = int(row['id'])\n",
    "    title = str(row['title'])\n",
    "    processed_dir = str(row['processed_dir'])\n",
    "    base = PROCESSED_DIR / processed_dir\n",
    "\n",
    "    emb_path = base / 'embeddings.npy'\n",
    "    label_path = base / 'label.npy'\n",
    "\n",
    "    if not emb_path.exists():\n",
    "        raise FileNotFoundError(f'Missing embeddings.npy for book {book_id}: {emb_path}')\n",
    "    if not label_path.exists():\n",
    "        raise FileNotFoundError(f'Missing label.npy for book {book_id}: {label_path}')\n",
    "\n",
    "    X = np.load(emb_path)\n",
    "    y_raw = np.load(label_path)\n",
    "    y = normalize_label_shape(y_raw, label_path)\n",
    "\n",
    "    if X.ndim != 2:\n",
    "        raise ValueError(f'Expected embeddings shape (T,D), got {X.shape} at {emb_path}')\n",
    "    T, D = X.shape\n",
    "\n",
    "    if len(y) != T:\n",
    "        raise ValueError(f'Length mismatch for book {book_id}: label_len={len(y)} vs emb_T={T}')\n",
    "    if not np.issubdtype(y.dtype, np.number):\n",
    "        raise ValueError(f'label.npy must be numeric for book {book_id}, got dtype={y.dtype}')\n",
    "    if np.min(y) < 0 or np.max(y) > 4:\n",
    "        raise ValueError(f'label out of range [0,4] for book {book_id}: min={np.min(y)} max={np.max(y)}')\n",
    "\n",
    "    payloads.append({\n",
    "        'book_id': book_id,\n",
    "        'title': title,\n",
    "        'processed_dir': processed_dir,\n",
    "        'X': X.astype(np.float64),\n",
    "        'y': y.astype(np.float64),\n",
    "        'T': int(T),\n",
    "        'D': int(D),\n",
    "    })\n",
    "\n",
    "    integrity_rows.append({\n",
    "        'check': f'book_{book_id}_alignment',\n",
    "        'expected': 'label_len == emb_T and labels in [0,4]',\n",
    "        'actual': f'label_len={len(y)}, emb_T={T}, min={float(np.min(y))}, max={float(np.max(y))}',\n",
    "        'pass': True,\n",
    "    })\n",
    "\n",
    "D_unique = sorted({p['D'] for p in payloads})\n",
    "if len(D_unique) != 1:\n",
    "    raise RuntimeError(f'Expected a single embedding dimension across books, got {D_unique}')\n",
    "\n",
    "print(f'Loaded books: {len(payloads)}')\n",
    "print(f'Embedding dimension: {D_unique[0]}')\n",
    "print(f'Total chunks: {sum(p[\"T\"] for p in payloads)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved label visualizations.\n"
     ]
    }
   ],
   "source": [
    "# 2) Visualize labels for all 20 novels\n",
    "payloads_sorted = sorted(payloads, key=lambda p: p['book_id'])\n",
    "\n",
    "fig, axes = plt.subplots(5, 4, figsize=(24, 16), sharex=False, sharey=True)\n",
    "axes = axes.flatten()\n",
    "for ax, p in zip(axes, payloads_sorted):\n",
    "    t = np.arange(p['T'])\n",
    "    ax.plot(t, p['y'], linewidth=1.0, color='#1f77b4')\n",
    "    ax.set_title(f\"{p['book_id']} | {p['title'][:28]}\", fontsize=9)\n",
    "    ax.set_ylim(-0.2, 4.2)\n",
    "    ax.grid(alpha=0.2)\n",
    "for ax in axes[len(payloads_sorted):]:\n",
    "    ax.axis('off')\n",
    "fig.suptitle('LLM Excitement Label per Chunk (All 20 Novels)', fontsize=16)\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "fig.savefig(FIG_DIR / 'labels_all_20_novels_grid.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "for p in payloads_sorted:\n",
    "    t_norm = np.linspace(0.0, 1.0, p['T'])\n",
    "    ax.plot(t_norm, p['y'], alpha=0.35, linewidth=1.1, label=str(p['book_id']))\n",
    "ax.set_title('Excitement Labels Overlay Across Novels (Normalized Position)')\n",
    "ax.set_xlabel('Normalized position in novel')\n",
    "ax.set_ylabel('Excitement label (0-4)')\n",
    "ax.set_ylim(-0.2, 4.2)\n",
    "ax.grid(alpha=0.2)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'labels_all_20_novels_overlay_normpos.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "print('Saved label visualizations.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train IDs: [11, 35, 36, 43, 55, 84, 103, 120, 175, 345, 521, 1184, 1257, 1260, 1513, 1661]\n",
      "Test IDs: [16, 113, 768, 1342]\n",
      "Saved split manifest: /Users/kongfha/Desktop/Time_Series_Mining/story-trajectory-analysis/outputs/excitement_linear/tables/split_manifest.csv\n"
     ]
    }
   ],
   "source": [
    "# 3) Deterministic novel-level 80:20 split (16/4)\n",
    "book_ids = np.array([p['book_id'] for p in payloads_sorted], dtype=int)\n",
    "rng_split = np.random.default_rng(SEED)\n",
    "book_ids_shuffled = book_ids.copy()\n",
    "rng_split.shuffle(book_ids_shuffled)\n",
    "\n",
    "train_ids = sorted(book_ids_shuffled[:16].tolist())\n",
    "test_ids = sorted(book_ids_shuffled[16:].tolist())\n",
    "\n",
    "if len(train_ids) != 16 or len(test_ids) != 4:\n",
    "    raise RuntimeError(f'Unexpected split sizes: train={len(train_ids)}, test={len(test_ids)}')\n",
    "if set(train_ids) & set(test_ids):\n",
    "    raise RuntimeError('Train/Test split overlap detected.')\n",
    "\n",
    "split_rows = []\n",
    "for p in payloads_sorted:\n",
    "    split_rows.append({\n",
    "        'book_id': p['book_id'],\n",
    "        'title': p['title'],\n",
    "        'processed_dir': p['processed_dir'],\n",
    "        'T': p['T'],\n",
    "        'split': 'train' if p['book_id'] in set(train_ids) else 'test',\n",
    "    })\n",
    "\n",
    "split_df = pd.DataFrame(split_rows).sort_values(['split', 'book_id']).reset_index(drop=True)\n",
    "split_df.to_csv(TABLE_DIR / 'split_manifest.csv', index=False)\n",
    "\n",
    "integrity_rows.extend([\n",
    "    {'check': 'split_train_count', 'expected': 16, 'actual': len(train_ids), 'pass': len(train_ids) == 16},\n",
    "    {'check': 'split_test_count', 'expected': 4, 'actual': len(test_ids), 'pass': len(test_ids) == 4},\n",
    "    {'check': 'split_no_overlap', 'expected': True, 'actual': len(set(train_ids) & set(test_ids)) == 0, 'pass': len(set(train_ids) & set(test_ids)) == 0},\n",
    "    {'check': 'split_total_books', 'expected': 20, 'actual': len(set(train_ids + test_ids)), 'pass': len(set(train_ids + test_ids)) == 20},\n",
    "])\n",
    "\n",
    "print('Train IDs:', train_ids)\n",
    "print('Test IDs:', test_ids)\n",
    "print(f'Saved split manifest: {TABLE_DIR / \"split_manifest.csv\"}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (17631, 768) y_train (17631, 1)\n",
      "X_test  (3706, 768) y_test  (3706, 1)\n"
     ]
    }
   ],
   "source": [
    "# 4) Assemble datasets and standardize features using train stats only\n",
    "train_set = [p for p in payloads_sorted if p['book_id'] in set(train_ids)]\n",
    "test_set = [p for p in payloads_sorted if p['book_id'] in set(test_ids)]\n",
    "\n",
    "X_train = np.vstack([p['X'] for p in train_set])\n",
    "y_train = np.concatenate([p['y'] for p in train_set]).reshape(-1, 1)\n",
    "X_test = np.vstack([p['X'] for p in test_set])\n",
    "y_test = np.concatenate([p['y'] for p in test_set]).reshape(-1, 1)\n",
    "\n",
    "x_mean = X_train.mean(axis=0, keepdims=True)\n",
    "x_std = X_train.std(axis=0, keepdims=True)\n",
    "x_std_safe = np.where(x_std < EPS, 1.0, x_std)\n",
    "\n",
    "X_train_z = (X_train - x_mean) / x_std_safe\n",
    "X_test_z = (X_test - x_mean) / x_std_safe\n",
    "\n",
    "print('X_train', X_train_z.shape, 'y_train', y_train.shape)\n",
    "print('X_test ', X_test_z.shape, 'y_test ', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/200 | train_mse=3.401970 | test_mse=3.734706\n",
      "Epoch 025/200 | train_mse=1.501502 | test_mse=1.422093\n",
      "Epoch 050/200 | train_mse=1.481516 | test_mse=1.431584\n",
      "Epoch 075/200 | train_mse=1.471347 | test_mse=1.424586\n",
      "Epoch 100/200 | train_mse=1.460349 | test_mse=1.415930\n",
      "Epoch 125/200 | train_mse=1.458950 | test_mse=1.426686\n",
      "Epoch 150/200 | train_mse=1.451695 | test_mse=1.410563\n",
      "Epoch 175/200 | train_mse=1.450789 | test_mse=1.433173\n",
      "Epoch 200/200 | train_mse=1.447614 | test_mse=1.424222\n",
      "Training done.\n"
     ]
    }
   ],
   "source": [
    "# 5) Train 1-layer linear perceptron with NumPy GD (MSE)\n",
    "N_train, D = X_train_z.shape\n",
    "W = rng.normal(loc=0.0, scale=0.01, size=(D, 1))\n",
    "b = np.zeros((1,), dtype=np.float64)\n",
    "\n",
    "train_loss_hist = []\n",
    "test_loss_hist = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for idx in iter_minibatches(N_train, BATCH_SIZE, rng):\n",
    "        xb = X_train_z[idx]\n",
    "        yb = y_train[idx]\n",
    "\n",
    "        pred = xb @ W + b\n",
    "        err = pred - yb\n",
    "\n",
    "        grad_W = (2.0 / len(idx)) * (xb.T @ err) + WEIGHT_DECAY * W\n",
    "        grad_b = (2.0 / len(idx)) * np.sum(err, axis=0)\n",
    "\n",
    "        W -= LR * grad_W\n",
    "        b -= LR * grad_b\n",
    "\n",
    "    train_pred_epoch = X_train_z @ W + b\n",
    "    test_pred_epoch = X_test_z @ W + b\n",
    "\n",
    "    train_mse = float(np.mean((train_pred_epoch - y_train) ** 2))\n",
    "    test_mse = float(np.mean((test_pred_epoch - y_test) ** 2))\n",
    "\n",
    "    train_loss_hist.append(train_mse)\n",
    "    test_loss_hist.append(test_mse)\n",
    "\n",
    "    if (epoch + 1) % 25 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:03d}/{EPOCHS} | train_mse={train_mse:.6f} | test_mse={test_mse:.6f}\")\n",
    "\n",
    "train_pred = (X_train_z @ W + b).reshape(-1)\n",
    "test_pred = (X_test_z @ W + b).reshape(-1)\n",
    "\n",
    "integrity_rows.extend([\n",
    "    {'check': 'model_W_shape', 'expected': '(768,1)', 'actual': str(W.shape), 'pass': tuple(W.shape) == (768, 1)},\n",
    "    {'check': 'model_b_shape', 'expected': '(1,)', 'actual': str(b.shape), 'pass': tuple(b.shape) == (1,)},\n",
    "    {'check': 'prediction_shape_train', 'expected': y_train.reshape(-1).shape[0], 'actual': train_pred.shape[0], 'pass': train_pred.shape[0] == y_train.reshape(-1).shape[0]},\n",
    "    {'check': 'prediction_shape_test', 'expected': y_test.reshape(-1).shape[0], 'actual': test_pred.shape[0], 'pass': test_pred.shape[0] == y_test.reshape(-1).shape[0]},\n",
    "    {'check': 'training_net_loss_decrease', 'expected': 'final < first', 'actual': f\"first={train_loss_hist[0]:.6f},final={train_loss_hist[-1]:.6f}\", 'pass': train_loss_hist[-1] < train_loss_hist[0]},\n",
    "])\n",
    "\n",
    "print('Training done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9f6f11c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global metrics:\n",
      "   split  n_samples  n_novels       mse      rmse       mae        r2\n",
      "0  train      17631        16  1.447614  1.203168  1.063862  0.173388\n",
      "1   test       3706         4  1.424222  1.193408  1.040986  0.136427\n",
      "\n",
      "Presentation MAE (raw vs moving average):\n",
      "   split  ma_window   mae_raw  mae_moving_average  n_samples\n",
      "0  train          9  1.063862            0.328034      17631\n",
      "1   test          9  1.040986            0.348689       3706\n",
      "\n",
      "Saved metrics tables.\n"
     ]
    }
   ],
   "source": [
    "# 6) Metrics (regression-only): global + per-novel\n",
    "train_metrics = metric_dict(y_train.reshape(-1), train_pred)\n",
    "test_metrics = metric_dict(y_test.reshape(-1), test_pred)\n",
    "\n",
    "global_metrics_df = pd.DataFrame([\n",
    "    {'split': 'train', 'n_samples': int(len(train_pred)), 'n_novels': int(len(train_ids)), **train_metrics},\n",
    "    {'split': 'test', 'n_samples': int(len(test_pred)), 'n_novels': int(len(test_ids)), **test_metrics},\n",
    "])\n",
    "global_metrics_df.to_csv(TABLE_DIR / 'global_metrics.csv', index=False)\n",
    "\n",
    "per_rows = []\n",
    "mae_ma_abs_err_by_split = {'train': [], 'test': []}\n",
    "train_id_set = set(train_ids)\n",
    "for p in payloads_sorted:\n",
    "    Xp = (p['X'] - x_mean) / x_std_safe\n",
    "    yp = p['y'].reshape(-1)\n",
    "    yhat = (Xp @ W + b).reshape(-1)\n",
    "\n",
    "    y_true_ma = moving_average_1d(yp, MA_WINDOW)\n",
    "    y_pred_ma = moving_average_1d(yhat, MA_WINDOW)\n",
    "\n",
    "    m = metric_dict(yp, yhat)\n",
    "    mae_ma = float(np.mean(np.abs(y_pred_ma - y_true_ma)))\n",
    "    split = 'train' if p['book_id'] in train_id_set else 'test'\n",
    "\n",
    "    mae_ma_abs_err_by_split[split].append(np.abs(y_pred_ma - y_true_ma))\n",
    "    per_rows.append({\n",
    "        'book_id': p['book_id'],\n",
    "        'title': p['title'],\n",
    "        'processed_dir': p['processed_dir'],\n",
    "        'split': split,\n",
    "        'T': p['T'],\n",
    "        **m,\n",
    "        'mae_ma': mae_ma,\n",
    "    })\n",
    "\n",
    "per_novel_metrics_df = pd.DataFrame(per_rows).sort_values(['split', 'book_id']).reset_index(drop=True)\n",
    "per_novel_metrics_df.to_csv(TABLE_DIR / 'per_novel_metrics.csv', index=False)\n",
    "\n",
    "presentation_rows = []\n",
    "for split in ['train', 'test']:\n",
    "    split_abs = mae_ma_abs_err_by_split[split]\n",
    "    if len(split_abs) == 0:\n",
    "        mae_ma_value = np.nan\n",
    "        n_samples_ma = 0\n",
    "    else:\n",
    "        all_abs = np.concatenate(split_abs)\n",
    "        mae_ma_value = float(np.mean(all_abs))\n",
    "        n_samples_ma = int(all_abs.shape[0])\n",
    "\n",
    "    raw_mae_value = float(global_metrics_df.loc[global_metrics_df['split'] == split, 'mae'].iloc[0])\n",
    "    presentation_rows.append({\n",
    "        'split': split,\n",
    "        'ma_window': MA_WINDOW,\n",
    "        'mae_raw': raw_mae_value,\n",
    "        'mae_moving_average': mae_ma_value,\n",
    "        'n_samples': n_samples_ma,\n",
    "    })\n",
    "\n",
    "presentation_mae_df = pd.DataFrame(presentation_rows)\n",
    "presentation_mae_df.to_csv(TABLE_DIR / 'presentation_mae.csv', index=False)\n",
    "\n",
    "print('Global metrics:')\n",
    "print(global_metrics_df)\n",
    "print('')\n",
    "print('Presentation MAE (raw vs moving average):')\n",
    "print(presentation_mae_df)\n",
    "print('')\n",
    "print('Saved metrics tables.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved diagnostics plots.\n"
     ]
    }
   ],
   "source": [
    "# 7) Diagnostics plots\n",
    "# Loss curve\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(np.arange(1, EPOCHS + 1), train_loss_hist, label='train_mse', linewidth=1.6)\n",
    "ax.plot(np.arange(1, EPOCHS + 1), test_loss_hist, label='test_mse', linewidth=1.4)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_title('Training/Test MSE Across Epochs')\n",
    "ax.grid(alpha=0.25)\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'train_loss_curve.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Scatter true vs pred\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(y_train.reshape(-1), train_pred, s=8, alpha=0.15, label='train')\n",
    "ax.scatter(y_test.reshape(-1), test_pred, s=12, alpha=0.35, label='test')\n",
    "lo = min(float(y_train.min()), float(y_test.min()), float(train_pred.min()), float(test_pred.min()))\n",
    "hi = max(float(y_train.max()), float(y_test.max()), float(train_pred.max()), float(test_pred.max()))\n",
    "ax.plot([lo, hi], [lo, hi], linestyle='--', linewidth=1.2, color='black')\n",
    "ax.set_xlabel('True excitement label')\n",
    "ax.set_ylabel('Predicted excitement')\n",
    "ax.set_title('Prediction Scatter: True vs Predicted')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.2)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'prediction_scatter_train_test.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Residual hist\n",
    "train_res = train_pred - y_train.reshape(-1)\n",
    "test_res = test_pred - y_test.reshape(-1)\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(train_res, bins=60, alpha=0.45, label='train')\n",
    "ax.hist(test_res, bins=60, alpha=0.55, label='test')\n",
    "ax.set_xlabel('Residual (pred - true)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Residual Distribution')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.2)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'residual_hist_train_test.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Presentation figure: raw MAE vs moving-average MAE\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "plot_df = presentation_mae_df.copy().set_index('split').loc[['train', 'test']].reset_index()\n",
    "x = np.arange(len(plot_df))\n",
    "bar_w = 0.34\n",
    "ax.bar(x - bar_w / 2, plot_df['mae_raw'], width=bar_w, label='Raw MAE', alpha=0.9)\n",
    "ax.bar(x + bar_w / 2, plot_df['mae_moving_average'], width=bar_w, label=f'MA({MA_WINDOW}) MAE', alpha=0.9)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(plot_df['split'])\n",
    "ax.set_ylabel('MAE')\n",
    "ax.set_title('Train/Test MAE: Raw vs Moving Average (Presentation)')\n",
    "ax.grid(axis='y', alpha=0.25)\n",
    "ax.legend()\n",
    "for i, row in plot_df.iterrows():\n",
    "    ax.text(i - bar_w / 2, row['mae_raw'] + 0.01, f\"{row['mae_raw']:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "    ax.text(i + bar_w / 2, row['mae_moving_average'] + 0.01, f\"{row['mae_moving_average']:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / 'mae_raw_vs_moving_average.png', dpi=180, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "print('Saved diagnostics plots.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved per-novel overlay plots.\n",
      "Selected train overlays: [43, 1513]\n"
     ]
    }
   ],
   "source": [
    "# 8) Per-novel overlay plots: all 4 test + 2 train (best/worst RMSE)\n",
    "by_id = {p['book_id']: p for p in payloads_sorted}\n",
    "\n",
    "# test overlays\n",
    "for bid in test_ids:\n",
    "    p = by_id[bid]\n",
    "    Xp = (p['X'] - x_mean) / x_std_safe\n",
    "    y_true = p['y'].reshape(-1)\n",
    "    y_pred = (Xp @ W + b).reshape(-1)\n",
    "    y_true_ma = moving_average_1d(y_true, MA_WINDOW)\n",
    "    y_pred_ma = moving_average_1d(y_pred, MA_WINDOW)\n",
    "    raw_mae = float(np.mean(np.abs(y_pred - y_true)))\n",
    "    ma_mae = float(np.mean(np.abs(y_pred_ma - y_true_ma)))\n",
    "    t = np.arange(p['T'])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 4.5))\n",
    "    ax.plot(t, y_true, label='llm_label_raw', linewidth=1.0, alpha=0.30, color='tab:blue')\n",
    "    ax.plot(t, y_pred, label='linear_pred_raw', linewidth=1.0, alpha=0.30, color='tab:orange')\n",
    "    ax.plot(t, y_true_ma, label=f'llm_label_MA{MA_WINDOW}', linewidth=2.0, color='tab:blue')\n",
    "    ax.plot(t, y_pred_ma, label=f'linear_pred_MA{MA_WINDOW}', linewidth=2.0, color='tab:orange')\n",
    "    ax.set_title(f\"Test Novel {bid} | {p['title']} | raw MAE={raw_mae:.3f}, MA MAE={ma_mae:.3f}\")\n",
    "    ax.set_xlabel('Chunk index')\n",
    "    ax.set_ylabel('Excitement')\n",
    "    ax.set_ylim(-0.5, 4.5)\n",
    "    ax.grid(alpha=0.25)\n",
    "    ax.legend(ncol=2)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(FIG_DIR / f'novel_overlay_test_{bid}.png', dpi=180, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "# choose 2 train novels: lowest and highest train RMSE\n",
    "train_novel_rmse = per_novel_metrics_df[per_novel_metrics_df['split'] == 'train'][['book_id', 'rmse']].copy()\n",
    "train_best_id = int(train_novel_rmse.sort_values('rmse', ascending=True).iloc[0]['book_id'])\n",
    "train_worst_id = int(train_novel_rmse.sort_values('rmse', ascending=False).iloc[0]['book_id'])\n",
    "selected_train_overlay_ids = sorted(list({train_best_id, train_worst_id}))\n",
    "\n",
    "# ensure exactly 2 train overlays even if best==worst (unlikely)\n",
    "if len(selected_train_overlay_ids) == 1:\n",
    "    alt_id = int(train_novel_rmse.sort_values('rmse', ascending=True).iloc[1]['book_id'])\n",
    "    selected_train_overlay_ids.append(alt_id)\n",
    "    selected_train_overlay_ids = sorted(selected_train_overlay_ids)\n",
    "\n",
    "for bid in selected_train_overlay_ids:\n",
    "    p = by_id[bid]\n",
    "    Xp = (p['X'] - x_mean) / x_std_safe\n",
    "    y_true = p['y'].reshape(-1)\n",
    "    y_pred = (Xp @ W + b).reshape(-1)\n",
    "    y_true_ma = moving_average_1d(y_true, MA_WINDOW)\n",
    "    y_pred_ma = moving_average_1d(y_pred, MA_WINDOW)\n",
    "    raw_mae = float(np.mean(np.abs(y_pred - y_true)))\n",
    "    ma_mae = float(np.mean(np.abs(y_pred_ma - y_true_ma)))\n",
    "    t = np.arange(p['T'])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 4.5))\n",
    "    ax.plot(t, y_true, label='llm_label_raw', linewidth=1.0, alpha=0.30, color='tab:blue')\n",
    "    ax.plot(t, y_pred, label='linear_pred_raw', linewidth=1.0, alpha=0.30, color='tab:orange')\n",
    "    ax.plot(t, y_true_ma, label=f'llm_label_MA{MA_WINDOW}', linewidth=2.0, color='tab:blue')\n",
    "    ax.plot(t, y_pred_ma, label=f'linear_pred_MA{MA_WINDOW}', linewidth=2.0, color='tab:orange')\n",
    "    ax.set_title(f\"Train Novel {bid} | {p['title']} | raw MAE={raw_mae:.3f}, MA MAE={ma_mae:.3f}\")\n",
    "    ax.set_xlabel('Chunk index')\n",
    "    ax.set_ylabel('Excitement')\n",
    "    ax.set_ylim(-0.5, 4.5)\n",
    "    ax.grid(alpha=0.25)\n",
    "    ax.legend(ncol=2)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(FIG_DIR / f'novel_overlay_train_{bid}.png', dpi=180, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "print('Saved per-novel overlay plots.')\n",
    "print('Selected train overlays:', selected_train_overlay_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model: /Users/kongfha/Desktop/Time_Series_Mining/story-trajectory-analysis/outputs/excitement_linear/model/linear_weights.npz\n",
      "Saved integrity checks: /Users/kongfha/Desktop/Time_Series_Mining/story-trajectory-analysis/outputs/excitement_linear/tables/integrity_checks.csv\n",
      "Integrity all-pass: True\n",
      "\n",
      "Global metrics preview:\n",
      "   split  n_samples  n_novels       mse      rmse       mae        r2\n",
      "0  train      17631        16  1.447614  1.203168  1.063862  0.173388\n",
      "1   test       3706         4  1.424222  1.193408  1.040986  0.136427\n"
     ]
    }
   ],
   "source": [
    "# 9) Save model and finalize integrity checks\n",
    "np.savez(\n",
    "    MODEL_DIR / 'linear_weights.npz',\n",
    "    W=W.astype(np.float32),\n",
    "    b=b.astype(np.float32),\n",
    "    x_mean=x_mean.reshape(-1).astype(np.float32),\n",
    "    x_std=x_std_safe.reshape(-1).astype(np.float32),\n",
    "    seed=np.array([SEED], dtype=np.int32),\n",
    "    lr=np.array([LR], dtype=np.float32),\n",
    "    epochs=np.array([EPOCHS], dtype=np.int32),\n",
    "    batch_size=np.array([BATCH_SIZE], dtype=np.int32),\n",
    "    weight_decay=np.array([WEIGHT_DECAY], dtype=np.float32),\n",
    ")\n",
    "\n",
    "# output completeness + overlay coverage checks\n",
    "expected_files = required_output_files(test_ids=test_ids, train_ids=selected_train_overlay_ids)\n",
    "missing_files = [str(p) for p in expected_files if not p.exists()]\n",
    "\n",
    "integrity_rows.extend([\n",
    "    {'check': 'output_files_complete', 'expected': len(expected_files), 'actual': len(expected_files) - len(missing_files), 'pass': len(missing_files) == 0},\n",
    "    {'check': 'overlay_test_count', 'expected': 4, 'actual': len(list(FIG_DIR.glob('novel_overlay_test_*.png'))), 'pass': len(list(FIG_DIR.glob('novel_overlay_test_*.png'))) == 4},\n",
    "    {'check': 'overlay_train_count', 'expected': 2, 'actual': len(list(FIG_DIR.glob('novel_overlay_train_*.png'))), 'pass': len(list(FIG_DIR.glob('novel_overlay_train_*.png'))) == 2},\n",
    "])\n",
    "\n",
    "integrity_df = pd.DataFrame(integrity_rows)\n",
    "integrity_df.to_csv(TABLE_DIR / 'integrity_checks.csv', index=False)\n",
    "\n",
    "all_pass = bool(integrity_df['pass'].all())\n",
    "print(f'Saved model: {MODEL_DIR / \"linear_weights.npz\"}')\n",
    "print(f'Saved integrity checks: {TABLE_DIR / \"integrity_checks.csv\"}')\n",
    "print(f'Integrity all-pass: {all_pass}')\n",
    "if missing_files:\n",
    "    print('Missing files:')\n",
    "    for m in missing_files:\n",
    "        print('-', m)\n",
    "\n",
    "print(\"\\nGlobal metrics preview:\")\n",
    "print(pd.read_csv(TABLE_DIR / 'global_metrics.csv'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
